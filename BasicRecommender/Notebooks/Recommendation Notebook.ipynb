{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3, io, json\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import json_deserializer\n",
    "from sagemaker.amazon.common import write_spmatrix_to_sparse_tensor\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-eu-west-1-800020257645\n"
     ]
    }
   ],
   "source": [
    "# Set sagemaker environment variables\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to datasets\n",
    "df_location = 's3://' + bucket + \"/Dataset/UserArtistIDDataset.csv\"\n",
    "train_location = 's3://' + bucket + \"/Dataset/TrainDataset.csv\"\n",
    "test_location = 's3://' + bucket + \"/Dataset/TestDataset.csv\"\n",
    "\n",
    "# Read the datasets into dataframes\n",
    "df = pd.read_csv(df_location, sep=',', names=['user_id', 'artist_id', 'track_count'], encoding=\"utf8\")\n",
    "train = pd.read_csv(train_location, sep=',', names=['user_id', 'artist_id', 'track_count'], encoding=\"utf8\")\n",
    "test = pd.read_csv(test_location, sep=',', names=['user_id', 'artist_id', 'track_count'], encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>track_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4619</td>\n",
       "      <td>13865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6353</td>\n",
       "      <td>27558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11840</td>\n",
       "      <td>16025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5797</td>\n",
       "      <td>11888</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6807</td>\n",
       "      <td>13585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  artist_id  track_count\n",
       "0     4619      13865            1\n",
       "1     6353      27558            1\n",
       "2    11840      16025            1\n",
       "3     5797      11888            5\n",
       "4     6807      13585            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Users:  14601 \n",
      "#Artists:  33588 \n",
      "#Features:  48189\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of users, artist and features\n",
    "numUsers = df.user_id.max()\n",
    "numArtists = df.artist_id.max()\n",
    "numFeatures = numUsers + numArtists\n",
    "print(\"#Users: \", numUsers, \"\\n#Artists: \", numArtists, \"\\n#Features: \", numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the sparse one hot encoded matrix. This creates a sparse matrix of 1's and 0's. 1 for each artist the user\n",
    "# has 4 or more tracks for, indicating a strong preference, 0 for all other.\n",
    "def one_hot_encode(df):\n",
    "    feature_matrix = lil_matrix((df.shape[0], numFeatures)).astype('float32')\n",
    "    label_matrix = []\n",
    "\n",
    "    rowNum = 0\n",
    "    for index, row in df.iterrows():\n",
    "        feature_matrix[rowNum, int(row['user_id']) - 1] = 1\n",
    "        feature_matrix[rowNum, int(numUsers) + int(row['artist_id']) - 1] = 1\n",
    "        if int(row['track_count']) >= 4:\n",
    "            label_matrix.append(1)\n",
    "        else:\n",
    "            label_matrix.append(0)\n",
    "        rowNum += 1\n",
    "    \n",
    "    label_matrix = np.array(label_matrix).astype('float32')\n",
    "    return feature_matrix, label_matrix\n",
    "\n",
    "\n",
    "# One hot encode the datasets\n",
    "full_feature, full_label = one_hot_encode(df)\n",
    "train_feature, train_label = one_hot_encode(train)\n",
    "test_feature, test_label = one_hot_encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert the dataset into protobuf format. The required format of the factorization machine algo \n",
    "def convert_df_protobuf_write(feature, label, name):\n",
    "    buf = io.BytesIO()\n",
    "    write_spmatrix_to_sparse_tensor(buf, feature, label)\n",
    "    buf.seek(0)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(name).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket, name)\n",
    "\n",
    "\n",
    "# Convert all of the datasets\n",
    "allData_file = convert_df_protobuf_write(full_feature, full_label, 'full.protobuf')\n",
    "trainData_file = convert_df_protobuf_write(train_feature, train_label, 'train.protobuf')\n",
    "testData_file = convert_df_protobuf_write(test_feature, test_label, 'test.protobuf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData_file = 's3://{}/{}'.format(bucket, 'full.protobuf')\n",
    "trainData_file = 's3://{}/{}'.format(bucket, 'train.protobuf')\n",
    "testData_file = 's3://{}/{}'.format(bucket, 'test.protobuf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the factorization machine algo from the amazon container\n",
    "container = get_image_uri(boto3.Session().region_name, 'factorization-machines')\n",
    "\n",
    "# Set the location the model will be stored at\n",
    "output_file = 's3://{}/Output'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and set parameters\n",
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   get_execution_role(),\n",
    "                                   train_instance_count = 1,\n",
    "                                   train_instance_type = 'ml.c4.xlarge',\n",
    "                                   output_path = output_file,\n",
    "                                   sagemaker_session = sess)\n",
    "\n",
    "# Set the hyperparameters for the model\n",
    "fm.set_hyperparameters(feature_dim = numFeatures,\n",
    "                       predictor_type = 'binary_classifier',\n",
    "                       mini_batch_size = 1000,\n",
    "                       num_factors = 64,\n",
    "                       epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2019-03-25-18-55-44-623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-25 18:55:44 Starting - Starting the training job...\n",
      "2019-03-25 18:55:45 Starting - Launching requested ML instances......\n",
      "2019-03-25 18:56:47 Starting - Preparing the instances for training...\n",
      "2019-03-25 18:57:36 Downloading - Downloading input data\n",
      "2019-03-25 18:57:36 Training - Downloading the training image....\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'100', u'feature_dim': u'48189', u'mini_batch_size': u'1000', u'predictor_type': u'binary_classifier', u'num_factors': u'64'}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'100', u'feature_dim': u'48189', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'binary_classifier', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 WARNING 140146317055808] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Using default worker.\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:13.442] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/train\", \"num_examples\": 1000, \"features\": [{\"name\": \"label_values\", \"shape\": [1], \"storage_type\": \"dense\"}, {\"name\": \"values\", \"shape\": [48189], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:13.450] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:13.454] [tensorio] [info] batch={\"data_pipeline\": \"/opt/ml/input/data/test\", \"num_examples\": 1000, \"features\": [{\"name\": \"label_values\", \"shape\": [1], \"storage_type\": \"dense\"}, {\"name\": \"values\", \"shape\": [48189], \"storage_type\": \"CSR\"}]}\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:13.474] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 32, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] nvidia-smi took: 0.0251879692078 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 58.60495567321777, \"sum\": 58.60495567321777, \"min\": 58.60495567321777}}, \"EndTime\": 1553540293.520328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540293.440366}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1553540293.520569, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540293.520482}\n",
      "\u001b[0m\n",
      "\u001b[31m[18:58:13] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.201325.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[18:58:13] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.201325.0/RHEL5_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_accuracy <score>=0.251\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_classification_cross_entropy <loss>=0.699696899414\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=0 train binary_f_1.000 <score>=0.345851528384\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:18 INFO 140146317055808] Iter[0] Batch [500]#011Speed: 106006.79 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=500 train binary_classification_accuracy <score>=0.804273453094\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=500 train binary_classification_cross_entropy <loss>=0.491402861696\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=500 train binary_f_1.000 <score>=0.00402214209537\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:22 INFO 140146317055808] Iter[0] Batch [1000]#011Speed: 120501.94 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1000 train binary_classification_accuracy <score>=0.80432967033\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1000 train binary_classification_cross_entropy <loss>=0.48749724479\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1000 train binary_f_1.000 <score>=0.00201771101894\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:26 INFO 140146317055808] Iter[0] Batch [1500]#011Speed: 121396.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1500 train binary_classification_accuracy <score>=0.804848767488\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1500 train binary_classification_cross_entropy <loss>=0.484013632147\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=1500 train binary_f_1.000 <score>=0.00135007057187\u001b[0m\n",
      "\n",
      "2019-03-25 18:58:41 Training - Training image download completed. Training in progress.\u001b[31m[03/25/2019 18:58:30 INFO 140146317055808] Iter[0] Batch [2000]#011Speed: 120390.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=2000 train binary_classification_accuracy <score>=0.804617191404\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=2000 train binary_classification_cross_entropy <loss>=0.482122741699\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, batch=2000 train binary_f_1.000 <score>=0.00102717937056\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, train binary_classification_accuracy <score>=0.804722288916\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, train binary_classification_cross_entropy <loss>=0.480210625889\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=0, train binary_f_1.000 <score>=0.00106444171056\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 21508.76212120056, \"sum\": 21508.76212120056, \"min\": 21508.76212120056}}, \"EndTime\": 1553540315.029558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540293.520419}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2500, \"sum\": 2500.0, \"min\": 2500}, \"Total Records Seen\": {\"count\": 1, \"max\": 2499688, \"sum\": 2499688.0, \"min\": 2499688}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1553540315.029768, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1553540293.520766}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=116168.828641 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:35.029] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 21396, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_accuracy <score>=0.792\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_classification_cross_entropy <loss>=0.488450408936\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=0 train binary_f_1.000 <score>=0.00952380952381\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:39 INFO 140146317055808] Iter[1] Batch [500]#011Speed: 118738.36 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=500 train binary_classification_accuracy <score>=0.805706586826\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=500 train binary_classification_cross_entropy <loss>=0.468929203117\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=500 train binary_f_1.000 <score>=0.00397016238783\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:43 INFO 140146317055808] Iter[1] Batch [1000]#011Speed: 116860.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1000 train binary_classification_accuracy <score>=0.805425574426\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1000 train binary_classification_cross_entropy <loss>=0.468632932954\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1000 train binary_f_1.000 <score>=0.00615384615385\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 18:58:47 INFO 140146317055808] Iter[1] Batch [1500]#011Speed: 118676.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1500 train binary_classification_accuracy <score>=0.805967355097\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1500 train binary_classification_cross_entropy <loss>=0.467183761221\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=1500 train binary_f_1.000 <score>=0.00826775541337\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:51 INFO 140146317055808] Iter[1] Batch [2000]#011Speed: 121389.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=2000 train binary_classification_accuracy <score>=0.805784607696\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=2000 train binary_classification_cross_entropy <loss>=0.466605473036\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, batch=2000 train binary_f_1.000 <score>=0.00977421960399\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, train binary_classification_accuracy <score>=0.805904761905\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, train binary_classification_cross_entropy <loss>=0.465661433739\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=1, train binary_f_1.000 <score>=0.0108490835379\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20989.799976348877, \"sum\": 20989.799976348877, \"min\": 20989.799976348877}}, \"EndTime\": 1553540336.019837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540315.029631}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4999, \"sum\": 4999.0, \"min\": 4999}, \"Total Records Seen\": {\"count\": 1, \"max\": 4998376, \"sum\": 4998376.0, \"min\": 4998376}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1553540336.020071, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1553540315.030006}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119040.723334 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:58:56.020] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 20988, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_accuracy <score>=0.793\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_classification_cross_entropy <loss>=0.478390167236\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:58:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=0 train binary_f_1.000 <score>=0.0189573459716\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:00 INFO 140146317055808] Iter[2] Batch [500]#011Speed: 117955.30 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=500 train binary_classification_accuracy <score>=0.807035928144\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=500 train binary_classification_cross_entropy <loss>=0.459236629524\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=500 train binary_f_1.000 <score>=0.0186574360744\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:04 INFO 140146317055808] Iter[2] Batch [1000]#011Speed: 120046.23 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1000 train binary_classification_accuracy <score>=0.806703296703\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1000 train binary_classification_cross_entropy <loss>=0.459138512184\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1000 train binary_f_1.000 <score>=0.0204228346935\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:08 INFO 140146317055808] Iter[2] Batch [1500]#011Speed: 118115.10 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1500 train binary_classification_accuracy <score>=0.807248500999\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1500 train binary_classification_cross_entropy <loss>=0.457874115539\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=1500 train binary_f_1.000 <score>=0.0227128399832\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:12 INFO 140146317055808] Iter[2] Batch [2000]#011Speed: 120927.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=2000 train binary_classification_accuracy <score>=0.807132933533\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=2000 train binary_classification_cross_entropy <loss>=0.457468774021\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, batch=2000 train binary_f_1.000 <score>=0.025040483631\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, train binary_classification_accuracy <score>=0.807276510604\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, train binary_classification_cross_entropy <loss>=0.456718976714\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=2, train binary_f_1.000 <score>=0.0268103232273\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20968.07098388672, \"sum\": 20968.07098388672, \"min\": 20968.07098388672}}, \"EndTime\": 1553540356.988473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540336.019916}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:16 INFO 140146317055808] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7498, \"sum\": 7498.0, \"min\": 7498}, \"Total Records Seen\": {\"count\": 1, \"max\": 7497064, \"sum\": 7497064.0, \"min\": 7497064}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1553540356.98874, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1553540336.020368}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:16 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119163.855391 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:59:16.988] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 3, \"duration\": 20966, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_accuracy <score>=0.795\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_classification_cross_entropy <loss>=0.4703722229\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=0 train binary_f_1.000 <score>=0.037558685446\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:21 INFO 140146317055808] Iter[3] Batch [500]#011Speed: 118968.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=500 train binary_classification_accuracy <score>=0.809017964072\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=500 train binary_classification_cross_entropy <loss>=0.451591658906\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=500 train binary_f_1.000 <score>=0.042777966746\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 18:59:25 INFO 140146317055808] Iter[3] Batch [1000]#011Speed: 119520.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1000 train binary_classification_accuracy <score>=0.80871028971\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1000 train binary_classification_cross_entropy <loss>=0.451666382507\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1000 train binary_f_1.000 <score>=0.0451397026923\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:29 INFO 140146317055808] Iter[3] Batch [1500]#011Speed: 119612.15 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1500 train binary_classification_accuracy <score>=0.809282478348\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1500 train binary_classification_cross_entropy <loss>=0.450566309065\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=1500 train binary_f_1.000 <score>=0.0481847592258\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:33 INFO 140146317055808] Iter[3] Batch [2000]#011Speed: 120490.06 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=2000 train binary_classification_accuracy <score>=0.809165417291\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=2000 train binary_classification_cross_entropy <loss>=0.450313448165\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, batch=2000 train binary_f_1.000 <score>=0.0509682678543\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, train binary_classification_accuracy <score>=0.809329731893\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, train binary_classification_cross_entropy <loss>=0.449729344338\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=3, train binary_f_1.000 <score>=0.053238859559\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20883.08310508728, \"sum\": 20883.08310508728, \"min\": 20883.08310508728}}, \"EndTime\": 1553540377.872135, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540356.988572}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9997, \"sum\": 9997.0, \"min\": 9997}, \"Total Records Seen\": {\"count\": 1, \"max\": 9995752, \"sum\": 9995752.0, \"min\": 9995752}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1553540377.872336, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1553540356.989022}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119649.298165 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:59:37.872] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 20881, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_accuracy <score>=0.797\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_classification_cross_entropy <loss>=0.463988525391\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=0 train binary_f_1.000 <score>=0.0558139534884\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:42 INFO 140146317055808] Iter[4] Batch [500]#011Speed: 119343.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=500 train binary_classification_accuracy <score>=0.811023952096\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=500 train binary_classification_cross_entropy <loss>=0.445637604003\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=500 train binary_f_1.000 <score>=0.0714026501368\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:46 INFO 140146317055808] Iter[4] Batch [1000]#011Speed: 118565.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1000 train binary_classification_accuracy <score>=0.810797202797\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1000 train binary_classification_cross_entropy <loss>=0.445853705712\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1000 train binary_f_1.000 <score>=0.074511336982\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:50 INFO 140146317055808] Iter[4] Batch [1500]#011Speed: 120388.02 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1500 train binary_classification_accuracy <score>=0.811315123251\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1500 train binary_classification_cross_entropy <loss>=0.44488589197\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=1500 train binary_f_1.000 <score>=0.0770815003096\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:54 INFO 140146317055808] Iter[4] Batch [2000]#011Speed: 121590.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=2000 train binary_classification_accuracy <score>=0.811183408296\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=2000 train binary_classification_cross_entropy <loss>=0.444751149907\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, batch=2000 train binary_f_1.000 <score>=0.0798026245287\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, train binary_classification_accuracy <score>=0.811292917167\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, train binary_classification_cross_entropy <loss>=0.444292308855\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=4, train binary_f_1.000 <score>=0.0816860651923\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20833.09507369995, \"sum\": 20833.09507369995, \"min\": 20833.09507369995}}, \"EndTime\": 1553540398.705719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540377.872208}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12496, \"sum\": 12496.0, \"min\": 12496}, \"Total Records Seen\": {\"count\": 1, \"max\": 12494440, \"sum\": 12494440.0, \"min\": 12494440}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1553540398.705909, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1553540377.872595}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119936.478177 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 18:59:58.706] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 20832, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_accuracy <score>=0.798\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_classification_cross_entropy <loss>=0.458881408691\u001b[0m\n",
      "\u001b[31m[03/25/2019 18:59:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=0 train binary_f_1.000 <score>=0.0818181818182\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:02 INFO 140146317055808] Iter[5] Batch [500]#011Speed: 119453.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=500 train binary_classification_accuracy <score>=0.812878243513\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=500 train binary_classification_cross_entropy <loss>=0.44095883063\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=500 train binary_f_1.000 <score>=0.0987502403384\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:00:07 INFO 140146317055808] Iter[5] Batch [1000]#011Speed: 119030.10 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1000 train binary_classification_accuracy <score>=0.812607392607\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1000 train binary_classification_cross_entropy <loss>=0.441276615749\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1000 train binary_f_1.000 <score>=0.101163437027\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:11 INFO 140146317055808] Iter[5] Batch [1500]#011Speed: 120559.75 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1500 train binary_classification_accuracy <score>=0.813055296469\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1500 train binary_classification_cross_entropy <loss>=0.440404030329\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=1500 train binary_f_1.000 <score>=0.102968537214\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:15 INFO 140146317055808] Iter[5] Batch [2000]#011Speed: 118127.05 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=2000 train binary_classification_accuracy <score>=0.812923038481\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=2000 train binary_classification_cross_entropy <loss>=0.44035133297\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, batch=2000 train binary_f_1.000 <score>=0.105438234683\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, train binary_classification_accuracy <score>=0.812985994398\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, train binary_classification_cross_entropy <loss>=0.439979522778\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=5, train binary_f_1.000 <score>=0.106918046696\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20972.684860229492, \"sum\": 20972.684860229492, \"min\": 20972.684860229492}}, \"EndTime\": 1553540419.678852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540398.705785}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14995, \"sum\": 14995.0, \"min\": 14995}, \"Total Records Seen\": {\"count\": 1, \"max\": 14993128, \"sum\": 14993128.0, \"min\": 14993128}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1553540419.679045, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1553540398.706138}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119138.245927 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:00:19.679] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 20971, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_accuracy <score>=0.799\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_classification_cross_entropy <loss>=0.454712890625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=0 train binary_f_1.000 <score>=0.106666666667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:23 INFO 140146317055808] Iter[6] Batch [500]#011Speed: 117895.09 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=500 train binary_classification_accuracy <score>=0.814243512974\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=500 train binary_classification_cross_entropy <loss>=0.4371691766\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=500 train binary_f_1.000 <score>=0.120677274273\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:28 INFO 140146317055808] Iter[6] Batch [1000]#011Speed: 119982.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1000 train binary_classification_accuracy <score>=0.814028971029\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1000 train binary_classification_cross_entropy <loss>=0.437557982027\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1000 train binary_f_1.000 <score>=0.123195470838\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:32 INFO 140146317055808] Iter[6] Batch [1500]#011Speed: 117796.60 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1500 train binary_classification_accuracy <score>=0.814393737508\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1500 train binary_classification_cross_entropy <loss>=0.43675259068\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=1500 train binary_f_1.000 <score>=0.124402455237\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:36 INFO 140146317055808] Iter[6] Batch [2000]#011Speed: 119705.72 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=2000 train binary_classification_accuracy <score>=0.81423888056\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=2000 train binary_classification_cross_entropy <loss>=0.436755486698\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, batch=2000 train binary_f_1.000 <score>=0.126527427905\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, train binary_classification_accuracy <score>=0.814318927571\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, train binary_classification_cross_entropy <loss>=0.436443266418\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=6, train binary_f_1.000 <score>=0.12796954019\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21003.720998764038, \"sum\": 21003.720998764038, \"min\": 21003.720998764038}}, \"EndTime\": 1553540440.68303, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540419.67892}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17494, \"sum\": 17494.0, \"min\": 17494}, \"Total Records Seen\": {\"count\": 1, \"max\": 17491816, \"sum\": 17491816.0, \"min\": 17491816}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1553540440.683265, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1553540419.679275}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118961.809805 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:00:40.683] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 7, \"duration\": 21002, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_accuracy <score>=0.8\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_classification_cross_entropy <loss>=0.45122064209\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=0 train binary_f_1.000 <score>=0.115044247788\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:00:45 INFO 140146317055808] Iter[7] Batch [500]#011Speed: 115915.07 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=500 train binary_classification_accuracy <score>=0.815487025948\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=500 train binary_classification_cross_entropy <loss>=0.433993879124\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=500 train binary_f_1.000 <score>=0.140059722597\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:49 INFO 140146317055808] Iter[7] Batch [1000]#011Speed: 117495.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1000 train binary_classification_accuracy <score>=0.815202797203\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1000 train binary_classification_cross_entropy <loss>=0.434433481923\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1000 train binary_f_1.000 <score>=0.14186969995\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:53 INFO 140146317055808] Iter[7] Batch [1500]#011Speed: 119774.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1500 train binary_classification_accuracy <score>=0.815540306462\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1500 train binary_classification_cross_entropy <loss>=0.433676875044\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=1500 train binary_f_1.000 <score>=0.14280495356\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:57 INFO 140146317055808] Iter[7] Batch [2000]#011Speed: 120812.11 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=2000 train binary_classification_accuracy <score>=0.8153998001\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=2000 train binary_classification_cross_entropy <loss>=0.433718289811\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:00:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, batch=2000 train binary_f_1.000 <score>=0.144888685172\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, train binary_classification_accuracy <score>=0.815462985194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, train binary_classification_cross_entropy <loss>=0.433447778819\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=7, train binary_f_1.000 <score>=0.146101743874\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21066.35093688965, \"sum\": 21066.35093688965, \"min\": 21066.35093688965}}, \"EndTime\": 1553540461.749946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540440.683105}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19993, \"sum\": 19993.0, \"min\": 19993}, \"Total Records Seen\": {\"count\": 1, \"max\": 19990504, \"sum\": 19990504.0, \"min\": 19990504}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1553540461.750176, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1553540440.68356}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118608.228074 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:01:01.750] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 21065, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_accuracy <score>=0.803\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_classification_cross_entropy <loss>=0.448223632812\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=0 train binary_f_1.000 <score>=0.139737991266\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:05 INFO 140146317055808] Iter[8] Batch [500]#011Speed: 118217.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=500 train binary_classification_accuracy <score>=0.816483033932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=500 train binary_classification_cross_entropy <loss>=0.431255893365\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=500 train binary_f_1.000 <score>=0.156325129843\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:10 INFO 140146317055808] Iter[8] Batch [1000]#011Speed: 118143.31 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1000 train binary_classification_accuracy <score>=0.816254745255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1000 train binary_classification_cross_entropy <loss>=0.431733339597\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1000 train binary_f_1.000 <score>=0.158254351079\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:14 INFO 140146317055808] Iter[8] Batch [1500]#011Speed: 118523.52 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1500 train binary_classification_accuracy <score>=0.816638241173\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1500 train binary_classification_cross_entropy <loss>=0.431013806063\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=1500 train binary_f_1.000 <score>=0.159276410645\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:18 INFO 140146317055808] Iter[8] Batch [2000]#011Speed: 120169.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=2000 train binary_classification_accuracy <score>=0.816463768116\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=2000 train binary_classification_cross_entropy <loss>=0.431083036317\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, batch=2000 train binary_f_1.000 <score>=0.160979621676\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, train binary_classification_accuracy <score>=0.816513805522\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, train binary_classification_cross_entropy <loss>=0.430842739195\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=8, train binary_f_1.000 <score>=0.161962898657\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20997.164011001587, \"sum\": 20997.164011001587, \"min\": 20997.164011001587}}, \"EndTime\": 1553540482.747651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540461.75003}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22492, \"sum\": 22492.0, \"min\": 22492}, \"Total Records Seen\": {\"count\": 1, \"max\": 22489192, \"sum\": 22489192.0, \"min\": 22489192}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1553540482.747867, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1553540461.750454}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118999.065201 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:01:22.748] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 9, \"duration\": 20995, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_accuracy <score>=0.803\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_classification_cross_entropy <loss>=0.445600280762\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=0 train binary_f_1.000 <score>=0.147186147186\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:01:26 INFO 140146317055808] Iter[9] Batch [500]#011Speed: 119640.47 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=500 train binary_classification_accuracy <score>=0.817365269461\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=500 train binary_classification_cross_entropy <loss>=0.428842889295\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=500 train binary_f_1.000 <score>=0.17014329766\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:31 INFO 140146317055808] Iter[9] Batch [1000]#011Speed: 118795.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1000 train binary_classification_accuracy <score>=0.817162837163\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1000 train binary_classification_cross_entropy <loss>=0.429349583126\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1000 train binary_f_1.000 <score>=0.171982590914\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:35 INFO 140146317055808] Iter[9] Batch [1500]#011Speed: 117741.12 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1500 train binary_classification_accuracy <score>=0.817542971352\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1500 train binary_classification_cross_entropy <loss>=0.428659514385\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=1500 train binary_f_1.000 <score>=0.172894091497\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:39 INFO 140146317055808] Iter[9] Batch [2000]#011Speed: 119380.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=2000 train binary_classification_accuracy <score>=0.817350824588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=2000 train binary_classification_cross_entropy <loss>=0.428749716008\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, batch=2000 train binary_f_1.000 <score>=0.17428748941\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, train binary_classification_accuracy <score>=0.817386154462\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, train binary_classification_cross_entropy <loss>=0.42853204121\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=9, train binary_f_1.000 <score>=0.17509553196\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20993.612051010132, \"sum\": 20993.612051010132, \"min\": 20993.612051010132}}, \"EndTime\": 1553540503.741787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540482.747723}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24991, \"sum\": 24991.0, \"min\": 24991}, \"Total Records Seen\": {\"count\": 1, \"max\": 24987880, \"sum\": 24987880.0, \"min\": 24987880}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1553540503.742023, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1553540482.748144}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119019.10261 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:01:43.742] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 20992, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_classification_accuracy <score>=0.805\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_classification_cross_entropy <loss>=0.443267456055\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=0 train binary_f_1.000 <score>=0.163090128755\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:48 INFO 140146317055808] Iter[10] Batch [500]#011Speed: 116711.17 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=500 train binary_classification_accuracy <score>=0.818331337325\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=500 train binary_classification_cross_entropy <loss>=0.426681547184\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=500 train binary_f_1.000 <score>=0.182862888745\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:52 INFO 140146317055808] Iter[10] Batch [1000]#011Speed: 119258.22 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1000 train binary_classification_accuracy <score>=0.818051948052\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1000 train binary_classification_cross_entropy <loss>=0.42721142072\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1000 train binary_f_1.000 <score>=0.184363496314\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:56 INFO 140146317055808] Iter[10] Batch [1500]#011Speed: 119518.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1500 train binary_classification_accuracy <score>=0.818416389074\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1500 train binary_classification_cross_entropy <loss>=0.426545633559\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:01:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=1500 train binary_f_1.000 <score>=0.185173559105\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:00 INFO 140146317055808] Iter[10] Batch [2000]#011Speed: 120921.06 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=2000 train binary_classification_accuracy <score>=0.818196901549\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=2000 train binary_classification_cross_entropy <loss>=0.426652233194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, batch=2000 train binary_f_1.000 <score>=0.18635318536\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, train binary_classification_accuracy <score>=0.818211284514\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, train binary_classification_cross_entropy <loss>=0.426451942948\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=10, train binary_f_1.000 <score>=0.186932647977\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20975.337982177734, \"sum\": 20975.337982177734, \"min\": 20975.337982177734}}, \"EndTime\": 1553540524.717663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540503.741872}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 27490, \"sum\": 27490.0, \"min\": 27490}, \"Total Records Seen\": {\"count\": 1, \"max\": 27486568, \"sum\": 27486568.0, \"min\": 27486568}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1553540524.717865, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1553540503.742296}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119123.115712 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:02:04.718] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 20974, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_classification_accuracy <score>=0.807\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_classification_cross_entropy <loss>=0.441165710449\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=0 train binary_f_1.000 <score>=0.178723404255\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:02:08 INFO 140146317055808] Iter[11] Batch [500]#011Speed: 118380.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=500 train binary_classification_accuracy <score>=0.819147704591\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=500 train binary_classification_cross_entropy <loss>=0.424721585455\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=500 train binary_f_1.000 <score>=0.194038480355\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:13 INFO 140146317055808] Iter[11] Batch [1000]#011Speed: 119595.39 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1000 train binary_classification_accuracy <score>=0.818848151848\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1000 train binary_classification_cross_entropy <loss>=0.425270083005\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1000 train binary_f_1.000 <score>=0.19528798832\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:17 INFO 140146317055808] Iter[11] Batch [1500]#011Speed: 119184.85 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1500 train binary_classification_accuracy <score>=0.819192538308\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1500 train binary_classification_cross_entropy <loss>=0.424624850279\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=1500 train binary_f_1.000 <score>=0.195832666631\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:21 INFO 140146317055808] Iter[11] Batch [2000]#011Speed: 119140.04 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=2000 train binary_classification_accuracy <score>=0.818948025987\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=2000 train binary_classification_cross_entropy <loss>=0.424744625322\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, batch=2000 train binary_f_1.000 <score>=0.196872928642\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, train binary_classification_accuracy <score>=0.818952380952\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, train binary_classification_cross_entropy <loss>=0.424557950194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=11, train binary_f_1.000 <score>=0.19726696261\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20946.267127990723, \"sum\": 20946.267127990723, \"min\": 20946.267127990723}}, \"EndTime\": 1553540545.664386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540524.717731}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29989, \"sum\": 29989.0, \"min\": 29989}, \"Total Records Seen\": {\"count\": 1, \"max\": 29985256, \"sum\": 29985256.0, \"min\": 29985256}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1553540545.664623, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1553540524.718091}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119288.243102 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:02:25.664] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 20945, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_classification_accuracy <score>=0.807\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_classification_cross_entropy <loss>=0.439250793457\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=0 train binary_f_1.000 <score>=0.178723404255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:29 INFO 140146317055808] Iter[12] Batch [500]#011Speed: 118923.40 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=500 train binary_classification_accuracy <score>=0.819764471058\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=500 train binary_classification_cross_entropy <loss>=0.422926615945\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=500 train binary_f_1.000 <score>=0.203341979426\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:34 INFO 140146317055808] Iter[12] Batch [1000]#011Speed: 118960.19 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1000 train binary_classification_accuracy <score>=0.819452547453\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1000 train binary_classification_cross_entropy <loss>=0.423490158645\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1000 train binary_f_1.000 <score>=0.204177968789\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:38 INFO 140146317055808] Iter[12] Batch [1500]#011Speed: 119649.20 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1500 train binary_classification_accuracy <score>=0.819790806129\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1500 train binary_classification_cross_entropy <loss>=0.422862653922\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=1500 train binary_f_1.000 <score>=0.204700749156\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:42 INFO 140146317055808] Iter[12] Batch [2000]#011Speed: 116871.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=2000 train binary_classification_accuracy <score>=0.819585707146\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=2000 train binary_classification_cross_entropy <loss>=0.422993230451\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, batch=2000 train binary_f_1.000 <score>=0.205933137278\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, train binary_classification_accuracy <score>=0.819577831132\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, train binary_classification_cross_entropy <loss>=0.422817343515\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=12, train binary_f_1.000 <score>=0.206176636812\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20971.184015274048, \"sum\": 20971.184015274048, \"min\": 20971.184015274048}}, \"EndTime\": 1553540566.636075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540545.664448}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32488, \"sum\": 32488.0, \"min\": 32488}, \"Total Records Seen\": {\"count\": 1, \"max\": 32483944, \"sum\": 32483944.0, \"min\": 32483944}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1553540566.636257, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1553540545.664853}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119146.768098 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:02:46.636] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 13, \"duration\": 20970, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_classification_accuracy <score>=0.809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_classification_cross_entropy <loss>=0.437488647461\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=0 train binary_f_1.000 <score>=0.194092827004\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:50 INFO 140146317055808] Iter[13] Batch [500]#011Speed: 120140.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=500 train binary_classification_accuracy <score>=0.820349301397\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=500 train binary_classification_cross_entropy <loss>=0.421269046867\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=500 train binary_f_1.000 <score>=0.211720193731\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:02:54 INFO 140146317055808] Iter[13] Batch [1000]#011Speed: 119734.50 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1000 train binary_classification_accuracy <score>=0.82005994006\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1000 train binary_classification_cross_entropy <loss>=0.421844756562\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1000 train binary_f_1.000 <score>=0.212638352188\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:59 INFO 140146317055808] Iter[13] Batch [1500]#011Speed: 118656.73 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1500 train binary_classification_accuracy <score>=0.820403730846\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1500 train binary_classification_cross_entropy <loss>=0.421232753482\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:02:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=1500 train binary_f_1.000 <score>=0.213161550945\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:03 INFO 140146317055808] Iter[13] Batch [2000]#011Speed: 121019.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=2000 train binary_classification_accuracy <score>=0.820184407796\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=2000 train binary_classification_cross_entropy <loss>=0.421372318296\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, batch=2000 train binary_f_1.000 <score>=0.214281814827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, train binary_classification_accuracy <score>=0.820174469788\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, train binary_classification_cross_entropy <loss>=0.421205056718\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=13, train binary_f_1.000 <score>=0.214457022592\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20801.60903930664, \"sum\": 20801.60903930664, \"min\": 20801.60903930664}}, \"EndTime\": 1553540587.438125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540566.636134}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 34987, \"sum\": 34987.0, \"min\": 34987}, \"Total Records Seen\": {\"count\": 1, \"max\": 34982632, \"sum\": 34982632.0, \"min\": 34982632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1553540587.438351, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1553540566.636486}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120117.84318 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:03:07.438] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 20800, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_classification_accuracy <score>=0.809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_classification_cross_entropy <loss>=0.435852783203\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=0 train binary_f_1.000 <score>=0.200836820084\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:11 INFO 140146317055808] Iter[14] Batch [500]#011Speed: 118271.42 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=500 train binary_classification_accuracy <score>=0.820914171657\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=500 train binary_classification_cross_entropy <loss>=0.419727268973\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=500 train binary_f_1.000 <score>=0.219319922038\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:15 INFO 140146317055808] Iter[14] Batch [1000]#011Speed: 118674.67 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1000 train binary_classification_accuracy <score>=0.820614385614\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1000 train binary_classification_cross_entropy <loss>=0.420312794841\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1000 train binary_f_1.000 <score>=0.220174322405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:20 INFO 140146317055808] Iter[14] Batch [1500]#011Speed: 120195.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1500 train binary_classification_accuracy <score>=0.820933377748\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1500 train binary_classification_cross_entropy <loss>=0.419714492605\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=1500 train binary_f_1.000 <score>=0.220625462137\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:24 INFO 140146317055808] Iter[14] Batch [2000]#011Speed: 121470.70 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=2000 train binary_classification_accuracy <score>=0.820731134433\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=2000 train binary_classification_cross_entropy <loss>=0.419861628588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, batch=2000 train binary_f_1.000 <score>=0.22180618317\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, train binary_classification_accuracy <score>=0.820722689076\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, train binary_classification_cross_entropy <loss>=0.419701314364\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=14, train binary_f_1.000 <score>=0.221970991539\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20844.928979873657, \"sum\": 20844.928979873657, \"min\": 20844.928979873657}}, \"EndTime\": 1553540608.283569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540587.4382}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 37486, \"sum\": 37486.0, \"min\": 37486}, \"Total Records Seen\": {\"count\": 1, \"max\": 37481320, \"sum\": 37481320.0, \"min\": 37481320}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1553540608.283792, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1553540587.43861}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119868.182925 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:03:28.284] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 15, \"duration\": 20844, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_classification_accuracy <score>=0.81\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_classification_cross_entropy <loss>=0.43432220459\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=0 train binary_f_1.000 <score>=0.208333333333\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:32 INFO 140146317055808] Iter[15] Batch [500]#011Speed: 118256.07 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=500 train binary_classification_accuracy <score>=0.821508982036\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=500 train binary_classification_cross_entropy <loss>=0.41828398178\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=500 train binary_f_1.000 <score>=0.226810541606\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:36 INFO 140146317055808] Iter[15] Batch [1000]#011Speed: 120485.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1000 train binary_classification_accuracy <score>=0.821156843157\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1000 train binary_classification_cross_entropy <loss>=0.418877399547\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1000 train binary_f_1.000 <score>=0.227334328899\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:03:40 INFO 140146317055808] Iter[15] Batch [1500]#011Speed: 120560.05 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1500 train binary_classification_accuracy <score>=0.821481012658\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1500 train binary_classification_cross_entropy <loss>=0.418291315372\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=1500 train binary_f_1.000 <score>=0.22776959633\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:45 INFO 140146317055808] Iter[15] Batch [2000]#011Speed: 116770.98 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=2000 train binary_classification_accuracy <score>=0.821274862569\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=2000 train binary_classification_cross_entropy <loss>=0.418444899468\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, batch=2000 train binary_f_1.000 <score>=0.228897192473\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, train binary_classification_accuracy <score>=0.821269307723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, train binary_classification_cross_entropy <loss>=0.418290207392\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=15, train binary_f_1.000 <score>=0.229082523836\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20930.505990982056, \"sum\": 20930.505990982056, \"min\": 20930.505990982056}}, \"EndTime\": 1553540629.214605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540608.283648}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 39985, \"sum\": 39985.0, \"min\": 39985}, \"Total Records Seen\": {\"count\": 1, \"max\": 39980008, \"sum\": 39980008.0, \"min\": 39980008}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1553540629.21484, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1553540608.284066}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119377.941739 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:03:49.215] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 20929, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_classification_accuracy <score>=0.809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_classification_cross_entropy <loss>=0.432880401611\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=0 train binary_f_1.000 <score>=0.207468879668\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:53 INFO 140146317055808] Iter[16] Batch [500]#011Speed: 120767.70 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=500 train binary_classification_accuracy <score>=0.821996007984\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=500 train binary_classification_cross_entropy <loss>=0.416925153294\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=500 train binary_f_1.000 <score>=0.233361415333\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:57 INFO 140146317055808] Iter[16] Batch [1000]#011Speed: 120317.65 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1000 train binary_classification_accuracy <score>=0.821669330669\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1000 train binary_classification_cross_entropy <loss>=0.417524874753\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:03:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1000 train binary_f_1.000 <score>=0.233968871094\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:01 INFO 140146317055808] Iter[16] Batch [1500]#011Speed: 117791.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1500 train binary_classification_accuracy <score>=0.822001332445\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1500 train binary_classification_cross_entropy <loss>=0.416949775996\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=1500 train binary_f_1.000 <score>=0.234369358268\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:05 INFO 140146317055808] Iter[16] Batch [2000]#011Speed: 119722.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=2000 train binary_classification_accuracy <score>=0.821790104948\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=2000 train binary_classification_cross_entropy <loss>=0.417108905996\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, batch=2000 train binary_f_1.000 <score>=0.235398298398\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, train binary_classification_accuracy <score>=0.821783513405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, train binary_classification_cross_entropy <loss>=0.416958796556\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=16, train binary_f_1.000 <score>=0.235490011965\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20894.96612548828, \"sum\": 20894.96612548828, \"min\": 20894.96612548828}}, \"EndTime\": 1553540650.110119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540629.214687}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42484, \"sum\": 42484.0, \"min\": 42484}, \"Total Records Seen\": {\"count\": 1, \"max\": 42478696, \"sum\": 42478696.0, \"min\": 42478696}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1553540650.11034, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1553540629.21512}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119581.071606 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:04:10.110] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 20893, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_classification_accuracy <score>=0.81\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_classification_cross_entropy <loss>=0.431514221191\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=0 train binary_f_1.000 <score>=0.214876033058\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:04:14 INFO 140146317055808] Iter[17] Batch [500]#011Speed: 118774.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=500 train binary_classification_accuracy <score>=0.822429141717\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=500 train binary_classification_cross_entropy <loss>=0.415639308663\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=500 train binary_f_1.000 <score>=0.239235840909\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:18 INFO 140146317055808] Iter[17] Batch [1000]#011Speed: 120714.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1000 train binary_classification_accuracy <score>=0.822123876124\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1000 train binary_classification_cross_entropy <loss>=0.416244018524\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1000 train binary_f_1.000 <score>=0.239897545358\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:22 INFO 140146317055808] Iter[17] Batch [1500]#011Speed: 118894.75 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1500 train binary_classification_accuracy <score>=0.822448367755\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1500 train binary_classification_cross_entropy <loss>=0.415678873406\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=1500 train binary_f_1.000 <score>=0.240209145258\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:26 INFO 140146317055808] Iter[17] Batch [2000]#011Speed: 119340.37 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=2000 train binary_classification_accuracy <score>=0.82228085957\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=2000 train binary_classification_cross_entropy <loss>=0.415842817437\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, batch=2000 train binary_f_1.000 <score>=0.241401083664\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, train binary_classification_accuracy <score>=0.822286514606\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, train binary_classification_cross_entropy <loss>=0.41569647091\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=17, train binary_f_1.000 <score>=0.241557510033\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20890.270948410034, \"sum\": 20890.270948410034, \"min\": 20890.270948410034}}, \"EndTime\": 1553540671.000922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540650.110189}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 44983, \"sum\": 44983.0, \"min\": 44983}, \"Total Records Seen\": {\"count\": 1, \"max\": 44977384, \"sum\": 44977384.0, \"min\": 44977384}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1553540671.00115, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1553540650.110619}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119607.929724 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:04:31.001] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 20888, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_classification_accuracy <score>=0.811\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_classification_cross_entropy <loss>=0.430213165283\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=0 train binary_f_1.000 <score>=0.222222222222\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:35 INFO 140146317055808] Iter[18] Batch [500]#011Speed: 118918.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=500 train binary_classification_accuracy <score>=0.822992015968\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=500 train binary_classification_cross_entropy <loss>=0.414416991639\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=500 train binary_f_1.000 <score>=0.245749521582\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:39 INFO 140146317055808] Iter[18] Batch [1000]#011Speed: 120948.44 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1000 train binary_classification_accuracy <score>=0.822541458541\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1000 train binary_classification_cross_entropy <loss>=0.415025594187\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1000 train binary_f_1.000 <score>=0.245572458782\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:43 INFO 140146317055808] Iter[18] Batch [1500]#011Speed: 120074.72 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1500 train binary_classification_accuracy <score>=0.822894736842\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1500 train binary_classification_cross_entropy <loss>=0.414469527558\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=1500 train binary_f_1.000 <score>=0.246005746393\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:47 INFO 140146317055808] Iter[18] Batch [2000]#011Speed: 121196.52 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=2000 train binary_classification_accuracy <score>=0.822730134933\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=2000 train binary_classification_cross_entropy <loss>=0.414637690066\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, batch=2000 train binary_f_1.000 <score>=0.247104345445\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, train binary_classification_accuracy <score>=0.822743497399\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, train binary_classification_cross_entropy <loss>=0.414494458562\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=18, train binary_f_1.000 <score>=0.247317820816\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20791.37897491455, \"sum\": 20791.37897491455, \"min\": 20791.37897491455}}, \"EndTime\": 1553540691.792824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540671.000999}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 47482, \"sum\": 47482.0, \"min\": 47482}, \"Total Records Seen\": {\"count\": 1, \"max\": 47476072, \"sum\": 47476072.0, \"min\": 47476072}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1553540691.793092, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1553540671.001413}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120176.526653 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:04:51.793] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 19, \"duration\": 20789, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_classification_accuracy <score>=0.812\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_classification_cross_entropy <loss>=0.428968780518\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=0 train binary_f_1.000 <score>=0.229508196721\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:04:56 INFO 140146317055808] Iter[19] Batch [500]#011Speed: 118073.71 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=500 train binary_classification_accuracy <score>=0.823550898204\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=500 train binary_classification_cross_entropy <loss>=0.413250381561\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:04:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=500 train binary_f_1.000 <score>=0.251809094987\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:00 INFO 140146317055808] Iter[19] Batch [1000]#011Speed: 118048.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1000 train binary_classification_accuracy <score>=0.823054945055\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1000 train binary_classification_cross_entropy <loss>=0.413861950885\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1000 train binary_f_1.000 <score>=0.251392634044\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:04 INFO 140146317055808] Iter[19] Batch [1500]#011Speed: 117366.92 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1500 train binary_classification_accuracy <score>=0.82337308461\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1500 train binary_classification_cross_entropy <loss>=0.413314229194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=1500 train binary_f_1.000 <score>=0.251701275498\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:08 INFO 140146317055808] Iter[19] Batch [2000]#011Speed: 117547.02 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=2000 train binary_classification_accuracy <score>=0.823211894053\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=2000 train binary_classification_cross_entropy <loss>=0.413486118223\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, batch=2000 train binary_f_1.000 <score>=0.252728705325\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, train binary_classification_accuracy <score>=0.823217687075\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, train binary_classification_cross_entropy <loss>=0.413345487316\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=19, train binary_f_1.000 <score>=0.252836666526\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21187.56103515625, \"sum\": 21187.56103515625, \"min\": 21187.56103515625}}, \"EndTime\": 1553540712.980977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540691.792934}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:12 INFO 140146317055808] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 49981, \"sum\": 49981.0, \"min\": 49981}, \"Total Records Seen\": {\"count\": 1, \"max\": 49974760, \"sum\": 49974760.0, \"min\": 49974760}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1553540712.981191, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1553540691.793383}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:12 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=117929.744528 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:05:12.981] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 21186, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_classification_accuracy <score>=0.811\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_classification_cross_entropy <loss>=0.427774414062\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=0 train binary_f_1.000 <score>=0.228571428571\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:17 INFO 140146317055808] Iter[20] Batch [500]#011Speed: 117929.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=500 train binary_classification_accuracy <score>=0.823926147705\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=500 train binary_classification_cross_entropy <loss>=0.41213296588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=500 train binary_f_1.000 <score>=0.256659166941\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:21 INFO 140146317055808] Iter[20] Batch [1000]#011Speed: 120371.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1000 train binary_classification_accuracy <score>=0.823473526474\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1000 train binary_classification_cross_entropy <loss>=0.412746714894\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1000 train binary_f_1.000 <score>=0.256497645826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:25 INFO 140146317055808] Iter[20] Batch [1500]#011Speed: 122866.76 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1500 train binary_classification_accuracy <score>=0.823766155896\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1500 train binary_classification_cross_entropy <loss>=0.412206704685\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=1500 train binary_f_1.000 <score>=0.256652213377\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:29 INFO 140146317055808] Iter[20] Batch [2000]#011Speed: 119159.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=2000 train binary_classification_accuracy <score>=0.823595702149\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=2000 train binary_classification_cross_entropy <loss>=0.412381912407\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, batch=2000 train binary_f_1.000 <score>=0.257641048937\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, train binary_classification_accuracy <score>=0.823606242497\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, train binary_classification_cross_entropy <loss>=0.412243476553\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=20, train binary_f_1.000 <score>=0.257731542132\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20803.099155426025, \"sum\": 20803.099155426025, \"min\": 20803.099155426025}}, \"EndTime\": 1553540733.784598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540712.981053}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 52480, \"sum\": 52480.0, \"min\": 52480}, \"Total Records Seen\": {\"count\": 1, \"max\": 52473448, \"sum\": 52473448.0, \"min\": 52473448}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1553540733.784825, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 20}, \"StartTime\": 1553540712.981466}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120109.100318 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:05:33.785] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 21, \"duration\": 20801, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_classification_accuracy <score>=0.811\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_classification_cross_entropy <loss>=0.426624298096\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=0 train binary_f_1.000 <score>=0.228571428571\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:05:38 INFO 140146317055808] Iter[21] Batch [500]#011Speed: 117065.53 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=500 train binary_classification_accuracy <score>=0.824341317365\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=500 train binary_classification_cross_entropy <loss>=0.41105928737\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=500 train binary_f_1.000 <score>=0.261696826316\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:42 INFO 140146317055808] Iter[21] Batch [1000]#011Speed: 118590.53 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1000 train binary_classification_accuracy <score>=0.823863136863\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1000 train binary_classification_cross_entropy <loss>=0.41167453823\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1000 train binary_f_1.000 <score>=0.261420331017\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:46 INFO 140146317055808] Iter[21] Batch [1500]#011Speed: 119240.22 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1500 train binary_classification_accuracy <score>=0.824152564957\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1500 train binary_classification_cross_entropy <loss>=0.411141701248\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=1500 train binary_f_1.000 <score>=0.261486333355\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:50 INFO 140146317055808] Iter[21] Batch [2000]#011Speed: 120038.51 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=2000 train binary_classification_accuracy <score>=0.823993003498\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=2000 train binary_classification_cross_entropy <loss>=0.411319884964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, batch=2000 train binary_f_1.000 <score>=0.262382976941\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, train binary_classification_accuracy <score>=0.824024409764\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, train binary_classification_cross_entropy <loss>=0.411183323366\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=21, train binary_f_1.000 <score>=0.262539806581\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20950.223922729492, \"sum\": 20950.223922729492, \"min\": 20950.223922729492}}, \"EndTime\": 1553540754.735345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540733.784673}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 54979, \"sum\": 54979.0, \"min\": 54979}, \"Total Records Seen\": {\"count\": 1, \"max\": 54972136, \"sum\": 54972136.0, \"min\": 54972136}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 23, \"sum\": 23.0, \"min\": 23}}, \"EndTime\": 1553540754.735596, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 21}, \"StartTime\": 1553540733.78509}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119265.507654 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:05:54.735] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 20948, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_classification_accuracy <score>=0.813\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_classification_cross_entropy <loss>=0.425513977051\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=0 train binary_f_1.000 <score>=0.242914979757\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:58 INFO 140146317055808] Iter[22] Batch [500]#011Speed: 119177.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=500 train binary_classification_accuracy <score>=0.824768463074\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=500 train binary_classification_cross_entropy <loss>=0.410024744213\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:05:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=500 train binary_f_1.000 <score>=0.266446076588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:03 INFO 140146317055808] Iter[22] Batch [1000]#011Speed: 119347.33 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1000 train binary_classification_accuracy <score>=0.824254745255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1000 train binary_classification_cross_entropy <loss>=0.410640914231\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1000 train binary_f_1.000 <score>=0.265959000421\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:07 INFO 140146317055808] Iter[22] Batch [1500]#011Speed: 119549.07 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1500 train binary_classification_accuracy <score>=0.824527648235\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1500 train binary_classification_cross_entropy <loss>=0.410114785008\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=1500 train binary_f_1.000 <score>=0.265894052656\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:11 INFO 140146317055808] Iter[22] Batch [2000]#011Speed: 119849.07 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=2000 train binary_classification_accuracy <score>=0.824374812594\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=2000 train binary_classification_cross_entropy <loss>=0.410295654312\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, batch=2000 train binary_f_1.000 <score>=0.266781001978\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, train binary_classification_accuracy <score>=0.824420168067\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, train binary_classification_cross_entropy <loss>=0.410160712503\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=22, train binary_f_1.000 <score>=0.267029053296\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20970.659017562866, \"sum\": 20970.659017562866, \"min\": 20970.659017562866}}, \"EndTime\": 1553540775.706556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540754.735413}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 57478, \"sum\": 57478.0, \"min\": 57478}, \"Total Records Seen\": {\"count\": 1, \"max\": 57470824, \"sum\": 57470824.0, \"min\": 57470824}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1553540775.706747, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 22}, \"StartTime\": 1553540754.735869}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119149.756294 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:06:15.706] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 20969, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_classification_accuracy <score>=0.816\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_classification_cross_entropy <loss>=0.424439758301\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=0 train binary_f_1.000 <score>=0.258064516129\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:06:19 INFO 140146317055808] Iter[23] Batch [500]#011Speed: 120863.30 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=500 train binary_classification_accuracy <score>=0.82523752495\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=500 train binary_classification_cross_entropy <loss>=0.409025451477\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=500 train binary_f_1.000 <score>=0.271107707164\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:23 INFO 140146317055808] Iter[23] Batch [1000]#011Speed: 121923.54 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1000 train binary_classification_accuracy <score>=0.824677322677\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1000 train binary_classification_cross_entropy <loss>=0.409642023717\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1000 train binary_f_1.000 <score>=0.270357466552\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:28 INFO 140146317055808] Iter[23] Batch [1500]#011Speed: 121027.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1500 train binary_classification_accuracy <score>=0.824941372418\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1500 train binary_classification_cross_entropy <loss>=0.409122197547\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=1500 train binary_f_1.000 <score>=0.270335864932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:32 INFO 140146317055808] Iter[23] Batch [2000]#011Speed: 120999.40 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=2000 train binary_classification_accuracy <score>=0.824776111944\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=2000 train binary_classification_cross_entropy <loss>=0.40930550388\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, batch=2000 train binary_f_1.000 <score>=0.271140426187\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, train binary_classification_accuracy <score>=0.824822328932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, train binary_classification_cross_entropy <loss>=0.409171981451\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=23, train binary_f_1.000 <score>=0.271357737421\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20640.323877334595, \"sum\": 20640.323877334595, \"min\": 20640.323877334595}}, \"EndTime\": 1553540796.347333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540775.706615}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 59977, \"sum\": 59977.0, \"min\": 59977}, \"Total Records Seen\": {\"count\": 1, \"max\": 59969512, \"sum\": 59969512.0, \"min\": 59969512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}}, \"EndTime\": 1553540796.347529, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 23}, \"StartTime\": 1553540775.706981}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=121056.646978 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:06:36.347] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 20639, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_classification_cross_entropy <loss>=0.423398376465\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:40 INFO 140146317055808] Iter[24] Batch [500]#011Speed: 110738.93 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=500 train binary_classification_accuracy <score>=0.825656686627\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=500 train binary_classification_cross_entropy <loss>=0.408058092982\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=500 train binary_f_1.000 <score>=0.275545750116\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:45 INFO 140146317055808] Iter[24] Batch [1000]#011Speed: 103249.30 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1000 train binary_classification_accuracy <score>=0.825065934066\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1000 train binary_classification_cross_entropy <loss>=0.408674607924\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1000 train binary_f_1.000 <score>=0.274577549101\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:50 INFO 140146317055808] Iter[24] Batch [1500]#011Speed: 108664.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1500 train binary_classification_accuracy <score>=0.825321785476\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1500 train binary_classification_cross_entropy <loss>=0.408160730188\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=1500 train binary_f_1.000 <score>=0.274474102197\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:54 INFO 140146317055808] Iter[24] Batch [2000]#011Speed: 118855.50 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=2000 train binary_classification_accuracy <score>=0.825144427786\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=2000 train binary_classification_cross_entropy <loss>=0.408346257063\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, batch=2000 train binary_f_1.000 <score>=0.275259231027\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, train binary_classification_accuracy <score>=0.825191676671\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, train binary_classification_cross_entropy <loss>=0.408213993837\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=24, train binary_f_1.000 <score>=0.275396598969\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22357.903957366943, \"sum\": 22357.903957366943, \"min\": 22357.903957366943}}, \"EndTime\": 1553540818.705682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540796.347395}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 62476, \"sum\": 62476.0, \"min\": 62476}, \"Total Records Seen\": {\"count\": 1, \"max\": 62468200, \"sum\": 62468200.0, \"min\": 62468200}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1553540818.705919, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 24}, \"StartTime\": 1553540796.347747}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=111756.618494 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:06:58.706] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 25, \"duration\": 22356, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=0 train binary_classification_cross_entropy <loss>=0.422387268066\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:06:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:07:02 INFO 140146317055808] Iter[25] Batch [500]#011Speed: 119092.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=500 train binary_classification_accuracy <score>=0.826063872255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=500 train binary_classification_cross_entropy <loss>=0.407119804154\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=500 train binary_f_1.000 <score>=0.279591937964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:07 INFO 140146317055808] Iter[25] Batch [1000]#011Speed: 119305.12 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1000 train binary_classification_accuracy <score>=0.825467532468\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1000 train binary_classification_cross_entropy <loss>=0.407735857526\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1000 train binary_f_1.000 <score>=0.278740840128\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:11 INFO 140146317055808] Iter[25] Batch [1500]#011Speed: 119489.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1500 train binary_classification_accuracy <score>=0.825707528314\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1500 train binary_classification_cross_entropy <loss>=0.407227616566\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=1500 train binary_f_1.000 <score>=0.278573654356\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:15 INFO 140146317055808] Iter[25] Batch [2000]#011Speed: 121299.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=2000 train binary_classification_accuracy <score>=0.825538730635\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=2000 train binary_classification_cross_entropy <loss>=0.40741517585\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, batch=2000 train binary_f_1.000 <score>=0.279393702949\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, train binary_classification_accuracy <score>=0.825595438175\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, train binary_classification_cross_entropy <loss>=0.40728404532\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=25, train binary_f_1.000 <score>=0.279590403068\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20902.727842330933, \"sum\": 20902.727842330933, \"min\": 20902.727842330933}}, \"EndTime\": 1553540839.608941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540818.705748}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 64975, \"sum\": 64975.0, \"min\": 64975}, \"Total Records Seen\": {\"count\": 1, \"max\": 64966888, \"sum\": 64966888.0, \"min\": 64966888}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 27, \"sum\": 27.0, \"min\": 27}}, \"EndTime\": 1553540839.609124, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 25}, \"StartTime\": 1553540818.706183}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119536.970226 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:07:19.609] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 20901, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=0 train binary_classification_cross_entropy <loss>=0.421404174805\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:23 INFO 140146317055808] Iter[26] Batch [500]#011Speed: 118956.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=500 train binary_classification_accuracy <score>=0.826421157685\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=500 train binary_classification_cross_entropy <loss>=0.406208123\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=500 train binary_f_1.000 <score>=0.283541634055\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:28 INFO 140146317055808] Iter[26] Batch [1000]#011Speed: 119508.10 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1000 train binary_classification_accuracy <score>=0.825812187812\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1000 train binary_classification_cross_entropy <loss>=0.406823348161\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1000 train binary_f_1.000 <score>=0.282596710088\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:32 INFO 140146317055808] Iter[26] Batch [1500]#011Speed: 120067.74 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1500 train binary_classification_accuracy <score>=0.826059960027\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1500 train binary_classification_cross_entropy <loss>=0.406320467812\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=1500 train binary_f_1.000 <score>=0.282444510405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:36 INFO 140146317055808] Iter[26] Batch [2000]#011Speed: 120734.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=2000 train binary_classification_accuracy <score>=0.825890054973\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=2000 train binary_classification_cross_entropy <loss>=0.406509892118\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, batch=2000 train binary_f_1.000 <score>=0.28319891367\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, train binary_classification_accuracy <score>=0.825957583033\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, train binary_classification_cross_entropy <loss>=0.406379791687\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=26, train binary_f_1.000 <score>=0.283404345061\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20807.095050811768, \"sum\": 20807.095050811768, \"min\": 20807.095050811768}}, \"EndTime\": 1553540860.416491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540839.609001}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 67474, \"sum\": 67474.0, \"min\": 67474}, \"Total Records Seen\": {\"count\": 1, \"max\": 67465576, \"sum\": 67465576.0, \"min\": 67465576}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1553540860.41675, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 26}, \"StartTime\": 1553540839.609354}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120085.804624 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:07:40.416] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 27, \"duration\": 20806, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=0 train binary_classification_cross_entropy <loss>=0.420447052002\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:44 INFO 140146317055808] Iter[27] Batch [500]#011Speed: 120421.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=500 train binary_classification_accuracy <score>=0.826706586826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=500 train binary_classification_cross_entropy <loss>=0.405320896902\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=500 train binary_f_1.000 <score>=0.286875954857\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:07:48 INFO 140146317055808] Iter[27] Batch [1000]#011Speed: 117799.51 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1000 train binary_classification_accuracy <score>=0.826130869131\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1000 train binary_classification_cross_entropy <loss>=0.405934962133\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1000 train binary_f_1.000 <score>=0.286161935582\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:52 INFO 140146317055808] Iter[27] Batch [1500]#011Speed: 120770.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1500 train binary_classification_accuracy <score>=0.826415056629\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1500 train binary_classification_cross_entropy <loss>=0.405437194702\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=1500 train binary_f_1.000 <score>=0.286151865094\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:57 INFO 140146317055808] Iter[27] Batch [2000]#011Speed: 121413.04 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=2000 train binary_classification_accuracy <score>=0.82623888056\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=2000 train binary_classification_cross_entropy <loss>=0.405628335201\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:07:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, batch=2000 train binary_f_1.000 <score>=0.286906158478\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, train binary_classification_accuracy <score>=0.82631012405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, train binary_classification_cross_entropy <loss>=0.405499182998\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=27, train binary_f_1.000 <score>=0.287084105979\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20816.14398956299, \"sum\": 20816.14398956299, \"min\": 20816.14398956299}}, \"EndTime\": 1553540881.23321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540860.416589}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 69973, \"sum\": 69973.0, \"min\": 69973}, \"Total Records Seen\": {\"count\": 1, \"max\": 69964264, \"sum\": 69964264.0, \"min\": 69964264}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 29, \"sum\": 29.0, \"min\": 29}}, \"EndTime\": 1553540881.23344, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 27}, \"StartTime\": 1553540860.417034}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120033.804185 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:08:01.233] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 20814, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=0 train binary_classification_cross_entropy <loss>=0.419514343262\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:05 INFO 140146317055808] Iter[28] Batch [500]#011Speed: 119549.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=500 train binary_classification_accuracy <score>=0.827091816367\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=500 train binary_classification_cross_entropy <loss>=0.404456242361\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=500 train binary_f_1.000 <score>=0.290692628286\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:09 INFO 140146317055808] Iter[28] Batch [1000]#011Speed: 121987.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1000 train binary_classification_accuracy <score>=0.82646953047\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1000 train binary_classification_cross_entropy <loss>=0.405068842382\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1000 train binary_f_1.000 <score>=0.289652972593\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:13 INFO 140146317055808] Iter[28] Batch [1500]#011Speed: 121936.17 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1500 train binary_classification_accuracy <score>=0.826748167888\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1500 train binary_classification_cross_entropy <loss>=0.404575966222\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=1500 train binary_f_1.000 <score>=0.289549963528\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:17 INFO 140146317055808] Iter[28] Batch [2000]#011Speed: 119711.31 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=2000 train binary_classification_accuracy <score>=0.826570214893\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=2000 train binary_classification_cross_entropy <loss>=0.404768688898\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, batch=2000 train binary_f_1.000 <score>=0.290302196173\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, train binary_classification_accuracy <score>=0.826640656263\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, train binary_classification_cross_entropy <loss>=0.404640417715\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=28, train binary_f_1.000 <score>=0.290494119728\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20683.380842208862, \"sum\": 20683.380842208862, \"min\": 20683.380842208862}}, \"EndTime\": 1553540901.917131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540881.233287}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 72472, \"sum\": 72472.0, \"min\": 72472}, \"Total Records Seen\": {\"count\": 1, \"max\": 72462952, \"sum\": 72462952.0, \"min\": 72462952}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1553540901.917381, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 28}, \"StartTime\": 1553540881.233717}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120804.142479 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:08:21.917] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 20682, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=0 train binary_classification_cross_entropy <loss>=0.418604370117\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:26 INFO 140146317055808] Iter[29] Batch [500]#011Speed: 120131.20 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=500 train binary_classification_accuracy <score>=0.827403193613\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=500 train binary_classification_cross_entropy <loss>=0.403612493214\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=500 train binary_f_1.000 <score>=0.293877950988\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:08:30 INFO 140146317055808] Iter[29] Batch [1000]#011Speed: 121422.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1000 train binary_classification_accuracy <score>=0.826798201798\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1000 train binary_classification_cross_entropy <loss>=0.404223350776\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1000 train binary_f_1.000 <score>=0.292996227954\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:34 INFO 140146317055808] Iter[29] Batch [1500]#011Speed: 121130.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1500 train binary_classification_accuracy <score>=0.827064623584\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1500 train binary_classification_cross_entropy <loss>=0.403735163882\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=1500 train binary_f_1.000 <score>=0.292847179845\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:38 INFO 140146317055808] Iter[29] Batch [2000]#011Speed: 119016.22 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=2000 train binary_classification_accuracy <score>=0.82688005997\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=2000 train binary_classification_cross_entropy <loss>=0.403929346713\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, batch=2000 train binary_f_1.000 <score>=0.293509052085\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, train binary_classification_accuracy <score>=0.826963185274\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, train binary_classification_cross_entropy <loss>=0.403801901425\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=29, train binary_f_1.000 <score>=0.293775733018\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20765.63286781311, \"sum\": 20765.63286781311, \"min\": 20765.63286781311}}, \"EndTime\": 1553540922.683386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540901.91721}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 74971, \"sum\": 74971.0, \"min\": 74971}, \"Total Records Seen\": {\"count\": 1, \"max\": 74961640, \"sum\": 74961640.0, \"min\": 74961640}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 31, \"sum\": 31.0, \"min\": 31}}, \"EndTime\": 1553540922.683557, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 29}, \"StartTime\": 1553540901.917722}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120326.171371 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:08:42.683] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 20764, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=0 train binary_classification_accuracy <score>=0.817\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=0 train binary_classification_cross_entropy <loss>=0.4177159729\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=0 train binary_f_1.000 <score>=0.265060240964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:46 INFO 140146317055808] Iter[30] Batch [500]#011Speed: 119697.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=500 train binary_classification_accuracy <score>=0.827670658683\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=500 train binary_classification_cross_entropy <loss>=0.402788161653\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=500 train binary_f_1.000 <score>=0.296970042424\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:51 INFO 140146317055808] Iter[30] Batch [1000]#011Speed: 119250.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1000 train binary_classification_accuracy <score>=0.827117882118\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1000 train binary_classification_cross_entropy <loss>=0.403397021192\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1000 train binary_f_1.000 <score>=0.296315573321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:55 INFO 140146317055808] Iter[30] Batch [1500]#011Speed: 120350.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1500 train binary_classification_accuracy <score>=0.827396402398\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1500 train binary_classification_cross_entropy <loss>=0.402913340813\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=1500 train binary_f_1.000 <score>=0.296255249333\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:59 INFO 140146317055808] Iter[30] Batch [2000]#011Speed: 118868.41 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=2000 train binary_classification_accuracy <score>=0.827205397301\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=2000 train binary_classification_cross_entropy <loss>=0.403108873923\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:08:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, batch=2000 train binary_f_1.000 <score>=0.296868708642\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, train binary_classification_accuracy <score>=0.827300920368\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, train binary_classification_cross_entropy <loss>=0.402982208594\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=30, train binary_f_1.000 <score>=0.297167499117\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20932.162046432495, \"sum\": 20932.162046432495, \"min\": 20932.162046432495}}, \"EndTime\": 1553540943.616009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540922.683449}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 77470, \"sum\": 77470.0, \"min\": 77470}, \"Total Records Seen\": {\"count\": 1, \"max\": 77460328, \"sum\": 77460328.0, \"min\": 77460328}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1553540943.616202, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 30}, \"StartTime\": 1553540922.683818}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119368.850783 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:09:03.616] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 31, \"duration\": 20930, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=0 train binary_classification_accuracy <score>=0.819\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=0 train binary_classification_cross_entropy <loss>=0.416847839355\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=0 train binary_f_1.000 <score>=0.278884462151\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:07 INFO 140146317055808] Iter[31] Batch [500]#011Speed: 120484.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=500 train binary_classification_accuracy <score>=0.828045908184\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=500 train binary_classification_cross_entropy <loss>=0.401981924335\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=500 train binary_f_1.000 <score>=0.300369513136\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:09:11 INFO 140146317055808] Iter[31] Batch [1000]#011Speed: 119517.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1000 train binary_classification_accuracy <score>=0.827466533467\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1000 train binary_classification_cross_entropy <loss>=0.402588548232\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1000 train binary_f_1.000 <score>=0.299696694456\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:16 INFO 140146317055808] Iter[31] Batch [1500]#011Speed: 119983.40 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1500 train binary_classification_accuracy <score>=0.82772751499\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1500 train binary_classification_cross_entropy <loss>=0.402109205395\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=1500 train binary_f_1.000 <score>=0.299521331495\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:20 INFO 140146317055808] Iter[31] Batch [2000]#011Speed: 119576.08 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=2000 train binary_classification_accuracy <score>=0.827564717641\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=2000 train binary_classification_cross_entropy <loss>=0.40230598667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, batch=2000 train binary_f_1.000 <score>=0.300281677573\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, train binary_classification_accuracy <score>=0.827663065226\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, train binary_classification_cross_entropy <loss>=0.402180061807\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=31, train binary_f_1.000 <score>=0.300581076209\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20848.11305999756, \"sum\": 20848.11305999756, \"min\": 20848.11305999756}}, \"EndTime\": 1553540964.464568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540943.616077}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 79969, \"sum\": 79969.0, \"min\": 79969}, \"Total Records Seen\": {\"count\": 1, \"max\": 79959016, \"sum\": 79959016.0, \"min\": 79959016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 33, \"sum\": 33.0, \"min\": 33}}, \"EndTime\": 1553540964.464762, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 31}, \"StartTime\": 1553540943.616426}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119850.061109 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:09:24.464] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 20847, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=0 train binary_classification_accuracy <score>=0.819\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=0 train binary_classification_cross_entropy <loss>=0.41599887085\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=0 train binary_f_1.000 <score>=0.284584980237\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:28 INFO 140146317055808] Iter[32] Batch [500]#011Speed: 119347.87 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=500 train binary_classification_accuracy <score>=0.828371257485\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=500 train binary_classification_cross_entropy <loss>=0.401192577492\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=500 train binary_f_1.000 <score>=0.30347509113\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:32 INFO 140146317055808] Iter[32] Batch [1000]#011Speed: 118880.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1000 train binary_classification_accuracy <score>=0.827825174825\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1000 train binary_classification_cross_entropy <loss>=0.401796746649\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1000 train binary_f_1.000 <score>=0.302902097195\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:37 INFO 140146317055808] Iter[32] Batch [1500]#011Speed: 120855.74 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1500 train binary_classification_accuracy <score>=0.828060626249\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1500 train binary_classification_cross_entropy <loss>=0.401321588871\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=1500 train binary_f_1.000 <score>=0.302625143552\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:41 INFO 140146317055808] Iter[32] Batch [2000]#011Speed: 119074.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=2000 train binary_classification_accuracy <score>=0.827911044478\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=2000 train binary_classification_cross_entropy <loss>=0.401519525668\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, batch=2000 train binary_f_1.000 <score>=0.303479467461\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, train binary_classification_accuracy <score>=0.828016406563\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, train binary_classification_cross_entropy <loss>=0.401394308089\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=32, train binary_f_1.000 <score>=0.303820678998\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20904.39510345459, \"sum\": 20904.39510345459, \"min\": 20904.39510345459}}, \"EndTime\": 1553540985.369429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540964.464628}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 82468, \"sum\": 82468.0, \"min\": 82468}, \"Total Records Seen\": {\"count\": 1, \"max\": 82457704, \"sum\": 82457704.0, \"min\": 82457704}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1553540985.36964, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 32}, \"StartTime\": 1553540964.465002}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119527.290663 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:09:45.369] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 33, \"duration\": 20903, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=0 train binary_classification_accuracy <score>=0.821\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=0 train binary_classification_cross_entropy <loss>=0.415168029785\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=0 train binary_f_1.000 <score>=0.298039215686\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:09:49 INFO 140146317055808] Iter[33] Batch [500]#011Speed: 118894.79 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=500 train binary_classification_accuracy <score>=0.828690618762\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=500 train binary_classification_cross_entropy <loss>=0.400419041129\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=500 train binary_f_1.000 <score>=0.306445356693\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:53 INFO 140146317055808] Iter[33] Batch [1000]#011Speed: 120492.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1000 train binary_classification_accuracy <score>=0.828114885115\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1000 train binary_classification_cross_entropy <loss>=0.401020551683\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1000 train binary_f_1.000 <score>=0.305799095417\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:57 INFO 140146317055808] Iter[33] Batch [1500]#011Speed: 118458.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1500 train binary_classification_accuracy <score>=0.828379080613\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1500 train binary_classification_cross_entropy <loss>=0.400549434675\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:09:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=1500 train binary_f_1.000 <score>=0.305646676388\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:02 INFO 140146317055808] Iter[33] Batch [2000]#011Speed: 119753.88 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=2000 train binary_classification_accuracy <score>=0.828252873563\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=2000 train binary_classification_cross_entropy <loss>=0.400748441102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, batch=2000 train binary_f_1.000 <score>=0.306627337363\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, train binary_classification_accuracy <score>=0.828368147259\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, train binary_classification_cross_entropy <loss>=0.400623901892\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=33, train binary_f_1.000 <score>=0.307001273194\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20889.286994934082, \"sum\": 20889.286994934082, \"min\": 20889.286994934082}}, \"EndTime\": 1553541006.259182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553540985.369506}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 84967, \"sum\": 84967.0, \"min\": 84967}, \"Total Records Seen\": {\"count\": 1, \"max\": 84956392, \"sum\": 84956392.0, \"min\": 84956392}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 35, \"sum\": 35.0, \"min\": 35}}, \"EndTime\": 1553541006.259369, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 33}, \"StartTime\": 1553540985.369867}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119613.948525 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:10:06.259] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 20888, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=0 train binary_classification_accuracy <score>=0.822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=0 train binary_classification_cross_entropy <loss>=0.414354400635\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=0 train binary_f_1.000 <score>=0.31007751938\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:10 INFO 140146317055808] Iter[34] Batch [500]#011Speed: 119346.86 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=500 train binary_classification_accuracy <score>=0.828968063872\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=500 train binary_classification_cross_entropy <loss>=0.399660322361\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=500 train binary_f_1.000 <score>=0.309371247109\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:14 INFO 140146317055808] Iter[34] Batch [1000]#011Speed: 119394.69 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1000 train binary_classification_accuracy <score>=0.828418581419\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1000 train binary_classification_cross_entropy <loss>=0.400258983448\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1000 train binary_f_1.000 <score>=0.30871586404\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:18 INFO 140146317055808] Iter[34] Batch [1500]#011Speed: 119186.76 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1500 train binary_classification_accuracy <score>=0.828690872751\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1500 train binary_classification_cross_entropy <loss>=0.399791776694\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=1500 train binary_f_1.000 <score>=0.308574286552\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:23 INFO 140146317055808] Iter[34] Batch [2000]#011Speed: 118952.04 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=2000 train binary_classification_accuracy <score>=0.828566716642\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=2000 train binary_classification_cross_entropy <loss>=0.399991773934\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, batch=2000 train binary_f_1.000 <score>=0.309593772139\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, train binary_classification_accuracy <score>=0.828675070028\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, train binary_classification_cross_entropy <loss>=0.399867887853\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=34, train binary_f_1.000 <score>=0.309938607986\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21064.412117004395, \"sum\": 21064.412117004395, \"min\": 21064.412117004395}}, \"EndTime\": 1553541027.324028, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541006.259242}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 87466, \"sum\": 87466.0, \"min\": 87466}, \"Total Records Seen\": {\"count\": 1, \"max\": 87455080, \"sum\": 87455080.0, \"min\": 87455080}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1553541027.324222, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 34}, \"StartTime\": 1553541006.259588}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118619.484538 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:10:27.324] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 21062, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=0 train binary_classification_accuracy <score>=0.822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=0 train binary_classification_cross_entropy <loss>=0.413557067871\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=0 train binary_f_1.000 <score>=0.31007751938\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:10:31 INFO 140146317055808] Iter[35] Batch [500]#011Speed: 118850.47 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=500 train binary_classification_accuracy <score>=0.829299401198\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=500 train binary_classification_cross_entropy <loss>=0.398915520339\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=500 train binary_f_1.000 <score>=0.312460305337\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:35 INFO 140146317055808] Iter[35] Batch [1000]#011Speed: 120318.52 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1000 train binary_classification_accuracy <score>=0.828737262737\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1000 train binary_classification_cross_entropy <loss>=0.399511153062\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1000 train binary_f_1.000 <score>=0.311836157965\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:39 INFO 140146317055808] Iter[35] Batch [1500]#011Speed: 118530.82 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1500 train binary_classification_accuracy <score>=0.829012658228\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1500 train binary_classification_cross_entropy <loss>=0.399047732298\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=1500 train binary_f_1.000 <score>=0.311644423465\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:44 INFO 140146317055808] Iter[35] Batch [2000]#011Speed: 119204.15 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=2000 train binary_classification_accuracy <score>=0.828875562219\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=2000 train binary_classification_cross_entropy <loss>=0.399248647283\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, batch=2000 train binary_f_1.000 <score>=0.312569762866\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, train binary_classification_accuracy <score>=0.828983993597\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, train binary_classification_cross_entropy <loss>=0.399125391038\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=35, train binary_f_1.000 <score>=0.312861862997\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20980.496883392334, \"sum\": 20980.496883392334, \"min\": 20980.496883392334}}, \"EndTime\": 1553541048.304975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541027.324098}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 89965, \"sum\": 89965.0, \"min\": 89965}, \"Total Records Seen\": {\"count\": 1, \"max\": 89953768, \"sum\": 89953768.0, \"min\": 89953768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 37, \"sum\": 37.0, \"min\": 37}}, \"EndTime\": 1553541048.305173, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 35}, \"StartTime\": 1553541027.324448}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119093.716696 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:10:48.305] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 20979, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=0 train binary_classification_accuracy <score>=0.822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=0 train binary_classification_cross_entropy <loss>=0.41277520752\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=0 train binary_f_1.000 <score>=0.31007751938\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:52 INFO 140146317055808] Iter[36] Batch [500]#011Speed: 117789.90 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=500 train binary_classification_accuracy <score>=0.829614770459\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=500 train binary_classification_cross_entropy <loss>=0.398183796591\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=500 train binary_f_1.000 <score>=0.315282869037\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:56 INFO 140146317055808] Iter[36] Batch [1000]#011Speed: 117184.33 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1000 train binary_classification_accuracy <score>=0.829073926074\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1000 train binary_classification_cross_entropy <loss>=0.398776236733\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:10:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1000 train binary_f_1.000 <score>=0.314847369665\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:01 INFO 140146317055808] Iter[36] Batch [1500]#011Speed: 119402.22 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1500 train binary_classification_accuracy <score>=0.829325116589\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1500 train binary_classification_cross_entropy <loss>=0.398316487596\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=1500 train binary_f_1.000 <score>=0.314525688262\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:05 INFO 140146317055808] Iter[36] Batch [2000]#011Speed: 119354.82 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=2000 train binary_classification_accuracy <score>=0.829186906547\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=2000 train binary_classification_cross_entropy <loss>=0.398518253828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, batch=2000 train binary_f_1.000 <score>=0.315416091669\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, train binary_classification_accuracy <score>=0.829276910764\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, train binary_classification_cross_entropy <loss>=0.398395607535\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=36, train binary_f_1.000 <score>=0.315603559323\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21018.215894699097, \"sum\": 21018.215894699097, \"min\": 21018.215894699097}}, \"EndTime\": 1553541069.323732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541048.30504}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 92464, \"sum\": 92464.0, \"min\": 92464}, \"Total Records Seen\": {\"count\": 1, \"max\": 92452456, \"sum\": 92452456.0, \"min\": 92452456}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1553541069.323936, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 36}, \"StartTime\": 1553541048.305461}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118879.934304 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:11:09.324] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 37, \"duration\": 21016, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=0 train binary_classification_accuracy <score>=0.822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=0 train binary_classification_cross_entropy <loss>=0.412007904053\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=0 train binary_f_1.000 <score>=0.31007751938\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:13 INFO 140146317055808] Iter[37] Batch [500]#011Speed: 118349.98 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=500 train binary_classification_accuracy <score>=0.829916167665\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=500 train binary_classification_cross_entropy <loss>=0.397464389215\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=500 train binary_f_1.000 <score>=0.318325813574\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:11:17 INFO 140146317055808] Iter[37] Batch [1000]#011Speed: 120773.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1000 train binary_classification_accuracy <score>=0.829377622378\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1000 train binary_classification_cross_entropy <loss>=0.398053479845\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1000 train binary_f_1.000 <score>=0.317758577301\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:21 INFO 140146317055808] Iter[37] Batch [1500]#011Speed: 120122.43 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1500 train binary_classification_accuracy <score>=0.829623584277\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1500 train binary_classification_cross_entropy <loss>=0.397597293312\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=1500 train binary_f_1.000 <score>=0.317353712933\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:26 INFO 140146317055808] Iter[37] Batch [2000]#011Speed: 120216.03 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=2000 train binary_classification_accuracy <score>=0.82951974013\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=2000 train binary_classification_cross_entropy <loss>=0.39779984916\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, batch=2000 train binary_f_1.000 <score>=0.318366466118\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, train binary_classification_accuracy <score>=0.829621448579\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, train binary_classification_cross_entropy <loss>=0.397677794873\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=37, train binary_f_1.000 <score>=0.318605767692\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20842.809915542603, \"sum\": 20842.809915542603, \"min\": 20842.809915542603}}, \"EndTime\": 1553541090.166997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541069.323806}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 94963, \"sum\": 94963.0, \"min\": 94963}, \"Total Records Seen\": {\"count\": 1, \"max\": 94951144, \"sum\": 94951144.0, \"min\": 94951144}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 39, \"sum\": 39.0, \"min\": 39}}, \"EndTime\": 1553541090.167225, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 37}, \"StartTime\": 1553541069.324155}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119880.276322 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:11:30.167] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 20841, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=0 train binary_classification_accuracy <score>=0.823\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=0 train binary_classification_cross_entropy <loss>=0.411254516602\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=0 train binary_f_1.000 <score>=0.316602316602\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:34 INFO 140146317055808] Iter[38] Batch [500]#011Speed: 119953.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=500 train binary_classification_accuracy <score>=0.830169660679\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=500 train binary_classification_cross_entropy <loss>=0.396756582173\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=500 train binary_f_1.000 <score>=0.320521318309\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:38 INFO 140146317055808] Iter[38] Batch [1000]#011Speed: 121892.68 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1000 train binary_classification_accuracy <score>=0.829662337662\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1000 train binary_classification_cross_entropy <loss>=0.39734217836\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1000 train binary_f_1.000 <score>=0.320295308862\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:42 INFO 140146317055808] Iter[38] Batch [1500]#011Speed: 120612.21 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1500 train binary_classification_accuracy <score>=0.829938707528\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1500 train binary_classification_cross_entropy <loss>=0.39688945419\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=1500 train binary_f_1.000 <score>=0.320044751072\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:46 INFO 140146317055808] Iter[38] Batch [2000]#011Speed: 120500.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=2000 train binary_classification_accuracy <score>=0.829824087956\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=2000 train binary_classification_cross_entropy <loss>=0.397092742783\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, batch=2000 train binary_f_1.000 <score>=0.321033637071\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, train binary_classification_accuracy <score>=0.829934373749\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, train binary_classification_cross_entropy <loss>=0.39697126436\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=38, train binary_f_1.000 <score>=0.321340857773\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20758.810997009277, \"sum\": 20758.810997009277, \"min\": 20758.810997009277}}, \"EndTime\": 1553541110.926351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541090.167073}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 97462, \"sum\": 97462.0, \"min\": 97462}, \"Total Records Seen\": {\"count\": 1, \"max\": 97449832, \"sum\": 97449832.0, \"min\": 97449832}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1553541110.926615, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 38}, \"StartTime\": 1553541090.167511}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120365.182142 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:11:50.926] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 39, \"duration\": 20757, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=0 train binary_classification_accuracy <score>=0.822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=0 train binary_classification_cross_entropy <loss>=0.410514251709\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=0 train binary_f_1.000 <score>=0.315384615385\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:55 INFO 140146317055808] Iter[39] Batch [500]#011Speed: 119863.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=500 train binary_classification_accuracy <score>=0.830479041916\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=500 train binary_classification_cross_entropy <loss>=0.396059715469\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=500 train binary_f_1.000 <score>=0.323331633629\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:11:59 INFO 140146317055808] Iter[39] Batch [1000]#011Speed: 118861.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1000 train binary_classification_accuracy <score>=0.82998001998\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1000 train binary_classification_cross_entropy <loss>=0.396641680555\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:11:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1000 train binary_f_1.000 <score>=0.323047182645\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:03 INFO 140146317055808] Iter[39] Batch [1500]#011Speed: 120665.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1500 train binary_classification_accuracy <score>=0.830243171219\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1500 train binary_classification_cross_entropy <loss>=0.396192322448\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=1500 train binary_f_1.000 <score>=0.322645988458\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:07 INFO 140146317055808] Iter[39] Batch [2000]#011Speed: 119536.44 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=2000 train binary_classification_accuracy <score>=0.830133433283\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=2000 train binary_classification_cross_entropy <loss>=0.396396292104\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, batch=2000 train binary_f_1.000 <score>=0.323663605703\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, train binary_classification_accuracy <score>=0.830246098439\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, train binary_classification_cross_entropy <loss>=0.396275375433\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=39, train binary_f_1.000 <score>=0.323984937604\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20866.518020629883, \"sum\": 20866.518020629883, \"min\": 20866.518020629883}}, \"EndTime\": 1553541131.79341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541110.926413}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 99961, \"sum\": 99961.0, \"min\": 99961}, \"Total Records Seen\": {\"count\": 1, \"max\": 99948520, \"sum\": 99948520.0, \"min\": 99948520}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 41, \"sum\": 41.0, \"min\": 41}}, \"EndTime\": 1553541131.793643, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 39}, \"StartTime\": 1553541110.926859}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119744.056935 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:12:11.793] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 20865, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=0 train binary_classification_accuracy <score>=0.824\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=0 train binary_classification_cross_entropy <loss>=0.409786437988\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=0 train binary_f_1.000 <score>=0.328244274809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:16 INFO 140146317055808] Iter[40] Batch [500]#011Speed: 118172.55 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=500 train binary_classification_accuracy <score>=0.83081237525\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=500 train binary_classification_cross_entropy <loss>=0.395373172844\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=500 train binary_f_1.000 <score>=0.326224334873\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:20 INFO 140146317055808] Iter[40] Batch [1000]#011Speed: 120977.29 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1000 train binary_classification_accuracy <score>=0.830302697303\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1000 train binary_classification_cross_entropy <loss>=0.395951381657\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1000 train binary_f_1.000 <score>=0.325954025816\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:24 INFO 140146317055808] Iter[40] Batch [1500]#011Speed: 119451.48 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1500 train binary_classification_accuracy <score>=0.830574283811\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1500 train binary_classification_cross_entropy <loss>=0.395505299588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:24 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=1500 train binary_f_1.000 <score>=0.325596813459\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:28 INFO 140146317055808] Iter[40] Batch [2000]#011Speed: 118955.94 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=2000 train binary_classification_accuracy <score>=0.830453273363\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=2000 train binary_classification_cross_entropy <loss>=0.39570990161\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, batch=2000 train binary_f_1.000 <score>=0.326518377354\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, train binary_classification_accuracy <score>=0.830552220888\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, train binary_classification_cross_entropy <loss>=0.395589534202\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=40, train binary_f_1.000 <score>=0.326732893657\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20928.067922592163, \"sum\": 20928.067922592163, \"min\": 20928.067922592163}}, \"EndTime\": 1553541152.722025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541131.793487}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 102460, \"sum\": 102460.0, \"min\": 102460}, \"Total Records Seen\": {\"count\": 1, \"max\": 102447208, \"sum\": 102447208.0, \"min\": 102447208}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1553541152.722247, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 40}, \"StartTime\": 1553541131.793926}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119391.899036 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:12:32.722] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 20926, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=0 train binary_classification_cross_entropy <loss>=0.409070404053\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:37 INFO 140146317055808] Iter[41] Batch [500]#011Speed: 117268.63 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=500 train binary_classification_accuracy <score>=0.831105788423\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=500 train binary_classification_cross_entropy <loss>=0.394696377387\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=500 train binary_f_1.000 <score>=0.32875343096\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:12:41 INFO 140146317055808] Iter[41] Batch [1000]#011Speed: 118230.11 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1000 train binary_classification_accuracy <score>=0.830616383616\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1000 train binary_classification_cross_entropy <loss>=0.395270709363\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1000 train binary_f_1.000 <score>=0.328638572011\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:45 INFO 140146317055808] Iter[41] Batch [1500]#011Speed: 120380.20 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1500 train binary_classification_accuracy <score>=0.830864090606\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1500 train binary_classification_cross_entropy <loss>=0.394827817669\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=1500 train binary_f_1.000 <score>=0.328134800179\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:49 INFO 140146317055808] Iter[41] Batch [2000]#011Speed: 119992.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=2000 train binary_classification_accuracy <score>=0.830729135432\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=2000 train binary_classification_cross_entropy <loss>=0.395033009618\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, batch=2000 train binary_f_1.000 <score>=0.328996816454\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, train binary_classification_accuracy <score>=0.830829531813\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, train binary_classification_cross_entropy <loss>=0.394913181056\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=41, train binary_f_1.000 <score>=0.32922651697\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20994.10080909729, \"sum\": 20994.10080909729, \"min\": 20994.10080909729}}, \"EndTime\": 1553541173.716654, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541152.722093}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 104959, \"sum\": 104959.0, \"min\": 104959}, \"Total Records Seen\": {\"count\": 1, \"max\": 104945896, \"sum\": 104945896.0, \"min\": 104945896}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 43, \"sum\": 43.0, \"min\": 43}}, \"EndTime\": 1553541173.716849, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 41}, \"StartTime\": 1553541152.722525}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119016.68187 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:12:53.717] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 20993, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=0 train binary_classification_cross_entropy <loss>=0.40836541748\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:57 INFO 140146317055808] Iter[42] Batch [500]#011Speed: 117686.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=500 train binary_classification_accuracy <score>=0.831457085828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=500 train binary_classification_cross_entropy <loss>=0.394028790259\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:12:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=500 train binary_f_1.000 <score>=0.331718821723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:02 INFO 140146317055808] Iter[42] Batch [1000]#011Speed: 120092.11 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1000 train binary_classification_accuracy <score>=0.830946053946\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1000 train binary_classification_cross_entropy <loss>=0.394599134996\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1000 train binary_f_1.000 <score>=0.33133789321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:06 INFO 140146317055808] Iter[42] Batch [1500]#011Speed: 121180.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1500 train binary_classification_accuracy <score>=0.831156562292\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1500 train binary_classification_cross_entropy <loss>=0.394159353562\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=1500 train binary_f_1.000 <score>=0.330658792706\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:10 INFO 140146317055808] Iter[42] Batch [2000]#011Speed: 119789.40 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=2000 train binary_classification_accuracy <score>=0.831007996002\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=2000 train binary_classification_cross_entropy <loss>=0.394365094659\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, batch=2000 train binary_f_1.000 <score>=0.331413499732\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, train binary_classification_accuracy <score>=0.831109243697\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, train binary_classification_cross_entropy <loss>=0.394245795718\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=42, train binary_f_1.000 <score>=0.331668540482\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20831.040143966675, \"sum\": 20831.040143966675, \"min\": 20831.040143966675}}, \"EndTime\": 1553541194.54814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541173.716715}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 107458, \"sum\": 107458.0, \"min\": 107458}, \"Total Records Seen\": {\"count\": 1, \"max\": 107444584, \"sum\": 107444584.0, \"min\": 107444584}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1553541194.548327, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 42}, \"StartTime\": 1553541173.717072}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119948.350605 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:13:14.548] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 43, \"duration\": 20829, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=0 train binary_classification_cross_entropy <loss>=0.407670959473\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:13:18 INFO 140146317055808] Iter[43] Batch [500]#011Speed: 119650.71 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=500 train binary_classification_accuracy <score>=0.83177245509\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=500 train binary_classification_cross_entropy <loss>=0.39336990259\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=500 train binary_f_1.000 <score>=0.334296952751\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:22 INFO 140146317055808] Iter[43] Batch [1000]#011Speed: 120785.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1000 train binary_classification_accuracy <score>=0.831248751249\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1000 train binary_classification_cross_entropy <loss>=0.393936156922\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1000 train binary_f_1.000 <score>=0.333817103374\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:27 INFO 140146317055808] Iter[43] Batch [1500]#011Speed: 119193.63 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1500 train binary_classification_accuracy <score>=0.831461692205\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1500 train binary_classification_cross_entropy <loss>=0.393499408801\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=1500 train binary_f_1.000 <score>=0.333189239342\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:31 INFO 140146317055808] Iter[43] Batch [2000]#011Speed: 122068.79 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=2000 train binary_classification_accuracy <score>=0.831313343328\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=2000 train binary_classification_cross_entropy <loss>=0.393705663266\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, batch=2000 train binary_f_1.000 <score>=0.333924011666\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, train binary_classification_accuracy <score>=0.831420568227\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, train binary_classification_cross_entropy <loss>=0.393586886256\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=43, train binary_f_1.000 <score>=0.334230971029\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20702.322959899902, \"sum\": 20702.322959899902, \"min\": 20702.322959899902}}, \"EndTime\": 1553541215.250931, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541194.548201}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 109957, \"sum\": 109957.0, \"min\": 109957}, \"Total Records Seen\": {\"count\": 1, \"max\": 109943272, \"sum\": 109943272.0, \"min\": 109943272}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}}, \"EndTime\": 1553541215.251167, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 43}, \"StartTime\": 1553541194.548576}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120693.757802 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:13:35.251] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 20701, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=0 train binary_classification_cross_entropy <loss>=0.406986419678\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:39 INFO 140146317055808] Iter[44] Batch [500]#011Speed: 120028.02 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=500 train binary_classification_accuracy <score>=0.832073852295\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=500 train binary_classification_cross_entropy <loss>=0.392719237185\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=500 train binary_f_1.000 <score>=0.33676260751\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:43 INFO 140146317055808] Iter[44] Batch [1000]#011Speed: 119363.51 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1000 train binary_classification_accuracy <score>=0.831533466533\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1000 train binary_classification_cross_entropy <loss>=0.393281306188\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1000 train binary_f_1.000 <score>=0.336153243553\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:47 INFO 140146317055808] Iter[44] Batch [1500]#011Speed: 119614.59 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1500 train binary_classification_accuracy <score>=0.831762158561\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1500 train binary_classification_cross_entropy <loss>=0.392847518382\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=1500 train binary_f_1.000 <score>=0.335661576102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:51 INFO 140146317055808] Iter[44] Batch [2000]#011Speed: 122389.91 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=2000 train binary_classification_accuracy <score>=0.8315982009\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=2000 train binary_classification_cross_entropy <loss>=0.39305425304\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, batch=2000 train binary_f_1.000 <score>=0.336329617521\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, train binary_classification_accuracy <score>=0.831710684274\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, train binary_classification_cross_entropy <loss>=0.392935992537\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=44, train binary_f_1.000 <score>=0.33665043605\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20766.52693748474, \"sum\": 20766.52693748474, \"min\": 20766.52693748474}}, \"EndTime\": 1553541236.017984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541215.251006}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 112456, \"sum\": 112456.0, \"min\": 112456}, \"Total Records Seen\": {\"count\": 1, \"max\": 112441960, \"sum\": 112441960.0, \"min\": 112441960}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1553541236.018267, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 44}, \"StartTime\": 1553541215.251425}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120320.34314 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:13:56.018] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 45, \"duration\": 20765, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=0 train binary_classification_cross_entropy <loss>=0.406311248779\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:13:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:00 INFO 140146317055808] Iter[45] Batch [500]#011Speed: 117828.85 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=500 train binary_classification_accuracy <score>=0.832353293413\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=500 train binary_classification_cross_entropy <loss>=0.39207634292\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=500 train binary_f_1.000 <score>=0.339179078056\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:14:04 INFO 140146317055808] Iter[45] Batch [1000]#011Speed: 119518.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1000 train binary_classification_accuracy <score>=0.831798201798\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1000 train binary_classification_cross_entropy <loss>=0.392634137469\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1000 train binary_f_1.000 <score>=0.33853745158\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:08 INFO 140146317055808] Iter[45] Batch [1500]#011Speed: 119773.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1500 train binary_classification_accuracy <score>=0.832025982678\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1500 train binary_classification_cross_entropy <loss>=0.392203241012\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=1500 train binary_f_1.000 <score>=0.337988672795\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:12 INFO 140146317055808] Iter[45] Batch [2000]#011Speed: 120332.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=2000 train binary_classification_accuracy <score>=0.831859570215\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=2000 train binary_classification_cross_entropy <loss>=0.392410426089\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, batch=2000 train binary_f_1.000 <score>=0.338642685144\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, train binary_classification_accuracy <score>=0.831968787515\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, train binary_classification_cross_entropy <loss>=0.39229267774\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=45, train binary_f_1.000 <score>=0.338947189451\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20936.913013458252, \"sum\": 20936.913013458252, \"min\": 20936.913013458252}}, \"EndTime\": 1553541256.955496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541236.018065}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 114955, \"sum\": 114955.0, \"min\": 114955}, \"Total Records Seen\": {\"count\": 1, \"max\": 114940648, \"sum\": 114940648.0, \"min\": 114940648}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 47, \"sum\": 47.0, \"min\": 47}}, \"EndTime\": 1553541256.955693, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 45}, \"StartTime\": 1553541236.018552}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119341.701735 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:14:16.955] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 20935, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=0 train binary_classification_cross_entropy <loss>=0.405644836426\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:21 INFO 140146317055808] Iter[46] Batch [500]#011Speed: 119669.59 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=500 train binary_classification_accuracy <score>=0.832672654691\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=500 train binary_classification_cross_entropy <loss>=0.391440792548\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=500 train binary_f_1.000 <score>=0.341649978403\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:25 INFO 140146317055808] Iter[46] Batch [1000]#011Speed: 119037.95 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1000 train binary_classification_accuracy <score>=0.832166833167\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1000 train binary_classification_cross_entropy <loss>=0.391994230501\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1000 train binary_f_1.000 <score>=0.341200967801\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:29 INFO 140146317055808] Iter[46] Batch [1500]#011Speed: 120849.74 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1500 train binary_classification_accuracy <score>=0.832352431712\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1500 train binary_classification_cross_entropy <loss>=0.3915661603\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=1500 train binary_f_1.000 <score>=0.340508170299\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:33 INFO 140146317055808] Iter[46] Batch [2000]#011Speed: 118523.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=2000 train binary_classification_accuracy <score>=0.832180909545\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=2000 train binary_classification_cross_entropy <loss>=0.391773768115\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, batch=2000 train binary_f_1.000 <score>=0.341166730103\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, train binary_classification_accuracy <score>=0.832283713485\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, train binary_classification_cross_entropy <loss>=0.391656529543\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=46, train binary_f_1.000 <score>=0.341441646698\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20892.89903640747, \"sum\": 20892.89903640747, \"min\": 20892.89903640747}}, \"EndTime\": 1553541277.848861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541256.955564}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 117454, \"sum\": 117454.0, \"min\": 117454}, \"Total Records Seen\": {\"count\": 1, \"max\": 117439336, \"sum\": 117439336.0, \"min\": 117439336}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1553541277.849108, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 46}, \"StartTime\": 1553541256.95593}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119592.828737 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:14:37.849] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 20891, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=0 train binary_classification_cross_entropy <loss>=0.404986724854\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=0 train binary_f_1.000 <score>=0.347169811321\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:42 INFO 140146317055808] Iter[47] Batch [500]#011Speed: 119356.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=500 train binary_classification_accuracy <score>=0.83299001996\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=500 train binary_classification_cross_entropy <loss>=0.390812180814\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=500 train binary_f_1.000 <score>=0.34400627205\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:14:46 INFO 140146317055808] Iter[47] Batch [1000]#011Speed: 120868.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1000 train binary_classification_accuracy <score>=0.832483516484\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1000 train binary_classification_cross_entropy <loss>=0.391361188098\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1000 train binary_f_1.000 <score>=0.34361005856\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:50 INFO 140146317055808] Iter[47] Batch [1500]#011Speed: 120320.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1500 train binary_classification_accuracy <score>=0.832649566955\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1500 train binary_classification_cross_entropy <loss>=0.390935880576\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=1500 train binary_f_1.000 <score>=0.342834419483\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:54 INFO 140146317055808] Iter[47] Batch [2000]#011Speed: 120915.53 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=2000 train binary_classification_accuracy <score>=0.83248175912\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=2000 train binary_classification_cross_entropy <loss>=0.391143886202\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, batch=2000 train binary_f_1.000 <score>=0.34351216799\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, train binary_classification_accuracy <score>=0.832569427771\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, train binary_classification_cross_entropy <loss>=0.391027156077\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=47, train binary_f_1.000 <score>=0.343726717622\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20775.398015975952, \"sum\": 20775.398015975952, \"min\": 20775.398015975952}}, \"EndTime\": 1553541298.624818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541277.848939}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 119953, \"sum\": 119953.0, \"min\": 119953}, \"Total Records Seen\": {\"count\": 1, \"max\": 119938024, \"sum\": 119938024.0, \"min\": 119938024}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 49, \"sum\": 49.0, \"min\": 49}}, \"EndTime\": 1553541298.625011, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 47}, \"StartTime\": 1553541277.84938}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120269.478193 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:14:58.625] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 20774, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=0 train binary_classification_accuracy <score>=0.826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=0 train binary_classification_cross_entropy <loss>=0.404336425781\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:14:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=0 train binary_f_1.000 <score>=0.345864661654\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:02 INFO 140146317055808] Iter[48] Batch [500]#011Speed: 117519.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=500 train binary_classification_accuracy <score>=0.833253493014\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=500 train binary_classification_cross_entropy <loss>=0.390190120491\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=500 train binary_f_1.000 <score>=0.346158662576\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:07 INFO 140146317055808] Iter[48] Batch [1000]#011Speed: 118397.62 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1000 train binary_classification_accuracy <score>=0.832753246753\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1000 train binary_classification_cross_entropy <loss>=0.390734628134\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1000 train binary_f_1.000 <score>=0.345844860193\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:11 INFO 140146317055808] Iter[48] Batch [1500]#011Speed: 119419.13 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1500 train binary_classification_accuracy <score>=0.832912058628\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1500 train binary_classification_cross_entropy <loss>=0.39031202469\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=1500 train binary_f_1.000 <score>=0.345088536131\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:15 INFO 140146317055808] Iter[48] Batch [2000]#011Speed: 121522.31 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=2000 train binary_classification_accuracy <score>=0.832763618191\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=2000 train binary_classification_cross_entropy <loss>=0.39052040656\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, batch=2000 train binary_f_1.000 <score>=0.34584408476\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, train binary_classification_accuracy <score>=0.832848339336\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, train binary_classification_cross_entropy <loss>=0.390404185417\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=48, train binary_f_1.000 <score>=0.346062082397\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20948.246002197266, \"sum\": 20948.246002197266, \"min\": 20948.246002197266}}, \"EndTime\": 1553541319.573535, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541298.624879}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 122452, \"sum\": 122452.0, \"min\": 122452}, \"Total Records Seen\": {\"count\": 1, \"max\": 122436712, \"sum\": 122436712.0, \"min\": 122436712}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1553541319.573767, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 48}, \"StartTime\": 1553541298.625256}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119276.957102 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:15:19.573] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 49, \"duration\": 20947, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=0 train binary_classification_accuracy <score>=0.826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=0 train binary_classification_cross_entropy <loss>=0.40369342041\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=0 train binary_f_1.000 <score>=0.345864661654\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:23 INFO 140146317055808] Iter[49] Batch [500]#011Speed: 117735.15 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=500 train binary_classification_accuracy <score>=0.833508982036\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=500 train binary_classification_cross_entropy <loss>=0.389574247684\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=500 train binary_f_1.000 <score>=0.348364113621\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:15:27 INFO 140146317055808] Iter[49] Batch [1000]#011Speed: 121198.49 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1000 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1000 train binary_classification_cross_entropy <loss>=0.390114192875\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1000 train binary_f_1.000 <score>=0.347933610282\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:32 INFO 140146317055808] Iter[49] Batch [1500]#011Speed: 118885.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1500 train binary_classification_accuracy <score>=0.833158560959\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1500 train binary_classification_cross_entropy <loss>=0.389694236029\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=1500 train binary_f_1.000 <score>=0.347200246075\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:36 INFO 140146317055808] Iter[49] Batch [2000]#011Speed: 120175.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=2000 train binary_classification_accuracy <score>=0.833014492754\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=2000 train binary_classification_cross_entropy <loss>=0.389902973257\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, batch=2000 train binary_f_1.000 <score>=0.34798270331\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, train binary_classification_accuracy <score>=0.833098839536\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, train binary_classification_cross_entropy <loss>=0.389787262635\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=49, train binary_f_1.000 <score>=0.34820128145\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20868.19887161255, \"sum\": 20868.19887161255, \"min\": 20868.19887161255}}, \"EndTime\": 1553541340.442278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541319.573608}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 124951, \"sum\": 124951.0, \"min\": 124951}, \"Total Records Seen\": {\"count\": 1, \"max\": 124935400, \"sum\": 124935400.0, \"min\": 124935400}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 51, \"sum\": 51.0, \"min\": 51}}, \"EndTime\": 1553541340.442489, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 49}, \"StartTime\": 1553541319.574045}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119734.61329 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:15:40.442] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 20866, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=0 train binary_classification_accuracy <score>=0.826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=0 train binary_classification_cross_entropy <loss>=0.403057312012\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=0 train binary_f_1.000 <score>=0.345864661654\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:44 INFO 140146317055808] Iter[50] Batch [500]#011Speed: 119170.90 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=500 train binary_classification_accuracy <score>=0.833796407186\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=500 train binary_classification_cross_entropy <loss>=0.388964205927\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=500 train binary_f_1.000 <score>=0.350635576698\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:48 INFO 140146317055808] Iter[50] Batch [1000]#011Speed: 119689.84 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1000 train binary_classification_accuracy <score>=0.833288711289\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1000 train binary_classification_cross_entropy <loss>=0.389499534218\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1000 train binary_f_1.000 <score>=0.350224278105\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:52 INFO 140146317055808] Iter[50] Batch [1500]#011Speed: 120314.85 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1500 train binary_classification_accuracy <score>=0.833435043304\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1500 train binary_classification_cross_entropy <loss>=0.389082169403\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=1500 train binary_f_1.000 <score>=0.34947752961\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:57 INFO 140146317055808] Iter[50] Batch [2000]#011Speed: 117840.92 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=2000 train binary_classification_accuracy <score>=0.833287356322\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=2000 train binary_classification_cross_entropy <loss>=0.389291244362\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:15:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, batch=2000 train binary_f_1.000 <score>=0.350255151728\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, train binary_classification_accuracy <score>=0.833362545018\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, train binary_classification_cross_entropy <loss>=0.389176047262\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=50, train binary_f_1.000 <score>=0.35042389736\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20951.84302330017, \"sum\": 20951.84302330017, \"min\": 20951.84302330017}}, \"EndTime\": 1553541361.394616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541340.442337}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 127450, \"sum\": 127450.0, \"min\": 127450}, \"Total Records Seen\": {\"count\": 1, \"max\": 127434088, \"sum\": 127434088.0, \"min\": 127434088}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1553541361.39483, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 50}, \"StartTime\": 1553541340.442734}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119256.478596 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:16:01.395] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 51, \"duration\": 20950, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=0 train binary_classification_cross_entropy <loss>=0.402427642822\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=0 train binary_f_1.000 <score>=0.358208955224\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:05 INFO 140146317055808] Iter[51] Batch [500]#011Speed: 118222.37 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=500 train binary_classification_accuracy <score>=0.83401996008\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=500 train binary_classification_cross_entropy <loss>=0.388359659953\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=500 train binary_f_1.000 <score>=0.352609616343\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:09 INFO 140146317055808] Iter[51] Batch [1000]#011Speed: 119018.43 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1000 train binary_classification_accuracy <score>=0.833553446553\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1000 train binary_classification_cross_entropy <loss>=0.388890321653\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1000 train binary_f_1.000 <score>=0.352373215375\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:16:14 INFO 140146317055808] Iter[51] Batch [1500]#011Speed: 119505.24 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1500 train binary_classification_accuracy <score>=0.833703530979\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1500 train binary_classification_cross_entropy <loss>=0.388475498105\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=1500 train binary_f_1.000 <score>=0.351641215723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:18 INFO 140146317055808] Iter[51] Batch [2000]#011Speed: 120006.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=2000 train binary_classification_accuracy <score>=0.833552223888\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=2000 train binary_classification_cross_entropy <loss>=0.388684894232\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, batch=2000 train binary_f_1.000 <score>=0.352440243111\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, train binary_classification_accuracy <score>=0.833630652261\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, train binary_classification_cross_entropy <loss>=0.388570214927\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=51, train binary_f_1.000 <score>=0.352575871536\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20930.476188659668, \"sum\": 20930.476188659668, \"min\": 20930.476188659668}}, \"EndTime\": 1553541382.325619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541361.394679}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 129949, \"sum\": 129949.0, \"min\": 129949}, \"Total Records Seen\": {\"count\": 1, \"max\": 129932776, \"sum\": 129932776.0, \"min\": 129932776}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 53, \"sum\": 53.0, \"min\": 53}}, \"EndTime\": 1553541382.325849, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 51}, \"StartTime\": 1553541361.395112}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119378.220499 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:16:22.326] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 20929, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=0 train binary_classification_cross_entropy <loss>=0.401803985596\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=0 train binary_f_1.000 <score>=0.35687732342\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:26 INFO 140146317055808] Iter[52] Batch [500]#011Speed: 119654.55 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=500 train binary_classification_accuracy <score>=0.834283433134\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=500 train binary_classification_cross_entropy <loss>=0.387760287043\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=500 train binary_f_1.000 <score>=0.354672221618\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:30 INFO 140146317055808] Iter[52] Batch [1000]#011Speed: 120712.04 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1000 train binary_classification_accuracy <score>=0.833805194805\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1000 train binary_classification_cross_entropy <loss>=0.388286237932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1000 train binary_f_1.000 <score>=0.354446785641\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:34 INFO 140146317055808] Iter[52] Batch [1500]#011Speed: 120763.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1500 train binary_classification_accuracy <score>=0.833935376416\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1500 train binary_classification_cross_entropy <loss>=0.387873905779\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=1500 train binary_f_1.000 <score>=0.353616318359\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:38 INFO 140146317055808] Iter[52] Batch [2000]#011Speed: 119294.96 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=2000 train binary_classification_accuracy <score>=0.833792603698\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=2000 train binary_classification_cross_entropy <loss>=0.388083608602\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, batch=2000 train binary_f_1.000 <score>=0.354438537135\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, train binary_classification_accuracy <score>=0.83387595038\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, train binary_classification_cross_entropy <loss>=0.387969452698\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=52, train binary_f_1.000 <score>=0.354594779472\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20835.519790649414, \"sum\": 20835.519790649414, \"min\": 20835.519790649414}}, \"EndTime\": 1553541403.161668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541382.325684}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 132448, \"sum\": 132448.0, \"min\": 132448}, \"Total Records Seen\": {\"count\": 1, \"max\": 132431464, \"sum\": 132431464.0, \"min\": 132431464}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1553541403.161864, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 52}, \"StartTime\": 1553541382.326118}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119922.501677 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:16:43.162] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 20834, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=0 train binary_classification_cross_entropy <loss>=0.401185974121\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=0 train binary_f_1.000 <score>=0.35687732342\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:16:47 INFO 140146317055808] Iter[53] Batch [500]#011Speed: 119195.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=500 train binary_classification_accuracy <score>=0.834624750499\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=500 train binary_classification_cross_entropy <loss>=0.387165776418\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=500 train binary_f_1.000 <score>=0.357155603833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:51 INFO 140146317055808] Iter[53] Batch [1000]#011Speed: 120081.43 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1000 train binary_classification_accuracy <score>=0.834124875125\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1000 train binary_classification_cross_entropy <loss>=0.387686977634\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1000 train binary_f_1.000 <score>=0.356806675163\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:55 INFO 140146317055808] Iter[53] Batch [1500]#011Speed: 119212.84 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1500 train binary_classification_accuracy <score>=0.834251832112\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1500 train binary_classification_cross_entropy <loss>=0.387277089627\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=1500 train binary_f_1.000 <score>=0.355965373703\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:59 INFO 140146317055808] Iter[53] Batch [2000]#011Speed: 119486.02 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=2000 train binary_classification_accuracy <score>=0.834096951524\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=2000 train binary_classification_cross_entropy <loss>=0.38748708594\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:16:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, batch=2000 train binary_f_1.000 <score>=0.356740647732\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, train binary_classification_accuracy <score>=0.834167266907\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, train binary_classification_cross_entropy <loss>=0.387373460297\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=53, train binary_f_1.000 <score>=0.356838449147\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20950.65402984619, \"sum\": 20950.65402984619, \"min\": 20950.65402984619}}, \"EndTime\": 1553541424.112777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541403.161731}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 134947, \"sum\": 134947.0, \"min\": 134947}, \"Total Records Seen\": {\"count\": 1, \"max\": 134930152, \"sum\": 134930152.0, \"min\": 134930152}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 55, \"sum\": 55.0, \"min\": 55}}, \"EndTime\": 1553541424.113009, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 53}, \"StartTime\": 1553541403.162093}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119263.201742 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:17:04.113] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 20949, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=0 train binary_classification_cross_entropy <loss>=0.400573272705\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=0 train binary_f_1.000 <score>=0.35687732342\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:08 INFO 140146317055808] Iter[54] Batch [500]#011Speed: 117434.49 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=500 train binary_classification_accuracy <score>=0.834868263473\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=500 train binary_classification_cross_entropy <loss>=0.386575823816\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=500 train binary_f_1.000 <score>=0.359176148907\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:12 INFO 140146317055808] Iter[54] Batch [1000]#011Speed: 119940.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1000 train binary_classification_accuracy <score>=0.834360639361\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1000 train binary_classification_cross_entropy <loss>=0.387092244365\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1000 train binary_f_1.000 <score>=0.358749250672\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:16 INFO 140146317055808] Iter[54] Batch [1500]#011Speed: 120806.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1500 train binary_classification_accuracy <score>=0.834485009993\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1500 train binary_classification_cross_entropy <loss>=0.386684755939\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=1500 train binary_f_1.000 <score>=0.357921887276\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:20 INFO 140146317055808] Iter[54] Batch [2000]#011Speed: 119583.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=2000 train binary_classification_accuracy <score>=0.834343828086\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=2000 train binary_classification_cross_entropy <loss>=0.386895034904\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, batch=2000 train binary_f_1.000 <score>=0.358771390214\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, train binary_classification_accuracy <score>=0.834412965186\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, train binary_classification_cross_entropy <loss>=0.386781946678\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=54, train binary_f_1.000 <score>=0.358895781561\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20971.752882003784, \"sum\": 20971.752882003784, \"min\": 20971.752882003784}}, \"EndTime\": 1553541445.085113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541424.11285}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 137446, \"sum\": 137446.0, \"min\": 137446}, \"Total Records Seen\": {\"count\": 1, \"max\": 137428840, \"sum\": 137428840.0, \"min\": 137428840}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1553541445.085341, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 54}, \"StartTime\": 1553541424.113325}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119143.143453 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:17:25.085] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 55, \"duration\": 20970, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=0 train binary_classification_cross_entropy <loss>=0.399965454102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=0 train binary_f_1.000 <score>=0.35687732342\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:29 INFO 140146317055808] Iter[55] Batch [500]#011Speed: 118357.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=500 train binary_classification_accuracy <score>=0.835151696607\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=500 train binary_classification_cross_entropy <loss>=0.38599014666\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=500 train binary_f_1.000 <score>=0.361443361142\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:17:33 INFO 140146317055808] Iter[55] Batch [1000]#011Speed: 120278.44 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1000 train binary_classification_accuracy <score>=0.834623376623\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1000 train binary_classification_cross_entropy <loss>=0.386501758709\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1000 train binary_f_1.000 <score>=0.360836763218\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:37 INFO 140146317055808] Iter[55] Batch [1500]#011Speed: 119613.65 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1500 train binary_classification_accuracy <score>=0.834755496336\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1500 train binary_classification_cross_entropy <loss>=0.386096627354\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=1500 train binary_f_1.000 <score>=0.36008255934\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:41 INFO 140146317055808] Iter[55] Batch [2000]#011Speed: 121212.88 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=2000 train binary_classification_accuracy <score>=0.834621189405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=2000 train binary_classification_cross_entropy <loss>=0.38630717861\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, batch=2000 train binary_f_1.000 <score>=0.360961291506\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, train binary_classification_accuracy <score>=0.834692276911\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, train binary_classification_cross_entropy <loss>=0.386194636546\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=55, train binary_f_1.000 <score>=0.36107003879\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20857.786893844604, \"sum\": 20857.786893844604, \"min\": 20857.786893844604}}, \"EndTime\": 1553541465.943445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541445.085173}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 139945, \"sum\": 139945.0, \"min\": 139945}, \"Total Records Seen\": {\"count\": 1, \"max\": 139927528, \"sum\": 139927528.0, \"min\": 139927528}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 57, \"sum\": 57.0, \"min\": 57}}, \"EndTime\": 1553541465.943672, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 55}, \"StartTime\": 1553541445.085624}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119794.211317 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:17:45.943] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 20856, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=0 train binary_classification_accuracy <score>=0.826\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=0 train binary_classification_cross_entropy <loss>=0.399362213135\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:45 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=0 train binary_f_1.000 <score>=0.355555555556\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:50 INFO 140146317055808] Iter[56] Batch [500]#011Speed: 118970.49 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=500 train binary_classification_accuracy <score>=0.835443113772\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=500 train binary_classification_cross_entropy <loss>=0.385408462981\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=500 train binary_f_1.000 <score>=0.363625136047\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:54 INFO 140146317055808] Iter[56] Batch [1000]#011Speed: 118994.63 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1000 train binary_classification_accuracy <score>=0.834903096903\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1000 train binary_classification_cross_entropy <loss>=0.385915245064\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1000 train binary_f_1.000 <score>=0.363098220273\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:58 INFO 140146317055808] Iter[56] Batch [1500]#011Speed: 119342.36 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1500 train binary_classification_accuracy <score>=0.835020652898\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1500 train binary_classification_cross_entropy <loss>=0.385512429194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:17:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=1500 train binary_f_1.000 <score>=0.362241429042\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:02 INFO 140146317055808] Iter[56] Batch [2000]#011Speed: 118674.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=2000 train binary_classification_accuracy <score>=0.834875562219\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=2000 train binary_classification_cross_entropy <loss>=0.385723243969\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, batch=2000 train binary_f_1.000 <score>=0.363015600059\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, train binary_classification_accuracy <score>=0.834943977591\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, train binary_classification_cross_entropy <loss>=0.385611257699\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=56, train binary_f_1.000 <score>=0.36309396053\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20922.70302772522, \"sum\": 20922.70302772522, \"min\": 20922.70302772522}}, \"EndTime\": 1553541486.866678, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541465.943516}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 142444, \"sum\": 142444.0, \"min\": 142444}, \"Total Records Seen\": {\"count\": 1, \"max\": 142426216, \"sum\": 142426216.0, \"min\": 142426216}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1553541486.866873, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 56}, \"StartTime\": 1553541465.943945}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119422.7709 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:18:06.867] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 57, \"duration\": 20921, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=0 train binary_classification_cross_entropy <loss>=0.398763183594\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=0 train binary_f_1.000 <score>=0.367647058824\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:11 INFO 140146317055808] Iter[57] Batch [500]#011Speed: 118130.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=500 train binary_classification_accuracy <score>=0.835674650699\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=500 train binary_classification_cross_entropy <loss>=0.384830501594\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=500 train binary_f_1.000 <score>=0.36564674336\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:15 INFO 140146317055808] Iter[57] Batch [1000]#011Speed: 119237.91 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1000 train binary_classification_accuracy <score>=0.835168831169\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1000 train binary_classification_cross_entropy <loss>=0.385332438009\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1000 train binary_f_1.000 <score>=0.365204678363\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:18:19 INFO 140146317055808] Iter[57] Batch [1500]#011Speed: 119641.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1500 train binary_classification_accuracy <score>=0.83528580946\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1500 train binary_classification_cross_entropy <loss>=0.384931899506\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=1500 train binary_f_1.000 <score>=0.364330561683\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:23 INFO 140146317055808] Iter[57] Batch [2000]#011Speed: 119963.74 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=2000 train binary_classification_accuracy <score>=0.835148425787\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=2000 train binary_classification_cross_entropy <loss>=0.385142970903\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, batch=2000 train binary_f_1.000 <score>=0.365150115473\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, train binary_classification_accuracy <score>=0.835200880352\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, train binary_classification_cross_entropy <loss>=0.385031551109\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=57, train binary_f_1.000 <score>=0.365163559794\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20899.254083633423, \"sum\": 20899.254083633423, \"min\": 20899.254083633423}}, \"EndTime\": 1553541507.766394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541486.866748}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 144943, \"sum\": 144943.0, \"min\": 144943}, \"Total Records Seen\": {\"count\": 1, \"max\": 144924904, \"sum\": 144924904.0, \"min\": 144924904}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 59, \"sum\": 59.0, \"min\": 59}}, \"EndTime\": 1553541507.766579, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 57}, \"StartTime\": 1553541486.86711}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119556.870059 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:18:27.766] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 58, \"duration\": 20898, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=0 train binary_classification_cross_entropy <loss>=0.398168151855\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:27 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=0 train binary_f_1.000 <score>=0.373626373626\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:31 INFO 140146317055808] Iter[58] Batch [500]#011Speed: 120738.41 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=500 train binary_classification_accuracy <score>=0.835914171657\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=500 train binary_classification_cross_entropy <loss>=0.384256003801\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=500 train binary_f_1.000 <score>=0.367565488326\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:36 INFO 140146317055808] Iter[58] Batch [1000]#011Speed: 120392.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1000 train binary_classification_accuracy <score>=0.835485514486\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1000 train binary_classification_cross_entropy <loss>=0.384753083708\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1000 train binary_f_1.000 <score>=0.367531445031\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:40 INFO 140146317055808] Iter[58] Batch [1500]#011Speed: 120184.36 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1500 train binary_classification_accuracy <score>=0.835567621586\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1500 train binary_classification_cross_entropy <loss>=0.384354785185\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=1500 train binary_f_1.000 <score>=0.366511040556\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:44 INFO 140146317055808] Iter[58] Batch [2000]#011Speed: 120186.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=2000 train binary_classification_accuracy <score>=0.835425787106\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=2000 train binary_classification_cross_entropy <loss>=0.384566106687\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, batch=2000 train binary_f_1.000 <score>=0.3673129055\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, train binary_classification_accuracy <score>=0.835467386955\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, train binary_classification_cross_entropy <loss>=0.384455264601\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=58, train binary_f_1.000 <score>=0.367259190636\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20815.747022628784, \"sum\": 20815.747022628784, \"min\": 20815.747022628784}}, \"EndTime\": 1553541528.582595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541507.766454}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 147442, \"sum\": 147442.0, \"min\": 147442}, \"Total Records Seen\": {\"count\": 1, \"max\": 147423592, \"sum\": 147423592.0, \"min\": 147423592}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1553541528.582821, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 58}, \"StartTime\": 1553541507.766816}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120036.167489 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:18:48.583] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 20814, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=0 train binary_classification_cross_entropy <loss>=0.397576782227\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=0 train binary_f_1.000 <score>=0.372262773723\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:18:52 INFO 140146317055808] Iter[59] Batch [500]#011Speed: 121194.88 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=500 train binary_classification_accuracy <score>=0.836179640719\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=500 train binary_classification_cross_entropy <loss>=0.383684719002\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=500 train binary_f_1.000 <score>=0.369379475674\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:56 INFO 140146317055808] Iter[59] Batch [1000]#011Speed: 121599.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1000 train binary_classification_accuracy <score>=0.835732267732\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1000 train binary_classification_cross_entropy <loss>=0.384176935003\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:18:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1000 train binary_f_1.000 <score>=0.369368954752\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:00 INFO 140146317055808] Iter[59] Batch [1500]#011Speed: 120746.29 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1500 train binary_classification_accuracy <score>=0.835816122585\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1500 train binary_classification_cross_entropy <loss>=0.383780840707\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=1500 train binary_f_1.000 <score>=0.368410261667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:05 INFO 140146317055808] Iter[59] Batch [2000]#011Speed: 118443.95 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=2000 train binary_classification_accuracy <score>=0.835687156422\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=2000 train binary_classification_cross_entropy <loss>=0.383992407498\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, batch=2000 train binary_f_1.000 <score>=0.36929315716\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, train binary_classification_accuracy <score>=0.835734293717\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, train binary_classification_cross_entropy <loss>=0.383882154888\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=59, train binary_f_1.000 <score>=0.369297946416\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20740.66686630249, \"sum\": 20740.66686630249, \"min\": 20740.66686630249}}, \"EndTime\": 1553541549.323792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541528.582665}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 149941, \"sum\": 149941.0, \"min\": 149941}, \"Total Records Seen\": {\"count\": 1, \"max\": 149922280, \"sum\": 149922280.0, \"min\": 149922280}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 61, \"sum\": 61.0, \"min\": 61}}, \"EndTime\": 1553541549.324023, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 59}, \"StartTime\": 1553541528.583096}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120470.609572 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:19:09.324] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 60, \"duration\": 20739, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=0 train binary_classification_cross_entropy <loss>=0.396988800049\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=0 train binary_f_1.000 <score>=0.372262773723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:13 INFO 140146317055808] Iter[60] Batch [500]#011Speed: 118904.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=500 train binary_classification_accuracy <score>=0.836481037924\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=500 train binary_classification_cross_entropy <loss>=0.383116400012\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=500 train binary_f_1.000 <score>=0.371529615736\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:17 INFO 140146317055808] Iter[60] Batch [1000]#011Speed: 119352.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1000 train binary_classification_accuracy <score>=0.836005994006\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1000 train binary_classification_cross_entropy <loss>=0.383603751382\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1000 train binary_f_1.000 <score>=0.371384151151\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:21 INFO 140146317055808] Iter[60] Batch [1500]#011Speed: 117478.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1500 train binary_classification_accuracy <score>=0.836080612925\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1500 train binary_classification_cross_entropy <loss>=0.383209828823\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=1500 train binary_f_1.000 <score>=0.370403920214\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:26 INFO 140146317055808] Iter[60] Batch [2000]#011Speed: 119810.50 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=2000 train binary_classification_accuracy <score>=0.83596001999\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=2000 train binary_classification_cross_entropy <loss>=0.383421637661\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, batch=2000 train binary_f_1.000 <score>=0.371372759307\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, train binary_classification_accuracy <score>=0.836012805122\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, train binary_classification_cross_entropy <loss>=0.383311987185\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=60, train binary_f_1.000 <score>=0.371371770604\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20956.13408088684, \"sum\": 20956.13408088684, \"min\": 20956.13408088684}}, \"EndTime\": 1553541570.280467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541549.32386}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 152440, \"sum\": 152440.0, \"min\": 152440}, \"Total Records Seen\": {\"count\": 1, \"max\": 152420968, \"sum\": 152420968.0, \"min\": 152420968}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1553541570.280678, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 60}, \"StartTime\": 1553541549.324305}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119232.169501 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:19:30.280] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 61, \"duration\": 20954, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=0 train binary_classification_cross_entropy <loss>=0.396403930664\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=0 train binary_f_1.000 <score>=0.372262773723\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:19:34 INFO 140146317055808] Iter[61] Batch [500]#011Speed: 118419.88 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=500 train binary_classification_accuracy <score>=0.836764471058\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=500 train binary_classification_cross_entropy <loss>=0.38255081658\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=500 train binary_f_1.000 <score>=0.373560885185\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:38 INFO 140146317055808] Iter[61] Batch [1000]#011Speed: 119229.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1000 train binary_classification_accuracy <score>=0.836285714286\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1000 train binary_classification_cross_entropy <loss>=0.383033305348\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1000 train binary_f_1.000 <score>=0.373396957948\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:42 INFO 140146317055808] Iter[61] Batch [1500]#011Speed: 117962.75 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1500 train binary_classification_accuracy <score>=0.836342438374\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1500 train binary_classification_cross_entropy <loss>=0.382641522209\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=1500 train binary_f_1.000 <score>=0.372333086338\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:47 INFO 140146317055808] Iter[61] Batch [2000]#011Speed: 118337.54 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=2000 train binary_classification_accuracy <score>=0.836210894553\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=2000 train binary_classification_cross_entropy <loss>=0.382853570285\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, batch=2000 train binary_f_1.000 <score>=0.373279943704\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, train binary_classification_accuracy <score>=0.836269307723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, train binary_classification_cross_entropy <loss>=0.382744535839\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=61, train binary_f_1.000 <score>=0.373298125392\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21078.338146209717, \"sum\": 21078.338146209717, \"min\": 21078.338146209717}}, \"EndTime\": 1553541591.359271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541570.28055}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 154939, \"sum\": 154939.0, \"min\": 154939}, \"Total Records Seen\": {\"count\": 1, \"max\": 154919656, \"sum\": 154919656.0, \"min\": 154919656}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 63, \"sum\": 63.0, \"min\": 63}}, \"EndTime\": 1553541591.359464, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 61}, \"StartTime\": 1553541570.280905}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118541.121621 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:19:51.359] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 21077, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=0 train binary_classification_cross_entropy <loss>=0.395821929932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=0 train binary_f_1.000 <score>=0.372262773723\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:55 INFO 140146317055808] Iter[62] Batch [500]#011Speed: 119008.23 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=500 train binary_classification_accuracy <score>=0.837035928144\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=500 train binary_classification_cross_entropy <loss>=0.381987736928\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=500 train binary_f_1.000 <score>=0.375607032786\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:59 INFO 140146317055808] Iter[62] Batch [1000]#011Speed: 119203.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1000 train binary_classification_accuracy <score>=0.836551448551\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1000 train binary_classification_cross_entropy <loss>=0.382465372274\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:19:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1000 train binary_f_1.000 <score>=0.37538844477\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:04 INFO 140146317055808] Iter[62] Batch [1500]#011Speed: 117746.10 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1500 train binary_classification_accuracy <score>=0.836590273151\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1500 train binary_classification_cross_entropy <loss>=0.382075698517\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=1500 train binary_f_1.000 <score>=0.374252505255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:08 INFO 140146317055808] Iter[62] Batch [2000]#011Speed: 119118.52 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=2000 train binary_classification_accuracy <score>=0.836466766617\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=2000 train binary_classification_cross_entropy <loss>=0.382287985112\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, batch=2000 train binary_f_1.000 <score>=0.375262510787\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, train binary_classification_accuracy <score>=0.836520608243\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, train binary_classification_cross_entropy <loss>=0.382179580621\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=62, train binary_f_1.000 <score>=0.375264744247\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20993.776082992554, \"sum\": 20993.776082992554, \"min\": 20993.776082992554}}, \"EndTime\": 1553541612.353484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541591.359336}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 157438, \"sum\": 157438.0, \"min\": 157438}, \"Total Records Seen\": {\"count\": 1, \"max\": 157418344, \"sum\": 157418344.0, \"min\": 157418344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1553541612.353668, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 62}, \"StartTime\": 1553541591.35968}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119018.643054 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:20:12.353] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 63, \"duration\": 20992, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=0 train binary_classification_cross_entropy <loss>=0.395242614746\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=0 train binary_f_1.000 <score>=0.372262773723\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:20:16 INFO 140146317055808] Iter[63] Batch [500]#011Speed: 118938.11 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=500 train binary_classification_accuracy <score>=0.837275449102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=500 train binary_classification_cross_entropy <loss>=0.381426949568\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=500 train binary_f_1.000 <score>=0.37741036313\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:20 INFO 140146317055808] Iter[63] Batch [1000]#011Speed: 117078.80 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1000 train binary_classification_accuracy <score>=0.836788211788\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1000 train binary_classification_cross_entropy <loss>=0.381899740976\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1000 train binary_f_1.000 <score>=0.377175359208\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:25 INFO 140146317055808] Iter[63] Batch [1500]#011Speed: 118048.82 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1500 train binary_classification_accuracy <score>=0.836846768821\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1500 train binary_classification_cross_entropy <loss>=0.38151214687\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=1500 train binary_f_1.000 <score>=0.376143493645\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:29 INFO 140146317055808] Iter[63] Batch [2000]#011Speed: 119400.73 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=2000 train binary_classification_accuracy <score>=0.836713143428\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=2000 train binary_classification_cross_entropy <loss>=0.381724671401\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, batch=2000 train binary_f_1.000 <score>=0.37710871625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, train binary_classification_accuracy <score>=0.836757503001\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, train binary_classification_cross_entropy <loss>=0.381616911315\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=63, train binary_f_1.000 <score>=0.377027425313\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21046.16403579712, \"sum\": 21046.16403579712, \"min\": 21046.16403579712}}, \"EndTime\": 1553541633.400086, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541612.353543}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 159937, \"sum\": 159937.0, \"min\": 159937}, \"Total Records Seen\": {\"count\": 1, \"max\": 159917032, \"sum\": 159917032.0, \"min\": 159917032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 65, \"sum\": 65.0, \"min\": 65}}, \"EndTime\": 1553541633.400288, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 63}, \"StartTime\": 1553541612.353884}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118722.206934 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:20:33.400] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 64, \"duration\": 21045, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=0 train binary_classification_cross_entropy <loss>=0.394665710449\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=0 train binary_f_1.000 <score>=0.373626373626\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:37 INFO 140146317055808] Iter[64] Batch [500]#011Speed: 118497.34 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=500 train binary_classification_accuracy <score>=0.837548902196\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=500 train binary_classification_cross_entropy <loss>=0.380868238133\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=500 train binary_f_1.000 <score>=0.379276682073\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:41 INFO 140146317055808] Iter[64] Batch [1000]#011Speed: 117811.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1000 train binary_classification_accuracy <score>=0.837054945055\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1000 train binary_classification_cross_entropy <loss>=0.381336201518\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1000 train binary_f_1.000 <score>=0.379043065115\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:46 INFO 140146317055808] Iter[64] Batch [1500]#011Speed: 119635.60 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1500 train binary_classification_accuracy <score>=0.837113924051\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1500 train binary_classification_cross_entropy <loss>=0.380950661209\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=1500 train binary_f_1.000 <score>=0.378044375251\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:50 INFO 140146317055808] Iter[64] Batch [2000]#011Speed: 119649.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=2000 train binary_classification_accuracy <score>=0.836974512744\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=2000 train binary_classification_cross_entropy <loss>=0.381163425078\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, batch=2000 train binary_f_1.000 <score>=0.378985402405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, train binary_classification_accuracy <score>=0.837017206883\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, train binary_classification_cross_entropy <loss>=0.381056324617\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=64, train binary_f_1.000 <score>=0.378922392366\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20905.946969985962, \"sum\": 20905.946969985962, \"min\": 20905.946969985962}}, \"EndTime\": 1553541654.306514, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541633.400152}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 162436, \"sum\": 162436.0, \"min\": 162436}, \"Total Records Seen\": {\"count\": 1, \"max\": 162415720, \"sum\": 162415720.0, \"min\": 162415720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1553541654.306743, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 64}, \"StartTime\": 1553541633.400535}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119518.148325 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:20:54.306] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 20904, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=0 train binary_classification_cross_entropy <loss>=0.394091094971\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=0 train binary_f_1.000 <score>=0.373626373626\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:20:58 INFO 140146317055808] Iter[65] Batch [500]#011Speed: 119246.82 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=500 train binary_classification_accuracy <score>=0.837840319361\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=500 train binary_classification_cross_entropy <loss>=0.380311403073\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:20:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=500 train binary_f_1.000 <score>=0.381286745667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:02 INFO 140146317055808] Iter[65] Batch [1000]#011Speed: 117561.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1000 train binary_classification_accuracy <score>=0.837328671329\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1000 train binary_classification_cross_entropy <loss>=0.38077455848\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1000 train binary_f_1.000 <score>=0.380948760255\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:07 INFO 140146317055808] Iter[65] Batch [1500]#011Speed: 119070.52 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1500 train binary_classification_accuracy <score>=0.837385076616\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1500 train binary_classification_cross_entropy <loss>=0.380391044947\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=1500 train binary_f_1.000 <score>=0.379945687932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:11 INFO 140146317055808] Iter[65] Batch [2000]#011Speed: 119755.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=2000 train binary_classification_accuracy <score>=0.837226886557\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=2000 train binary_classification_cross_entropy <loss>=0.38060405091\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, batch=2000 train binary_f_1.000 <score>=0.380761355405\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, train binary_classification_accuracy <score>=0.837273709484\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, train binary_classification_cross_entropy <loss>=0.380497626162\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=65, train binary_f_1.000 <score>=0.380744988815\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21075.629949569702, \"sum\": 21075.629949569702, \"min\": 21075.629949569702}}, \"EndTime\": 1553541675.382682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541654.306583}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 164935, \"sum\": 164935.0, \"min\": 164935}, \"Total Records Seen\": {\"count\": 1, \"max\": 164914408, \"sum\": 164914408.0, \"min\": 164914408}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 67, \"sum\": 67.0, \"min\": 67}}, \"EndTime\": 1553541675.382865, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 65}, \"StartTime\": 1553541654.307023}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118556.313565 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:21:15.383] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 66, \"duration\": 21074, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=0 train binary_classification_cross_entropy <loss>=0.393518432617\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=0 train binary_f_1.000 <score>=0.373626373626\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:19 INFO 140146317055808] Iter[66] Batch [500]#011Speed: 118893.79 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=500 train binary_classification_accuracy <score>=0.838161676647\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=500 train binary_classification_cross_entropy <loss>=0.379756251474\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=500 train binary_f_1.000 <score>=0.383428514939\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:23 INFO 140146317055808] Iter[66] Batch [1000]#011Speed: 118181.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1000 train binary_classification_accuracy <score>=0.837633366633\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1000 train binary_classification_cross_entropy <loss>=0.380214621286\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1000 train binary_f_1.000 <score>=0.383090978794\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:28 INFO 140146317055808] Iter[66] Batch [1500]#011Speed: 119025.92 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1500 train binary_classification_accuracy <score>=0.837664890073\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1500 train binary_classification_cross_entropy <loss>=0.379833110385\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=1500 train binary_f_1.000 <score>=0.381970319154\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:32 INFO 140146317055808] Iter[66] Batch [2000]#011Speed: 119606.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=2000 train binary_classification_accuracy <score>=0.837495752124\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=2000 train binary_classification_cross_entropy <loss>=0.380046362332\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, batch=2000 train binary_f_1.000 <score>=0.382713682312\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, train binary_classification_accuracy <score>=0.837540216086\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, train binary_classification_cross_entropy <loss>=0.379940628974\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=66, train binary_f_1.000 <score>=0.38268607061\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21043.97702217102, \"sum\": 21043.97702217102, \"min\": 21043.97702217102}}, \"EndTime\": 1553541696.427114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541675.382742}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 167434, \"sum\": 167434.0, \"min\": 167434}, \"Total Records Seen\": {\"count\": 1, \"max\": 167413096, \"sum\": 167413096.0, \"min\": 167413096}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1553541696.427398, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 66}, \"StartTime\": 1553541675.383104}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118734.082298 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:21:36.427] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 67, \"duration\": 21043, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=0 train binary_classification_cross_entropy <loss>=0.392947692871\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=0 train binary_f_1.000 <score>=0.373626373626\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:21:40 INFO 140146317055808] Iter[67] Batch [500]#011Speed: 118164.90 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=500 train binary_classification_accuracy <score>=0.838423153693\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=500 train binary_classification_cross_entropy <loss>=0.379202600597\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=500 train binary_f_1.000 <score>=0.385382816534\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:44 INFO 140146317055808] Iter[67] Batch [1000]#011Speed: 120485.76 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1000 train binary_classification_accuracy <score>=0.837932067932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1000 train binary_classification_cross_entropy <loss>=0.379656209666\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1000 train binary_f_1.000 <score>=0.385129091433\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:48 INFO 140146317055808] Iter[67] Batch [1500]#011Speed: 120205.92 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1500 train binary_classification_accuracy <score>=0.837958694204\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1500 train binary_classification_cross_entropy <loss>=0.379276677649\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=1500 train binary_f_1.000 <score>=0.384005997245\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:53 INFO 140146317055808] Iter[67] Batch [2000]#011Speed: 119227.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=2000 train binary_classification_accuracy <score>=0.837764117941\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=2000 train binary_classification_cross_entropy <loss>=0.379490179959\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, batch=2000 train binary_f_1.000 <score>=0.384601094181\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, train binary_classification_accuracy <score>=0.837796718687\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, train binary_classification_cross_entropy <loss>=0.379385154513\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=67, train binary_f_1.000 <score>=0.384503954034\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20873.809099197388, \"sum\": 20873.809099197388, \"min\": 20873.809099197388}}, \"EndTime\": 1553541717.301491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541696.427227}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 169933, \"sum\": 169933.0, \"min\": 169933}, \"Total Records Seen\": {\"count\": 1, \"max\": 169911784, \"sum\": 169911784.0, \"min\": 169911784}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 69, \"sum\": 69.0, \"min\": 69}}, \"EndTime\": 1553541717.301701, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 67}, \"StartTime\": 1553541696.427652}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119702.353614 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:21:57.301] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 20872, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=0 train binary_classification_cross_entropy <loss>=0.392378662109\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:21:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=0 train binary_f_1.000 <score>=0.367647058824\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:01 INFO 140146317055808] Iter[68] Batch [500]#011Speed: 119233.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=500 train binary_classification_accuracy <score>=0.838652694611\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=500 train binary_classification_cross_entropy <loss>=0.378650273184\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=500 train binary_f_1.000 <score>=0.387144709209\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:05 INFO 140146317055808] Iter[68] Batch [1000]#011Speed: 118349.30 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1000 train binary_classification_accuracy <score>=0.838157842158\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1000 train binary_classification_cross_entropy <loss>=0.379099150087\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1000 train binary_f_1.000 <score>=0.386887280874\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:09 INFO 140146317055808] Iter[68] Batch [1500]#011Speed: 119735.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1500 train binary_classification_accuracy <score>=0.83819053964\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1500 train binary_classification_cross_entropy <loss>=0.378721574757\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=1500 train binary_f_1.000 <score>=0.385755400778\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:14 INFO 140146317055808] Iter[68] Batch [2000]#011Speed: 118224.33 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=2000 train binary_classification_accuracy <score>=0.837994502749\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=2000 train binary_classification_cross_entropy <loss>=0.378935333877\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, batch=2000 train binary_f_1.000 <score>=0.386369530199\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, train binary_classification_accuracy <score>=0.838029611845\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, train binary_classification_cross_entropy <loss>=0.378831033378\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=68, train binary_f_1.000 <score>=0.386267421973\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20901.11207962036, \"sum\": 20901.11207962036, \"min\": 20901.11207962036}}, \"EndTime\": 1553541738.203114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541717.301547}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 172432, \"sum\": 172432.0, \"min\": 172432}, \"Total Records Seen\": {\"count\": 1, \"max\": 172410472, \"sum\": 172410472.0, \"min\": 172410472}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1553541738.203317, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 68}, \"StartTime\": 1553541717.301971}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119546.099081 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:22:18.203] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 69, \"duration\": 20900, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=0 train binary_classification_accuracy <score>=0.827\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=0 train binary_classification_cross_entropy <loss>=0.391811157227\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=0 train binary_f_1.000 <score>=0.361623616236\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:22:22 INFO 140146317055808] Iter[69] Batch [500]#011Speed: 117634.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=500 train binary_classification_accuracy <score>=0.838906187625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=500 train binary_classification_cross_entropy <loss>=0.378099102211\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=500 train binary_f_1.000 <score>=0.388881316917\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:26 INFO 140146317055808] Iter[69] Batch [1000]#011Speed: 119462.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1000 train binary_classification_accuracy <score>=0.838411588412\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1000 train binary_classification_cross_entropy <loss>=0.37854327795\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1000 train binary_f_1.000 <score>=0.388672199797\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:30 INFO 140146317055808] Iter[69] Batch [1500]#011Speed: 121936.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1500 train binary_classification_accuracy <score>=0.838427714857\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1500 train binary_classification_cross_entropy <loss>=0.378167638363\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=1500 train binary_f_1.000 <score>=0.387464387464\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:34 INFO 140146317055808] Iter[69] Batch [2000]#011Speed: 120312.33 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=2000 train binary_classification_accuracy <score>=0.838231384308\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=2000 train binary_classification_cross_entropy <loss>=0.378381661845\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, batch=2000 train binary_f_1.000 <score>=0.3880822169\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, train binary_classification_accuracy <score>=0.838268907563\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, train binary_classification_cross_entropy <loss>=0.378278103343\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=69, train binary_f_1.000 <score>=0.387998182919\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20859.84206199646, \"sum\": 20859.84206199646, \"min\": 20859.84206199646}}, \"EndTime\": 1553541759.063426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541738.20318}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 174931, \"sum\": 174931.0, \"min\": 174931}, \"Total Records Seen\": {\"count\": 1, \"max\": 174909160, \"sum\": 174909160.0, \"min\": 174909160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 71, \"sum\": 71.0, \"min\": 71}}, \"EndTime\": 1553541759.063625, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 69}, \"StartTime\": 1553541738.203556}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119782.68426 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:22:39.063] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 70, \"duration\": 20858, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=0 train binary_classification_cross_entropy <loss>=0.391245056152\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=0 train binary_f_1.000 <score>=0.362962962963\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:43 INFO 140146317055808] Iter[70] Batch [500]#011Speed: 118189.15 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=500 train binary_classification_accuracy <score>=0.839191616766\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=500 train binary_classification_cross_entropy <loss>=0.377548928327\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=500 train binary_f_1.000 <score>=0.390900362141\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:47 INFO 140146317055808] Iter[70] Batch [1000]#011Speed: 118145.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1000 train binary_classification_accuracy <score>=0.838703296703\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1000 train binary_classification_cross_entropy <loss>=0.377988437374\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1000 train binary_f_1.000 <score>=0.390706134525\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:51 INFO 140146317055808] Iter[70] Batch [1500]#011Speed: 120898.40 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1500 train binary_classification_accuracy <score>=0.838726182545\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1500 train binary_classification_cross_entropy <loss>=0.377614712669\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=1500 train binary_f_1.000 <score>=0.389530279875\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:55 INFO 140146317055808] Iter[70] Batch [2000]#011Speed: 117703.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=2000 train binary_classification_accuracy <score>=0.83851924038\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=2000 train binary_classification_cross_entropy <loss>=0.377829009064\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:22:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, batch=2000 train binary_f_1.000 <score>=0.390070426656\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, train binary_classification_accuracy <score>=0.838551420568\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, train binary_classification_cross_entropy <loss>=0.377726209991\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=70, train binary_f_1.000 <score>=0.389930050474\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21087.013006210327, \"sum\": 21087.013006210327, \"min\": 21087.013006210327}}, \"EndTime\": 1553541780.150888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541759.063492}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 177430, \"sum\": 177430.0, \"min\": 177430}, \"Total Records Seen\": {\"count\": 1, \"max\": 177407848, \"sum\": 177407848.0, \"min\": 177407848}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1553541780.151106, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 70}, \"StartTime\": 1553541759.063849}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118492.069747 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:23:00.151] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 21086, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=0 train binary_classification_cross_entropy <loss>=0.390680297852\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=0 train binary_f_1.000 <score>=0.362962962963\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:04 INFO 140146317055808] Iter[71] Batch [500]#011Speed: 119324.25 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=500 train binary_classification_accuracy <score>=0.839500998004\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=500 train binary_classification_cross_entropy <loss>=0.376999603454\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=500 train binary_f_1.000 <score>=0.392948814737\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:08 INFO 140146317055808] Iter[71] Batch [1000]#011Speed: 121020.48 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1000 train binary_classification_accuracy <score>=0.838985014985\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1000 train binary_classification_cross_entropy <loss>=0.377434480802\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1000 train binary_f_1.000 <score>=0.39262754083\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:23:12 INFO 140146317055808] Iter[71] Batch [1500]#011Speed: 119563.42 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1500 train binary_classification_accuracy <score>=0.839001998668\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1500 train binary_classification_cross_entropy <loss>=0.377062652771\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=1500 train binary_f_1.000 <score>=0.391372458154\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:16 INFO 140146317055808] Iter[71] Batch [2000]#011Speed: 121902.78 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=2000 train binary_classification_accuracy <score>=0.838779610195\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=2000 train binary_classification_cross_entropy <loss>=0.377277231959\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, batch=2000 train binary_f_1.000 <score>=0.391867742422\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, train binary_classification_accuracy <score>=0.83881352541\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, train binary_classification_cross_entropy <loss>=0.377175210209\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=71, train binary_f_1.000 <score>=0.391725271025\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20840.540885925293, \"sum\": 20840.540885925293, \"min\": 20840.540885925293}}, \"EndTime\": 1553541800.991959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541780.150951}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:20 INFO 140146317055808] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 179929, \"sum\": 179929.0, \"min\": 179929}, \"Total Records Seen\": {\"count\": 1, \"max\": 179906536, \"sum\": 179906536.0, \"min\": 179906536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 73, \"sum\": 73.0, \"min\": 73}}, \"EndTime\": 1553541800.992197, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 71}, \"StartTime\": 1553541780.151387}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:20 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119893.284246 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:23:20.992] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 72, \"duration\": 20839, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=0 train binary_classification_accuracy <score>=0.828\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=0 train binary_classification_cross_entropy <loss>=0.390116638184\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=0 train binary_f_1.000 <score>=0.362962962963\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:25 INFO 140146317055808] Iter[72] Batch [500]#011Speed: 117198.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=500 train binary_classification_accuracy <score>=0.839726546906\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=500 train binary_classification_cross_entropy <loss>=0.376450990171\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=500 train binary_f_1.000 <score>=0.394464763772\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:29 INFO 140146317055808] Iter[72] Batch [1000]#011Speed: 120016.44 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1000 train binary_classification_accuracy <score>=0.83920979021\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1000 train binary_classification_cross_entropy <loss>=0.376881271591\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1000 train binary_f_1.000 <score>=0.39423558237\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:33 INFO 140146317055808] Iter[72] Batch [1500]#011Speed: 117891.67 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1500 train binary_classification_accuracy <score>=0.839250499667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1500 train binary_classification_cross_entropy <loss>=0.376511322062\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=1500 train binary_f_1.000 <score>=0.393071580753\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:37 INFO 140146317055808] Iter[72] Batch [2000]#011Speed: 120611.69 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=2000 train binary_classification_accuracy <score>=0.83903898051\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=2000 train binary_classification_cross_entropy <loss>=0.376726194764\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, batch=2000 train binary_f_1.000 <score>=0.393670192639\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, train binary_classification_accuracy <score>=0.839074029612\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, train binary_classification_cross_entropy <loss>=0.376624967809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=72, train binary_f_1.000 <score>=0.393529842891\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21014.17899131775, \"sum\": 21014.17899131775, \"min\": 21014.17899131775}}, \"EndTime\": 1553541822.006683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541800.992023}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 182428, \"sum\": 182428.0, \"min\": 182428}, \"Total Records Seen\": {\"count\": 1, \"max\": 182405224, \"sum\": 182405224.0, \"min\": 182405224}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1553541822.006912, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 72}, \"StartTime\": 1553541800.99246}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118902.595819 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:23:42.007] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 73, \"duration\": 21012, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=0 train binary_classification_cross_entropy <loss>=0.389554016113\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=0 train binary_f_1.000 <score>=0.364312267658\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:46 INFO 140146317055808] Iter[73] Batch [500]#011Speed: 117962.98 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=500 train binary_classification_accuracy <score>=0.839962075848\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=500 train binary_classification_cross_entropy <loss>=0.375902954528\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=500 train binary_f_1.000 <score>=0.395992316095\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:23:50 INFO 140146317055808] Iter[73] Batch [1000]#011Speed: 120567.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1000 train binary_classification_accuracy <score>=0.839457542458\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1000 train binary_classification_cross_entropy <loss>=0.376328679957\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1000 train binary_f_1.000 <score>=0.395828398919\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:54 INFO 140146317055808] Iter[73] Batch [1500]#011Speed: 120586.88 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1500 train binary_classification_accuracy <score>=0.839493004664\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1500 train binary_classification_cross_entropy <loss>=0.375960590563\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=1500 train binary_f_1.000 <score>=0.394666291454\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:58 INFO 140146317055808] Iter[73] Batch [2000]#011Speed: 118625.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=2000 train binary_classification_accuracy <score>=0.839269865067\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=2000 train binary_classification_cross_entropy <loss>=0.376175768881\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:23:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, batch=2000 train binary_f_1.000 <score>=0.395229848422\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, train binary_classification_accuracy <score>=0.839318927571\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, train binary_classification_cross_entropy <loss>=0.376075354617\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=73, train binary_f_1.000 <score>=0.395160585767\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20985.18395423889, \"sum\": 20985.18395423889, \"min\": 20985.18395423889}}, \"EndTime\": 1553541842.992416, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541822.006752}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:02 INFO 140146317055808] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 184927, \"sum\": 184927.0, \"min\": 184927}, \"Total Records Seen\": {\"count\": 1, \"max\": 184903912, \"sum\": 184903912.0, \"min\": 184903912}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 75, \"sum\": 75.0, \"min\": 75}}, \"EndTime\": 1553541842.992646, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 73}, \"StartTime\": 1553541822.007203}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:02 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119066.867143 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:24:02.992] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 20983, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=0 train binary_classification_accuracy <score>=0.829\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=0 train binary_classification_cross_entropy <loss>=0.388992370605\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=0 train binary_f_1.000 <score>=0.364312267658\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:07 INFO 140146317055808] Iter[74] Batch [500]#011Speed: 117499.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=500 train binary_classification_accuracy <score>=0.840175648703\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=500 train binary_classification_cross_entropy <loss>=0.375355371532\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=500 train binary_f_1.000 <score>=0.397655979659\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:11 INFO 140146317055808] Iter[74] Batch [1000]#011Speed: 119837.49 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1000 train binary_classification_accuracy <score>=0.839709290709\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1000 train binary_classification_cross_entropy <loss>=0.375776582304\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1000 train binary_f_1.000 <score>=0.397609223711\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:15 INFO 140146317055808] Iter[74] Batch [1500]#011Speed: 118763.75 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1500 train binary_classification_accuracy <score>=0.839741505663\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1500 train binary_classification_cross_entropy <loss>=0.375410338339\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=1500 train binary_f_1.000 <score>=0.39639060916\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:19 INFO 140146317055808] Iter[74] Batch [2000]#011Speed: 121540.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=2000 train binary_classification_accuracy <score>=0.83951924038\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=2000 train binary_classification_cross_entropy <loss>=0.375625835169\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, batch=2000 train binary_f_1.000 <score>=0.39695172564\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, train binary_classification_accuracy <score>=0.839577831132\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, train binary_classification_cross_entropy <loss>=0.375526251773\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=74, train binary_f_1.000 <score>=0.396902788664\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20870.548009872437, \"sum\": 20870.548009872437, \"min\": 20870.548009872437}}, \"EndTime\": 1553541863.863542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541842.992477}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 187426, \"sum\": 187426.0, \"min\": 187426}, \"Total Records Seen\": {\"count\": 1, \"max\": 187402600, \"sum\": 187402600.0, \"min\": 187402600}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1553541863.863764, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 74}, \"StartTime\": 1553541842.992953}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119720.845175 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:24:23.863] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 75, \"duration\": 20869, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=0 train binary_classification_accuracy <score>=0.83\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=0 train binary_classification_cross_entropy <loss>=0.388431610107\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=0 train binary_f_1.000 <score>=0.37037037037\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:24:28 INFO 140146317055808] Iter[75] Batch [500]#011Speed: 118129.08 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=500 train binary_classification_accuracy <score>=0.840429141717\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=500 train binary_classification_cross_entropy <loss>=0.374808135134\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=500 train binary_f_1.000 <score>=0.399546345604\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:32 INFO 140146317055808] Iter[75] Batch [1000]#011Speed: 121383.05 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1000 train binary_classification_accuracy <score>=0.839958041958\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1000 train binary_classification_cross_entropy <loss>=0.375224869887\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1000 train binary_f_1.000 <score>=0.399398656349\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:36 INFO 140146317055808] Iter[75] Batch [1500]#011Speed: 120797.60 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1500 train binary_classification_accuracy <score>=0.839983344437\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1500 train binary_classification_cross_entropy <loss>=0.374860455009\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=1500 train binary_f_1.000 <score>=0.398133646731\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:40 INFO 140146317055808] Iter[75] Batch [2000]#011Speed: 117724.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=2000 train binary_classification_accuracy <score>=0.839755622189\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=2000 train binary_classification_cross_entropy <loss>=0.375076284566\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, batch=2000 train binary_f_1.000 <score>=0.398663233482\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, train binary_classification_accuracy <score>=0.839804321729\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, train binary_classification_cross_entropy <loss>=0.374977549943\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=75, train binary_f_1.000 <score>=0.398550496013\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20943.109035491943, \"sum\": 20943.109035491943, \"min\": 20943.109035491943}}, \"EndTime\": 1553541884.807182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541863.863609}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 189925, \"sum\": 189925.0, \"min\": 189925}, \"Total Records Seen\": {\"count\": 1, \"max\": 189901288, \"sum\": 189901288.0, \"min\": 189901288}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 77, \"sum\": 77.0, \"min\": 77}}, \"EndTime\": 1553541884.807414, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 75}, \"StartTime\": 1553541863.864043}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119306.154659 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:24:44.807] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 76, \"duration\": 20941, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=0 train binary_classification_accuracy <score>=0.83\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=0 train binary_classification_cross_entropy <loss>=0.387871704102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=0 train binary_f_1.000 <score>=0.37037037037\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:49 INFO 140146317055808] Iter[76] Batch [500]#011Speed: 119509.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=500 train binary_classification_accuracy <score>=0.84069261477\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=500 train binary_classification_cross_entropy <loss>=0.374261139769\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=500 train binary_f_1.000 <score>=0.401194415059\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:53 INFO 140146317055808] Iter[76] Batch [1000]#011Speed: 120318.12 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1000 train binary_classification_accuracy <score>=0.840250749251\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1000 train binary_classification_cross_entropy <loss>=0.37467343978\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1000 train binary_f_1.000 <score>=0.401213233229\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:57 INFO 140146317055808] Iter[76] Batch [1500]#011Speed: 118919.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1500 train binary_classification_accuracy <score>=0.840261159227\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1500 train binary_classification_cross_entropy <loss>=0.374310839527\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:24:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=1500 train binary_f_1.000 <score>=0.399898885235\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:01 INFO 140146317055808] Iter[76] Batch [2000]#011Speed: 120556.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=2000 train binary_classification_accuracy <score>=0.840031984008\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=2000 train binary_classification_cross_entropy <loss>=0.374527016414\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, batch=2000 train binary_f_1.000 <score>=0.400421077869\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, train binary_classification_accuracy <score>=0.840075630252\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, train binary_classification_cross_entropy <loss>=0.374429147978\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=76, train binary_f_1.000 <score>=0.400288113084\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20866.37306213379, \"sum\": 20866.37306213379, \"min\": 20866.37306213379}}, \"EndTime\": 1553541905.6741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541884.807247}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 192424, \"sum\": 192424.0, \"min\": 192424}, \"Total Records Seen\": {\"count\": 1, \"max\": 192399976, \"sum\": 192399976.0, \"min\": 192399976}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1553541905.674322, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 76}, \"StartTime\": 1553541884.807696}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119744.940771 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:25:05.674] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 20865, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=0 train binary_classification_accuracy <score>=0.83\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=0 train binary_classification_cross_entropy <loss>=0.3873125\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=0 train binary_f_1.000 <score>=0.375\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:25:09 INFO 140146317055808] Iter[77] Batch [500]#011Speed: 119388.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=500 train binary_classification_accuracy <score>=0.840880239521\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=500 train binary_classification_cross_entropy <loss>=0.37371428688\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=500 train binary_f_1.000 <score>=0.402733137039\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:14 INFO 140146317055808] Iter[77] Batch [1000]#011Speed: 120591.54 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1000 train binary_classification_accuracy <score>=0.840484515485\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1000 train binary_classification_cross_entropy <loss>=0.374122196041\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1000 train binary_f_1.000 <score>=0.402863115695\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:18 INFO 140146317055808] Iter[77] Batch [1500]#011Speed: 118942.21 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1500 train binary_classification_accuracy <score>=0.840493004664\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1500 train binary_classification_cross_entropy <loss>=0.373761396292\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=1500 train binary_f_1.000 <score>=0.401515833258\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:22 INFO 140146317055808] Iter[77] Batch [2000]#011Speed: 121536.63 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=2000 train binary_classification_accuracy <score>=0.840262868566\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=2000 train binary_classification_cross_entropy <loss>=0.373977936005\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, batch=2000 train binary_f_1.000 <score>=0.402048085485\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, train binary_classification_accuracy <score>=0.840323729492\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, train binary_classification_cross_entropy <loss>=0.373880951636\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=77, train binary_f_1.000 <score>=0.401981251546\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20824.93805885315, \"sum\": 20824.93805885315, \"min\": 20824.93805885315}}, \"EndTime\": 1553541926.499573, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541905.674165}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 194923, \"sum\": 194923.0, \"min\": 194923}, \"Total Records Seen\": {\"count\": 1, \"max\": 194898664, \"sum\": 194898664.0, \"min\": 194898664}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 79, \"sum\": 79.0, \"min\": 79}}, \"EndTime\": 1553541926.499795, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 77}, \"StartTime\": 1553541905.674601}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119983.168802 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:25:26.500] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 78, \"duration\": 20823, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=0 train binary_classification_cross_entropy <loss>=0.386753967285\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=0 train binary_f_1.000 <score>=0.392727272727\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:30 INFO 140146317055808] Iter[78] Batch [500]#011Speed: 119714.94 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=500 train binary_classification_accuracy <score>=0.841161676647\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=500 train binary_classification_cross_entropy <loss>=0.373167496489\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=500 train binary_f_1.000 <score>=0.404481096776\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:34 INFO 140146317055808] Iter[78] Batch [1000]#011Speed: 118173.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1000 train binary_classification_accuracy <score>=0.840729270729\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1000 train binary_classification_cross_entropy <loss>=0.373571053977\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1000 train binary_f_1.000 <score>=0.404538697702\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:39 INFO 140146317055808] Iter[78] Batch [1500]#011Speed: 119914.86 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1500 train binary_classification_accuracy <score>=0.840746835443\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1500 train binary_classification_cross_entropy <loss>=0.373212040972\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=1500 train binary_f_1.000 <score>=0.403206679\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:43 INFO 140146317055808] Iter[78] Batch [2000]#011Speed: 120271.70 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=2000 train binary_classification_accuracy <score>=0.840507246377\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=2000 train binary_classification_cross_entropy <loss>=0.373428959852\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, batch=2000 train binary_f_1.000 <score>=0.403709149747\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, train binary_classification_accuracy <score>=0.840578231293\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, train binary_classification_cross_entropy <loss>=0.373332877841\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=78, train binary_f_1.000 <score>=0.403676178929\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20864.871978759766, \"sum\": 20864.871978759766, \"min\": 20864.871978759766}}, \"EndTime\": 1553541947.364977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541926.499635}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 197422, \"sum\": 197422.0, \"min\": 197422}, \"Total Records Seen\": {\"count\": 1, \"max\": 197397352, \"sum\": 197397352.0, \"min\": 197397352}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1553541947.365217, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 78}, \"StartTime\": 1553541926.500072}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119753.389862 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:25:47.365] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 79, \"duration\": 20863, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=0 train binary_classification_cross_entropy <loss>=0.386196136475\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:25:51 INFO 140146317055808] Iter[79] Batch [500]#011Speed: 119191.59 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=500 train binary_classification_accuracy <score>=0.841357285429\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=500 train binary_classification_cross_entropy <loss>=0.372620688311\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=500 train binary_f_1.000 <score>=0.406014588066\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:55 INFO 140146317055808] Iter[79] Batch [1000]#011Speed: 119657.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1000 train binary_classification_accuracy <score>=0.840972027972\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1000 train binary_classification_cross_entropy <loss>=0.373019935997\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1000 train binary_f_1.000 <score>=0.406193696634\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:59 INFO 140146317055808] Iter[79] Batch [1500]#011Speed: 118082.32 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1500 train binary_classification_accuracy <score>=0.840995336442\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1500 train binary_classification_cross_entropy <loss>=0.37266269848\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:25:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=1500 train binary_f_1.000 <score>=0.40487043428\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:04 INFO 140146317055808] Iter[79] Batch [2000]#011Speed: 120147.60 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=2000 train binary_classification_accuracy <score>=0.84075962019\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=2000 train binary_classification_cross_entropy <loss>=0.372880013164\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, batch=2000 train binary_f_1.000 <score>=0.405380389347\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, train binary_classification_accuracy <score>=0.840825530212\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, train binary_classification_cross_entropy <loss>=0.37278485028\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=79, train binary_f_1.000 <score>=0.405343546827\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20938.62295150757, \"sum\": 20938.62295150757, \"min\": 20938.62295150757}}, \"EndTime\": 1553541968.304168, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541947.365047}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 199921, \"sum\": 199921.0, \"min\": 199921}, \"Total Records Seen\": {\"count\": 1, \"max\": 199896040, \"sum\": 199896040.0, \"min\": 199896040}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 81, \"sum\": 81.0, \"min\": 81}}, \"EndTime\": 1553541968.304358, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 79}, \"StartTime\": 1553541947.365516}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119332.006213 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:26:08.304] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 20937, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=0 train binary_classification_cross_entropy <loss>=0.385638946533\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:12 INFO 140146317055808] Iter[80] Batch [500]#011Speed: 118560.98 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=500 train binary_classification_accuracy <score>=0.841580838323\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=500 train binary_classification_cross_entropy <loss>=0.372073791626\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=500 train binary_f_1.000 <score>=0.407613076579\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:16 INFO 140146317055808] Iter[80] Batch [1000]#011Speed: 120031.34 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1000 train binary_classification_accuracy <score>=0.841204795205\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1000 train binary_classification_cross_entropy <loss>=0.372468771158\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1000 train binary_f_1.000 <score>=0.407802871683\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:20 INFO 140146317055808] Iter[80] Batch [1500]#011Speed: 118809.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1500 train binary_classification_accuracy <score>=0.841231845436\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1500 train binary_classification_cross_entropy <loss>=0.372113296641\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:20 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=1500 train binary_f_1.000 <score>=0.406491187436\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:25 INFO 140146317055808] Iter[80] Batch [2000]#011Speed: 118482.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=2000 train binary_classification_accuracy <score>=0.840995502249\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=2000 train binary_classification_cross_entropy <loss>=0.372331024932\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, batch=2000 train binary_f_1.000 <score>=0.406973883351\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, train binary_classification_accuracy <score>=0.841066026411\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, train binary_classification_cross_entropy <loss>=0.372236798565\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=80, train binary_f_1.000 <score>=0.406955701155\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20935.850143432617, \"sum\": 20935.850143432617, \"min\": 20935.850143432617}}, \"EndTime\": 1553541989.240486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541968.304234}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 202420, \"sum\": 202420.0, \"min\": 202420}, \"Total Records Seen\": {\"count\": 1, \"max\": 202394728, \"sum\": 202394728.0, \"min\": 202394728}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1553541989.240683, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 80}, \"StartTime\": 1553541968.304605}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119347.824246 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:26:29.240] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 81, \"duration\": 20934, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=0 train binary_classification_cross_entropy <loss>=0.385082336426\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:33 INFO 140146317055808] Iter[81] Batch [500]#011Speed: 118776.72 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=500 train binary_classification_accuracy <score>=0.841856287425\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=500 train binary_classification_cross_entropy <loss>=0.371526749419\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=500 train binary_f_1.000 <score>=0.409339635301\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:26:37 INFO 140146317055808] Iter[81] Batch [1000]#011Speed: 119269.60 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1000 train binary_classification_accuracy <score>=0.8415004995\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1000 train binary_classification_cross_entropy <loss>=0.371917500133\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1000 train binary_f_1.000 <score>=0.409653437716\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:41 INFO 140146317055808] Iter[81] Batch [1500]#011Speed: 118013.26 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1500 train binary_classification_accuracy <score>=0.841504330446\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1500 train binary_classification_cross_entropy <loss>=0.371563778323\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=1500 train binary_f_1.000 <score>=0.408239309096\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:46 INFO 140146317055808] Iter[81] Batch [2000]#011Speed: 120719.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=2000 train binary_classification_accuracy <score>=0.841257371314\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=2000 train binary_classification_cross_entropy <loss>=0.371781937004\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, batch=2000 train binary_f_1.000 <score>=0.408664765323\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, train binary_classification_accuracy <score>=0.841322529012\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, train binary_classification_cross_entropy <loss>=0.371688664064\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=81, train binary_f_1.000 <score>=0.408621875033\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20968.37282180786, \"sum\": 20968.37282180786, \"min\": 20968.37282180786}}, \"EndTime\": 1553542010.209349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553541989.240565}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 204919, \"sum\": 204919.0, \"min\": 204919}, \"Total Records Seen\": {\"count\": 1, \"max\": 204893416, \"sum\": 204893416.0, \"min\": 204893416}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 83, \"sum\": 83.0, \"min\": 83}}, \"EndTime\": 1553542010.209531, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 81}, \"StartTime\": 1553541989.240949}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119162.814814 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:26:50.209] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 82, \"duration\": 20966, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=0 train binary_classification_cross_entropy <loss>=0.384526306152\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:54 INFO 140146317055808] Iter[82] Batch [500]#011Speed: 119589.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=500 train binary_classification_accuracy <score>=0.842067864271\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=500 train binary_classification_cross_entropy <loss>=0.370979505588\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=500 train binary_f_1.000 <score>=0.410788752532\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:58 INFO 140146317055808] Iter[82] Batch [1000]#011Speed: 120250.64 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1000 train binary_classification_accuracy <score>=0.841736263736\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1000 train binary_classification_cross_entropy <loss>=0.37136606716\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:26:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1000 train binary_f_1.000 <score>=0.411258854047\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:02 INFO 140146317055808] Iter[82] Batch [1500]#011Speed: 118401.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1500 train binary_classification_accuracy <score>=0.841724183877\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1500 train binary_classification_cross_entropy <loss>=0.371014087146\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=1500 train binary_f_1.000 <score>=0.409767804704\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:06 INFO 140146317055808] Iter[82] Batch [2000]#011Speed: 120287.93 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=2000 train binary_classification_accuracy <score>=0.841492253873\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=2000 train binary_classification_cross_entropy <loss>=0.371232694154\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:06 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, batch=2000 train binary_f_1.000 <score>=0.410275289402\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, train binary_classification_accuracy <score>=0.84156462585\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, train binary_classification_cross_entropy <loss>=0.371140391484\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=82, train binary_f_1.000 <score>=0.410251521552\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20996.280908584595, \"sum\": 20996.280908584595, \"min\": 20996.280908584595}}, \"EndTime\": 1553542031.206059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542010.209409}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 207418, \"sum\": 207418.0, \"min\": 207418}, \"Total Records Seen\": {\"count\": 1, \"max\": 207392104, \"sum\": 207392104.0, \"min\": 207392104}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1553542031.206275, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 82}, \"StartTime\": 1553542010.209746}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119004.120203 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:27:11.206] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 20995, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=0 train binary_classification_cross_entropy <loss>=0.383970855713\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:27:15 INFO 140146317055808] Iter[83] Batch [500]#011Speed: 118447.62 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=500 train binary_classification_accuracy <score>=0.842321357285\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=500 train binary_classification_cross_entropy <loss>=0.370432016887\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=500 train binary_f_1.000 <score>=0.412473876407\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:19 INFO 140146317055808] Iter[83] Batch [1000]#011Speed: 118784.29 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1000 train binary_classification_accuracy <score>=0.84197002997\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1000 train binary_classification_cross_entropy <loss>=0.370814426418\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1000 train binary_f_1.000 <score>=0.412857248905\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:23 INFO 140146317055808] Iter[83] Batch [1500]#011Speed: 119150.71 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1500 train binary_classification_accuracy <score>=0.841966022652\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1500 train binary_classification_cross_entropy <loss>=0.370464178869\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=1500 train binary_f_1.000 <score>=0.411396441218\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:28 INFO 140146317055808] Iter[83] Batch [2000]#011Speed: 119959.36 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=2000 train binary_classification_accuracy <score>=0.841733633183\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=2000 train binary_classification_cross_entropy <loss>=0.370683252353\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, batch=2000 train binary_f_1.000 <score>=0.411889565061\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, train binary_classification_accuracy <score>=0.84181072429\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, train binary_classification_cross_entropy <loss>=0.370591936665\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=83, train binary_f_1.000 <score>=0.411888535311\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20943.989992141724, \"sum\": 20943.989992141724, \"min\": 20943.989992141724}}, \"EndTime\": 1553542052.150572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542031.206129}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 209917, \"sum\": 209917.0, \"min\": 209917}, \"Total Records Seen\": {\"count\": 1, \"max\": 209890792, \"sum\": 209890792.0, \"min\": 209890792}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 85, \"sum\": 85.0, \"min\": 85}}, \"EndTime\": 1553542052.150791, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 83}, \"StartTime\": 1553542031.20655}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119301.143227 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:27:32.151] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 84, \"duration\": 20942, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=0 train binary_classification_cross_entropy <loss>=0.383416015625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:36 INFO 140146317055808] Iter[84] Batch [500]#011Speed: 118313.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=500 train binary_classification_accuracy <score>=0.84255489022\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=500 train binary_classification_cross_entropy <loss>=0.369884240735\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=500 train binary_f_1.000 <score>=0.413958602654\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:40 INFO 140146317055808] Iter[84] Batch [1000]#011Speed: 119953.09 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1000 train binary_classification_accuracy <score>=0.842238761239\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1000 train binary_classification_cross_entropy <loss>=0.370262536627\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1000 train binary_f_1.000 <score>=0.414492441966\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:44 INFO 140146317055808] Iter[84] Batch [1500]#011Speed: 120911.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1500 train binary_classification_accuracy <score>=0.842225849434\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1500 train binary_classification_cross_entropy <loss>=0.369914012281\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=1500 train binary_f_1.000 <score>=0.413071979657\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:48 INFO 140146317055808] Iter[84] Batch [2000]#011Speed: 120284.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=2000 train binary_classification_accuracy <score>=0.841988005997\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=2000 train binary_classification_cross_entropy <loss>=0.37013357033\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:48 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, batch=2000 train binary_f_1.000 <score>=0.413567721761\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, train binary_classification_accuracy <score>=0.842069627851\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, train binary_classification_cross_entropy <loss>=0.370043257318\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=84, train binary_f_1.000 <score>=0.413574321774\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20790.41814804077, \"sum\": 20790.41814804077, \"min\": 20790.41814804077}}, \"EndTime\": 1553542072.941524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542052.150636}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 212416, \"sum\": 212416.0, \"min\": 212416}, \"Total Records Seen\": {\"count\": 1, \"max\": 212389480, \"sum\": 212389480.0, \"min\": 212389480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1553542072.941707, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 84}, \"StartTime\": 1553542052.151076}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120182.742014 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:27:52.941] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 85, \"duration\": 20789, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=0 train binary_classification_cross_entropy <loss>=0.382861755371\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:52 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=0 train binary_f_1.000 <score>=0.398550724638\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:27:57 INFO 140146317055808] Iter[85] Batch [500]#011Speed: 118215.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=500 train binary_classification_accuracy <score>=0.842840319361\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=500 train binary_classification_cross_entropy <loss>=0.36933614625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:27:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=500 train binary_f_1.000 <score>=0.415650534722\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:01 INFO 140146317055808] Iter[85] Batch [1000]#011Speed: 118370.34 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1000 train binary_classification_accuracy <score>=0.842533466533\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1000 train binary_classification_cross_entropy <loss>=0.369710363154\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1000 train binary_f_1.000 <score>=0.41619443395\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:05 INFO 140146317055808] Iter[85] Batch [1500]#011Speed: 119099.99 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1500 train binary_classification_accuracy <score>=0.842509660227\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1500 train binary_classification_cross_entropy <loss>=0.36936355481\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=1500 train binary_f_1.000 <score>=0.414763063623\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:09 INFO 140146317055808] Iter[85] Batch [2000]#011Speed: 120361.42 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=2000 train binary_classification_accuracy <score>=0.842268865567\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=2000 train binary_classification_cross_entropy <loss>=0.369583614702\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:09 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, batch=2000 train binary_f_1.000 <score>=0.415245633147\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, train binary_classification_accuracy <score>=0.842326930772\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, train binary_classification_cross_entropy <loss>=0.369494320348\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=85, train binary_f_1.000 <score>=0.415171912222\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20984.101057052612, \"sum\": 20984.101057052612, \"min\": 20984.101057052612}}, \"EndTime\": 1553542093.926053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542072.941582}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 214915, \"sum\": 214915.0, \"min\": 214915}, \"Total Records Seen\": {\"count\": 1, \"max\": 214888168, \"sum\": 214888168.0, \"min\": 214888168}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 87, \"sum\": 87.0, \"min\": 87}}, \"EndTime\": 1553542093.926249, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 85}, \"StartTime\": 1553542072.941924}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119073.394405 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:28:13.926] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 20983, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=0 train binary_classification_cross_entropy <loss>=0.382308105469\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:13 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=0 train binary_f_1.000 <score>=0.397111913357\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:18 INFO 140146317055808] Iter[86] Batch [500]#011Speed: 114047.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=500 train binary_classification_accuracy <score>=0.843103792415\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=500 train binary_classification_cross_entropy <loss>=0.368787708519\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=500 train binary_f_1.000 <score>=0.417330714206\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:22 INFO 140146317055808] Iter[86] Batch [1000]#011Speed: 119499.93 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1000 train binary_classification_accuracy <score>=0.842787212787\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1000 train binary_classification_cross_entropy <loss>=0.369157880694\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1000 train binary_f_1.000 <score>=0.417838117786\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:26 INFO 140146317055808] Iter[86] Batch [1500]#011Speed: 119305.35 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1500 train binary_classification_accuracy <score>=0.842776149234\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1500 train binary_classification_cross_entropy <loss>=0.368812780331\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=1500 train binary_f_1.000 <score>=0.416423708737\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:30 INFO 140146317055808] Iter[86] Batch [2000]#011Speed: 120358.66 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=2000 train binary_classification_accuracy <score>=0.842526736632\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=2000 train binary_classification_cross_entropy <loss>=0.36903335916\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:30 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, batch=2000 train binary_f_1.000 <score>=0.416871465372\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, train binary_classification_accuracy <score>=0.842591436575\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, train binary_classification_cross_entropy <loss>=0.368945098804\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=86, train binary_f_1.000 <score>=0.416829002885\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20997.920036315918, \"sum\": 20997.920036315918, \"min\": 20997.920036315918}}, \"EndTime\": 1553542114.924466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542093.926116}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 217414, \"sum\": 217414.0, \"min\": 217414}, \"Total Records Seen\": {\"count\": 1, \"max\": 217386856, \"sum\": 217386856.0, \"min\": 217386856}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1553542114.924668, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 86}, \"StartTime\": 1553542093.926483}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118994.792904 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:28:34.924] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 87, \"duration\": 20996, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=0 train binary_classification_cross_entropy <loss>=0.3817550354\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:34 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=0 train binary_f_1.000 <score>=0.397111913357\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:39 INFO 140146317055808] Iter[87] Batch [500]#011Speed: 118389.38 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=500 train binary_classification_accuracy <score>=0.843343313373\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=500 train binary_classification_cross_entropy <loss>=0.368238902749\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=500 train binary_f_1.000 <score>=0.418840568979\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:28:43 INFO 140146317055808] Iter[87] Batch [1000]#011Speed: 120812.32 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1000 train binary_classification_accuracy <score>=0.843040959041\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1000 train binary_classification_cross_entropy <loss>=0.368605062936\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1000 train binary_f_1.000 <score>=0.419482131773\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:47 INFO 140146317055808] Iter[87] Batch [1500]#011Speed: 120938.43 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1500 train binary_classification_accuracy <score>=0.843039973351\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1500 train binary_classification_cross_entropy <loss>=0.368261664099\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=1500 train binary_f_1.000 <score>=0.418108036682\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:51 INFO 140146317055808] Iter[87] Batch [2000]#011Speed: 121146.36 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=2000 train binary_classification_accuracy <score>=0.842786606697\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=2000 train binary_classification_cross_entropy <loss>=0.368482779606\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, batch=2000 train binary_f_1.000 <score>=0.418567600037\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, train binary_classification_accuracy <score>=0.842850340136\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, train binary_classification_cross_entropy <loss>=0.368395568652\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=87, train binary_f_1.000 <score>=0.418487362606\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20789.08896446228, \"sum\": 20789.08896446228, \"min\": 20789.08896446228}}, \"EndTime\": 1553542135.714019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542114.924537}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 219913, \"sum\": 219913.0, \"min\": 219913}, \"Total Records Seen\": {\"count\": 1, \"max\": 219885544, \"sum\": 219885544.0, \"min\": 219885544}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 89, \"sum\": 89.0, \"min\": 89}}, \"EndTime\": 1553542135.714247, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 87}, \"StartTime\": 1553542114.924899}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=120190.077242 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:28:55.714] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 88, \"duration\": 20788, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=0 train binary_classification_cross_entropy <loss>=0.381202636719\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:55 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=0 train binary_f_1.000 <score>=0.397111913357\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:59 INFO 140146317055808] Iter[88] Batch [500]#011Speed: 119370.12 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=500 train binary_classification_accuracy <score>=0.843502994012\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=500 train binary_classification_cross_entropy <loss>=0.367689712799\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:28:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=500 train binary_f_1.000 <score>=0.41999985205\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:04 INFO 140146317055808] Iter[88] Batch [1000]#011Speed: 117858.84 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1000 train binary_classification_accuracy <score>=0.843250749251\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1000 train binary_classification_cross_entropy <loss>=0.368051892809\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1000 train binary_f_1.000 <score>=0.420899951282\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:08 INFO 140146317055808] Iter[88] Batch [1500]#011Speed: 117958.83 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1500 train binary_classification_accuracy <score>=0.843265156562\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1500 train binary_classification_cross_entropy <loss>=0.367710190073\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=1500 train binary_f_1.000 <score>=0.419587941707\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:12 INFO 140146317055808] Iter[88] Batch [2000]#011Speed: 119136.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=2000 train binary_classification_accuracy <score>=0.843018490755\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=2000 train binary_classification_cross_entropy <loss>=0.36793185925\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, batch=2000 train binary_f_1.000 <score>=0.420091863711\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, train binary_classification_accuracy <score>=0.843089635854\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, train binary_classification_cross_entropy <loss>=0.367845712113\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=88, train binary_f_1.000 <score>=0.420051262559\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21086.92693710327, \"sum\": 21086.92693710327, \"min\": 21086.92693710327}}, \"EndTime\": 1553542156.801483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542135.714088}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 222412, \"sum\": 222412.0, \"min\": 222412}, \"Total Records Seen\": {\"count\": 1, \"max\": 222384232, \"sum\": 222384232.0, \"min\": 222384232}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1553542156.801677, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 88}, \"StartTime\": 1553542135.714526}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118492.761035 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:29:16.801] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 21085, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=0 train binary_classification_cross_entropy <loss>=0.380650909424\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:16 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=0 train binary_f_1.000 <score>=0.402877697842\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:21 INFO 140146317055808] Iter[89] Batch [500]#011Speed: 118619.72 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=500 train binary_classification_accuracy <score>=0.843804391218\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=500 train binary_classification_cross_entropy <loss>=0.367140130079\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=500 train binary_f_1.000 <score>=0.421779866407\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:29:25 INFO 140146317055808] Iter[89] Batch [1000]#011Speed: 118796.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1000 train binary_classification_accuracy <score>=0.843568431568\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1000 train binary_classification_cross_entropy <loss>=0.367498356929\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1000 train binary_f_1.000 <score>=0.422746844403\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:29 INFO 140146317055808] Iter[89] Batch [1500]#011Speed: 120897.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1500 train binary_classification_accuracy <score>=0.843565622918\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1500 train binary_classification_cross_entropy <loss>=0.367158344368\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=1500 train binary_f_1.000 <score>=0.42138439171\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:33 INFO 140146317055808] Iter[89] Batch [2000]#011Speed: 120605.21 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=2000 train binary_classification_accuracy <score>=0.843305347326\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=2000 train binary_classification_cross_entropy <loss>=0.367380583009\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, batch=2000 train binary_f_1.000 <score>=0.4217792203\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, train binary_classification_accuracy <score>=0.84337494998\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, train binary_classification_cross_entropy <loss>=0.367295514031\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=89, train binary_f_1.000 <score>=0.421734415582\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20833.648920059204, \"sum\": 20833.648920059204, \"min\": 20833.648920059204}}, \"EndTime\": 1553542177.635577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542156.801551}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 224911, \"sum\": 224911.0, \"min\": 224911}, \"Total Records Seen\": {\"count\": 1, \"max\": 224882920, \"sum\": 224882920.0, \"min\": 224882920}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 91, \"sum\": 91.0, \"min\": 91}}, \"EndTime\": 1553542177.635813, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 89}, \"StartTime\": 1553542156.801895}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119932.938455 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:29:37.636] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 90, \"duration\": 20832, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=0 train binary_classification_cross_entropy <loss>=0.380099853516\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:37 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=0 train binary_f_1.000 <score>=0.402877697842\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:41 INFO 140146317055808] Iter[90] Batch [500]#011Speed: 117612.97 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=500 train binary_classification_accuracy <score>=0.844081836327\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=500 train binary_classification_cross_entropy <loss>=0.366590139545\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:41 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=500 train binary_f_1.000 <score>=0.423492771058\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:46 INFO 140146317055808] Iter[90] Batch [1000]#011Speed: 118159.12 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1000 train binary_classification_accuracy <score>=0.843831168831\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1000 train binary_classification_cross_entropy <loss>=0.366944443252\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1000 train binary_f_1.000 <score>=0.424312729014\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:50 INFO 140146317055808] Iter[90] Batch [1500]#011Speed: 120457.29 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1500 train binary_classification_accuracy <score>=0.843816122585\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1500 train binary_classification_cross_entropy <loss>=0.366606117731\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=1500 train binary_f_1.000 <score>=0.422916728207\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:54 INFO 140146317055808] Iter[90] Batch [2000]#011Speed: 117604.31 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=2000 train binary_classification_accuracy <score>=0.84355922039\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=2000 train binary_classification_cross_entropy <loss>=0.366828941791\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, batch=2000 train binary_f_1.000 <score>=0.423337091319\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, train binary_classification_accuracy <score>=0.84362745098\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, train binary_classification_cross_entropy <loss>=0.366744964795\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=90, train binary_f_1.000 <score>=0.423298007831\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21047.07098007202, \"sum\": 21047.07098007202, \"min\": 21047.07098007202}}, \"EndTime\": 1553542198.683221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542177.635652}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 227410, \"sum\": 227410.0, \"min\": 227410}, \"Total Records Seen\": {\"count\": 1, \"max\": 227381608, \"sum\": 227381608.0, \"min\": 227381608}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1553542198.68342, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 90}, \"StartTime\": 1553542177.63612}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118717.13013 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:29:58.683] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 91, \"duration\": 21046, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=0 train binary_classification_cross_entropy <loss>=0.379549499512\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:29:58 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=0 train binary_f_1.000 <score>=0.402877697842\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:02 INFO 140146317055808] Iter[91] Batch [500]#011Speed: 117779.86 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=500 train binary_classification_accuracy <score>=0.844389221557\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=500 train binary_classification_cross_entropy <loss>=0.366039740344\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:02 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=500 train binary_f_1.000 <score>=0.425451945965\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:30:07 INFO 140146317055808] Iter[91] Batch [1000]#011Speed: 119590.33 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1000 train binary_classification_accuracy <score>=0.844120879121\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1000 train binary_classification_cross_entropy <loss>=0.3663901462\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1000 train binary_f_1.000 <score>=0.426128819893\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:11 INFO 140146317055808] Iter[91] Batch [1500]#011Speed: 118978.87 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1500 train binary_classification_accuracy <score>=0.844101265823\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1500 train binary_classification_cross_entropy <loss>=0.366053503699\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:11 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=1500 train binary_f_1.000 <score>=0.424650124411\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:15 INFO 140146317055808] Iter[91] Batch [2000]#011Speed: 118099.03 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=2000 train binary_classification_accuracy <score>=0.84384057971\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=2000 train binary_classification_cross_entropy <loss>=0.366276927698\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:15 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, batch=2000 train binary_f_1.000 <score>=0.425034086706\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, train binary_classification_accuracy <score>=0.843890756303\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, train binary_classification_cross_entropy <loss>=0.366194056346\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=91, train binary_f_1.000 <score>=0.424914094666\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21014.392852783203, \"sum\": 21014.392852783203, \"min\": 21014.392852783203}}, \"EndTime\": 1553542219.698062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542198.68329}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 229909, \"sum\": 229909.0, \"min\": 229909}, \"Total Records Seen\": {\"count\": 1, \"max\": 229880296, \"sum\": 229880296.0, \"min\": 229880296}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 93, \"sum\": 93.0, \"min\": 93}}, \"EndTime\": 1553542219.698243, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 91}, \"StartTime\": 1553542198.683642}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118901.888949 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:30:19.698] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 21013, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=0 train binary_classification_accuracy <score>=0.835\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=0 train binary_classification_cross_entropy <loss>=0.37899987793\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:19 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=0 train binary_f_1.000 <score>=0.408602150538\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:23 INFO 140146317055808] Iter[92] Batch [500]#011Speed: 119431.45 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=500 train binary_classification_accuracy <score>=0.844616766467\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=500 train binary_classification_cross_entropy <loss>=0.365488928028\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:23 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=500 train binary_f_1.000 <score>=0.426959344566\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:28 INFO 140146317055808] Iter[92] Batch [1000]#011Speed: 117618.55 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1000 train binary_classification_accuracy <score>=0.844381618382\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1000 train binary_classification_cross_entropy <loss>=0.365835461139\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:28 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1000 train binary_f_1.000 <score>=0.427793736271\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:32 INFO 140146317055808] Iter[92] Batch [1500]#011Speed: 118328.38 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1500 train binary_classification_accuracy <score>=0.844368421053\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1500 train binary_classification_cross_entropy <loss>=0.365500500135\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:32 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=1500 train binary_f_1.000 <score>=0.426306082404\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:36 INFO 140146317055808] Iter[92] Batch [2000]#011Speed: 118596.53 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=2000 train binary_classification_accuracy <score>=0.844109945027\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=2000 train binary_classification_cross_entropy <loss>=0.365724537862\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:36 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, batch=2000 train binary_f_1.000 <score>=0.426683085519\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, train binary_classification_accuracy <score>=0.844154861945\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, train binary_classification_cross_entropy <loss>=0.365642784922\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=92, train binary_f_1.000 <score>=0.426531198233\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21013.005018234253, \"sum\": 21013.005018234253, \"min\": 21013.005018234253}}, \"EndTime\": 1553542240.71151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542219.698122}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 232408, \"sum\": 232408.0, \"min\": 232408}, \"Total Records Seen\": {\"count\": 1, \"max\": 232378984, \"sum\": 232378984.0, \"min\": 232378984}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1553542240.711725, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 92}, \"StartTime\": 1553542219.698473}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118909.431576 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:30:40.711] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 93, \"duration\": 21012, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=0 train binary_classification_accuracy <score>=0.836\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=0 train binary_classification_cross_entropy <loss>=0.378451049805\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:40 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=0 train binary_f_1.000 <score>=0.414285714286\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:44 INFO 140146317055808] Iter[93] Batch [500]#011Speed: 119183.46 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=500 train binary_classification_accuracy <score>=0.844882235529\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=500 train binary_classification_cross_entropy <loss>=0.36493770406\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:44 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=500 train binary_f_1.000 <score>=0.428581932619\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:30:49 INFO 140146317055808] Iter[93] Batch [1000]#011Speed: 120459.14 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1000 train binary_classification_accuracy <score>=0.844632367632\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1000 train binary_classification_cross_entropy <loss>=0.365280385904\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:49 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1000 train binary_f_1.000 <score>=0.429367627364\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:53 INFO 140146317055808] Iter[93] Batch [1500]#011Speed: 119486.53 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1500 train binary_classification_accuracy <score>=0.84461892072\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1500 train binary_classification_cross_entropy <loss>=0.364947103034\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:53 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=1500 train binary_f_1.000 <score>=0.427800008342\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:57 INFO 140146317055808] Iter[93] Batch [2000]#011Speed: 116545.28 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=2000 train binary_classification_accuracy <score>=0.844354822589\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=2000 train binary_classification_cross_entropy <loss>=0.365171767983\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:30:57 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, batch=2000 train binary_f_1.000 <score>=0.428149121969\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, train binary_classification_accuracy <score>=0.844394957983\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, train binary_classification_cross_entropy <loss>=0.365091146578\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=93, train binary_f_1.000 <score>=0.428000900237\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20942.755937576294, \"sum\": 20942.755937576294, \"min\": 20942.755937576294}}, \"EndTime\": 1553542261.654789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542240.711573}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 234907, \"sum\": 234907.0, \"min\": 234907}, \"Total Records Seen\": {\"count\": 1, \"max\": 234877672, \"sum\": 234877672.0, \"min\": 234877672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 95, \"sum\": 95.0, \"min\": 95}}, \"EndTime\": 1553542261.655022, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 93}, \"StartTime\": 1553542240.712}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119308.126753 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:31:01.655] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 94, \"duration\": 20941, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=0 train binary_classification_cross_entropy <loss>=0.377902954102\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:01 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=0 train binary_f_1.000 <score>=0.401433691756\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:05 INFO 140146317055808] Iter[94] Batch [500]#011Speed: 117661.90 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=500 train binary_classification_accuracy <score>=0.845141716567\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=500 train binary_classification_cross_entropy <loss>=0.364386065637\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:05 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=500 train binary_f_1.000 <score>=0.43017465517\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:10 INFO 140146317055808] Iter[94] Batch [1000]#011Speed: 119178.32 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1000 train binary_classification_accuracy <score>=0.844913086913\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1000 train binary_classification_cross_entropy <loss>=0.364724917295\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:10 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1000 train binary_f_1.000 <score>=0.431047878735\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:14 INFO 140146317055808] Iter[94] Batch [1500]#011Speed: 120454.61 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1500 train binary_classification_accuracy <score>=0.844902731512\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1500 train binary_classification_cross_entropy <loss>=0.364393313475\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:14 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=1500 train binary_f_1.000 <score>=0.429483424947\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:18 INFO 140146317055808] Iter[94] Batch [2000]#011Speed: 118399.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=2000 train binary_classification_accuracy <score>=0.84463868066\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=2000 train binary_classification_cross_entropy <loss>=0.364618618517\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:18 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, batch=2000 train binary_f_1.000 <score>=0.42984109983\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, train binary_classification_accuracy <score>=0.844668267307\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, train binary_classification_cross_entropy <loss>=0.364539140925\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=94, train binary_f_1.000 <score>=0.429623926249\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20996.19698524475, \"sum\": 20996.19698524475, \"min\": 20996.19698524475}}, \"EndTime\": 1553542282.651508, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542261.654861}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 237406, \"sum\": 237406.0, \"min\": 237406}, \"Total Records Seen\": {\"count\": 1, \"max\": 237376360, \"sum\": 237376360.0, \"min\": 237376360}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1553542282.651714, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 94}, \"StartTime\": 1553542261.655278}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119004.687752 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:31:22.651] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 20994, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=0 train binary_classification_accuracy <score>=0.834\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=0 train binary_classification_cross_entropy <loss>=0.377355712891\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:22 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=0 train binary_f_1.000 <score>=0.402877697842\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:31:26 INFO 140146317055808] Iter[95] Batch [500]#011Speed: 119975.77 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=500 train binary_classification_accuracy <score>=0.845387225549\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=500 train binary_classification_cross_entropy <loss>=0.363834016598\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:26 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=500 train binary_f_1.000 <score>=0.431741653401\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:31 INFO 140146317055808] Iter[95] Batch [1000]#011Speed: 118626.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1000 train binary_classification_accuracy <score>=0.845184815185\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1000 train binary_classification_cross_entropy <loss>=0.364169059212\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:31 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1000 train binary_f_1.000 <score>=0.432693434077\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:35 INFO 140146317055808] Iter[95] Batch [1500]#011Speed: 119367.13 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1500 train binary_classification_accuracy <score>=0.845194536975\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1500 train binary_classification_cross_entropy <loss>=0.363839133916\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:35 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=1500 train binary_f_1.000 <score>=0.431175749149\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:39 INFO 140146317055808] Iter[95] Batch [2000]#011Speed: 118387.21 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=2000 train binary_classification_accuracy <score>=0.844916541729\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=2000 train binary_classification_cross_entropy <loss>=0.364065089205\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:39 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, batch=2000 train binary_f_1.000 <score>=0.431484338073\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, train binary_classification_accuracy <score>=0.844943577431\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, train binary_classification_cross_entropy <loss>=0.36398676729\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=95, train binary_f_1.000 <score>=0.431263301581\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20921.581983566284, \"sum\": 20921.581983566284, \"min\": 20921.581983566284}}, \"EndTime\": 1553542303.573582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542282.651579}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 239905, \"sum\": 239905.0, \"min\": 239905}, \"Total Records Seen\": {\"count\": 1, \"max\": 239875048, \"sum\": 239875048.0, \"min\": 239875048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 97, \"sum\": 97.0, \"min\": 97}}, \"EndTime\": 1553542303.573792, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 95}, \"StartTime\": 1553542282.65197}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119429.12357 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:31:43.573] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 96, \"duration\": 20920, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=0 train binary_classification_accuracy <score>=0.833\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=0 train binary_classification_cross_entropy <loss>=0.376809265137\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:43 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=0 train binary_f_1.000 <score>=0.397111913357\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:47 INFO 140146317055808] Iter[96] Batch [500]#011Speed: 119487.56 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=500 train binary_classification_accuracy <score>=0.845594810379\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=500 train binary_classification_cross_entropy <loss>=0.363281558221\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:47 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=500 train binary_f_1.000 <score>=0.433128393777\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:51 INFO 140146317055808] Iter[96] Batch [1000]#011Speed: 119549.18 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1000 train binary_classification_accuracy <score>=0.845431568432\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1000 train binary_classification_cross_entropy <loss>=0.363612810285\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:51 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1000 train binary_f_1.000 <score>=0.434258301126\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:56 INFO 140146317055808] Iter[96] Batch [1500]#011Speed: 118085.00 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1500 train binary_classification_accuracy <score>=0.845459027315\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1500 train binary_classification_cross_entropy <loss>=0.363284565254\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:31:56 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=1500 train binary_f_1.000 <score>=0.432779397093\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:00 INFO 140146317055808] Iter[96] Batch [2000]#011Speed: 119979.81 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=2000 train binary_classification_accuracy <score>=0.845179410295\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=2000 train binary_classification_cross_entropy <loss>=0.36351118258\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:00 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, batch=2000 train binary_f_1.000 <score>=0.433077379166\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, train binary_classification_accuracy <score>=0.845210484194\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, train binary_classification_cross_entropy <loss>=0.363434027591\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=96, train binary_f_1.000 <score>=0.432870767449\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21052.918195724487, \"sum\": 21052.918195724487, \"min\": 21052.918195724487}}, \"EndTime\": 1553542324.626964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542303.573652}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 242404, \"sum\": 242404.0, \"min\": 242404}, \"Total Records Seen\": {\"count\": 1, \"max\": 242373736, \"sum\": 242373736.0, \"min\": 242373736}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1553542324.627185, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 96}, \"StartTime\": 1553542303.574013}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118683.974154 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:32:04.627] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 97, \"duration\": 21051, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=0 train binary_classification_accuracy <score>=0.835\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=0 train binary_classification_cross_entropy <loss>=0.376263702393\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:04 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=0 train binary_f_1.000 <score>=0.408602150538\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:08 INFO 140146317055808] Iter[97] Batch [500]#011Speed: 119449.16 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=500 train binary_classification_accuracy <score>=0.845822355289\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=500 train binary_classification_cross_entropy <loss>=0.362728694284\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:08 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=500 train binary_f_1.000 <score>=0.434477658928\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:12 INFO 140146317055808] Iter[97] Batch [1000]#011Speed: 120873.85 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1000 train binary_classification_accuracy <score>=0.845667332667\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1000 train binary_classification_cross_entropy <loss>=0.36305617277\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:12 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1000 train binary_f_1.000 <score>=0.43564949606\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:32:17 INFO 140146317055808] Iter[97] Batch [1500]#011Speed: 120534.30 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1500 train binary_classification_accuracy <score>=0.845704863424\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1500 train binary_classification_cross_entropy <loss>=0.362729610009\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:17 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=1500 train binary_f_1.000 <score>=0.434236465838\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:21 INFO 140146317055808] Iter[97] Batch [2000]#011Speed: 119788.25 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=2000 train binary_classification_accuracy <score>=0.845428785607\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=2000 train binary_classification_cross_entropy <loss>=0.362956898519\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:21 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, batch=2000 train binary_f_1.000 <score>=0.434595831726\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, train binary_classification_accuracy <score>=0.845474189676\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, train binary_classification_cross_entropy <loss>=0.362880921656\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=97, train binary_f_1.000 <score>=0.434443084025\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20838.777780532837, \"sum\": 20838.777780532837, \"min\": 20838.777780532837}}, \"EndTime\": 1553542345.466266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542324.627036}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 244903, \"sum\": 244903.0, \"min\": 244903}, \"Total Records Seen\": {\"count\": 1, \"max\": 244872424, \"sum\": 244872424.0, \"min\": 244872424}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 99, \"sum\": 99.0, \"min\": 99}}, \"EndTime\": 1553542345.466495, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 97}, \"StartTime\": 1553542324.627455}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119903.563668 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:32:25.466] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 20837, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=0 train binary_classification_accuracy <score>=0.835\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=0 train binary_classification_cross_entropy <loss>=0.375718933105\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:25 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=0 train binary_f_1.000 <score>=0.408602150538\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:29 INFO 140146317055808] Iter[98] Batch [500]#011Speed: 118969.27 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=500 train binary_classification_accuracy <score>=0.846027944112\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=500 train binary_classification_cross_entropy <loss>=0.362175428257\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:29 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=500 train binary_f_1.000 <score>=0.435863682902\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:33 INFO 140146317055808] Iter[98] Batch [1000]#011Speed: 118961.09 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1000 train binary_classification_accuracy <score>=0.845884115884\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1000 train binary_classification_cross_entropy <loss>=0.36249915005\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:33 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1000 train binary_f_1.000 <score>=0.43700778781\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:38 INFO 140146317055808] Iter[98] Batch [1500]#011Speed: 120245.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1500 train binary_classification_accuracy <score>=0.845957361759\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1500 train binary_classification_cross_entropy <loss>=0.362174272714\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:38 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=1500 train binary_f_1.000 <score>=0.435783935735\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:42 INFO 140146317055808] Iter[98] Batch [2000]#011Speed: 118772.89 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=2000 train binary_classification_accuracy <score>=0.845691154423\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=2000 train binary_classification_cross_entropy <loss>=0.362402240805\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:42 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, batch=2000 train binary_f_1.000 <score>=0.436177050617\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, train binary_classification_accuracy <score>=0.845733493397\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, train binary_classification_cross_entropy <loss>=0.362327452453\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=98, train binary_f_1.000 <score>=0.43600172047\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20917.593955993652, \"sum\": 20917.593955993652, \"min\": 20917.593955993652}}, \"EndTime\": 1553542366.384363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542345.466334}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 247402, \"sum\": 247402.0, \"min\": 247402}, \"Total Records Seen\": {\"count\": 1, \"max\": 247371112, \"sum\": 247371112.0, \"min\": 247371112}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1553542366.384585, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 98}, \"StartTime\": 1553542345.466741}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=119451.846565 records/second\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:32:46.384] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 99, \"duration\": 20916, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=0 train binary_classification_accuracy <score>=0.836\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=0 train binary_classification_cross_entropy <loss>=0.375175079346\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:46 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=0 train binary_f_1.000 <score>=0.410071942446\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[03/25/2019 19:32:50 INFO 140146317055808] Iter[99] Batch [500]#011Speed: 118703.41 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=500 train binary_classification_accuracy <score>=0.846283433134\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=500 train binary_classification_cross_entropy <loss>=0.361621766355\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:50 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=500 train binary_f_1.000 <score>=0.437482652331\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:54 INFO 140146317055808] Iter[99] Batch [1000]#011Speed: 118006.58 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1000 train binary_classification_accuracy <score>=0.846140859141\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1000 train binary_classification_cross_entropy <loss>=0.361941744382\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:54 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1000 train binary_f_1.000 <score>=0.438562403625\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:59 INFO 140146317055808] Iter[99] Batch [1500]#011Speed: 118922.01 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1500 train binary_classification_accuracy <score>=0.846214523651\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1500 train binary_classification_cross_entropy <loss>=0.361618556216\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:32:59 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=1500 train binary_f_1.000 <score>=0.437351923171\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:03 INFO 140146317055808] Iter[99] Batch [2000]#011Speed: 121176.90 samples/sec\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=2000 train binary_classification_accuracy <score>=0.845951524238\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=2000 train binary_classification_cross_entropy <loss>=0.361847210825\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:03 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, batch=2000 train binary_f_1.000 <score>=0.437741341792\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, train binary_classification_accuracy <score>=0.84600160064\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, train binary_classification_cross_entropy <loss>=0.361773621167\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, epoch=99, train binary_f_1.000 <score>=0.437589145837\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, train binary_classification_accuracy <score>=0.84600160064\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, train binary_classification_cross_entropy <loss>=0.361773621167\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #quality_metric: host=algo-1, train binary_f_1.000 <score>=0.437589145837\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21023.744106292725, \"sum\": 21023.744106292725, \"min\": 21023.744106292725}}, \"EndTime\": 1553542387.408602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542366.384429}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 2499, \"sum\": 2499.0, \"min\": 2499}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Total Batches Seen\": {\"count\": 1, \"max\": 249901, \"sum\": 249901.0, \"min\": 249901}, \"Total Records Seen\": {\"count\": 1, \"max\": 249869800, \"sum\": 249869800.0, \"min\": 249869800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 2498688, \"sum\": 2498688.0, \"min\": 2498688}, \"Reset Count\": {\"count\": 1, \"max\": 101, \"sum\": 101.0, \"min\": 101}}, \"EndTime\": 1553542387.408848, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 99}, \"StartTime\": 1553542366.384824}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] #throughput_metric: host=algo-1, train throughput=118848.456933 records/second\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 WARNING 140146317055808] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 6.974935531616211, \"sum\": 6.974935531616211, \"min\": 6.974935531616211}}, \"EndTime\": 1553542387.416166, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542387.40868}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:07 INFO 140146317055808] Saved checkpoint to \"/tmp/tmpR3kNUt/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:33:07.482] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 2094031, \"num_examples\": 1}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 278, \"sum\": 278.0, \"min\": 278}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 278, \"sum\": 278.0, \"min\": 278}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 277632, \"sum\": 277632.0, \"min\": 277632}, \"Total Batches Seen\": {\"count\": 1, \"max\": 278, \"sum\": 278.0, \"min\": 278}, \"Total Records Seen\": {\"count\": 1, \"max\": 277632, \"sum\": 277632.0, \"min\": 277632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 277632, \"sum\": 277632.0, \"min\": 277632}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1553542390.529476, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"test_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542387.481435}\n",
      "\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #test_score (algo-1) : ('binary_classification_accuracy', 0.83019608690640845)\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #test_score (algo-1) : ('binary_classification_cross_entropy', 0.39791938455053166)\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #test_score (algo-1) : ('binary_f_1.000', 0.37317342339347687)\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #quality_metric: host=algo-1, test binary_classification_accuracy <score>=0.830196086906\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #quality_metric: host=algo-1, test binary_classification_cross_entropy <loss>=0.397919384551\u001b[0m\n",
      "\u001b[31m[03/25/2019 19:33:10 INFO 140146317055808] #quality_metric: host=algo-1, test binary_f_1.000 <score>=0.373173423393\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:33:10.530] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 3047, \"num_examples\": 278}\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:33:10.530] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/test\", \"duration\": 2097078, \"num_epochs\": 2, \"num_examples\": 279}\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2097132.1799755096, \"sum\": 2097132.1799755096, \"min\": 2097132.1799755096}, \"setuptime\": {\"count\": 1, \"max\": 36.06891632080078, \"sum\": 36.06891632080078, \"min\": 36.06891632080078}}, \"EndTime\": 1553542390.530287, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1553542387.416239}\n",
      "\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:33:10.542] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"epoch\": 100, \"duration\": 24156, \"num_examples\": 2499}\u001b[0m\n",
      "\u001b[31m[2019-03-25 19:33:10.542] [tensorio] [info] data_pipeline_stats={\"name\": \"/opt/ml/input/data/train\", \"duration\": 2096726, \"num_epochs\": 101, \"num_examples\": 249901}\u001b[0m\n",
      "\n",
      "2019-03-25 19:34:09 Uploading - Uploading generated training model\n",
      "2019-03-25 19:34:15 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billable seconds: 2208\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "fm.fit({'train': trainData_file, 'test': testData_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: factorization-machines-2019-03-25-19-36-34-420\n",
      "INFO:sagemaker:Creating endpoint with name factorization-machines-2019-03-25-18-55-44-623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "fm_predictor = fm.deploy(instance_type = 'ml.c4.xlarge', initial_instance_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the data returned by the model\n",
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    print(js)\n",
    "    return json.dumps(js)\n",
    "\n",
    "\n",
    "# Configure the endpoints serialization methods\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': [{'features': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}]}\n",
      "{'predictions': [{'score': 0.16122618317604065, 'predicted_label': 0.0}]}\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Make some predictions\n",
    "result = fm_predictor.predict(test_feature[1000].toarray())\n",
    "print(result)\n",
    "print(test_label[9000:9010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10159)\t1.0\n",
      "  (0, 16234)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(test_feature[9010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: factorization-machines-2019-03-25-18-55-44-623\n"
     ]
    }
   ],
   "source": [
    "# Delete endpoint\n",
    "sess.delete_endpoint(fm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
